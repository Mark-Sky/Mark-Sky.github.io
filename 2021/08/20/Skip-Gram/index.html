<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="John Doe">







<title>Skip-Gram | Hexo</title>



    <link rel="icon" href="/favicon.ico">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@300;400;500;700&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    




    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>




  <!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">Frame.</a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/categories/gallery/">Gallery</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
                
                    <span class="post-tag">
                        <a href="/tags/NLP/">
                            NLP
                        </a>
                    </span>    
                           
            
        </div>
        <div class="post-title">
            
            
                Skip-Gram
            
            
        </div>
        <span class="post-date">
            Aug 20, 2021
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li><p>给定一个中心单词centerWord，通过该算法可以预测出最有可能在该中心单词附近的单词context Words</p>
</li>
<li><p>当训练集较小的时候，也能有很高的准确率，对比较少见的单词或短语的效果预测效果较好</p>
</li>
</ul>
<hr>
<ul>
<li><p>需要注意的是，<strong>Skip-Gram算法的最终目标并不是用于预测context Words，而是用于给出一个比较好的单词的向量表示</strong>，这也是为什么Word2Vec经常会和Skip-Gram一起出现的原因</p>
</li>
<li><p>用Skip-Gram算法得到的单词向量Word Vectors可以用于寻找related words，还可以用于 Analogy tasks</p>
</li>
</ul>
<h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>Skip-Gram可以理解为一个<strong>神经网络</strong>的模型</p>
<p><img src="https://i.loli.net/2021/08/20/toOmJdU9CMyKbrF.png"></p>
<h3 id="输入-input"><a href="#输入-input" class="headerlink" title="输入 input"></a>输入 input</h3><p>Skip-Gram的输入为单词的one-hot编码的向量</p>
<h3 id="输入到隐层-W-input"><a href="#输入到隐层-W-input" class="headerlink" title="输入到隐层  $W_{input}$"></a>输入到隐层  $W_{input}$</h3><p>当隐层的维数为N时，$W_{imput}$为$V\times N$的矩阵，该矩阵不是普通的矩阵，可以理解为每个单词的单词向量，如图中，矩阵的每一行为一个3维的向量，因此在这里，单词空间为3维空间，每个单词的Word Vector为一个3维的向量。<strong>这是因为输入为one-hot编码的向量，因此在这里每一行就可以看作一个单词向量</strong></p>
<h3 id="隐层-hidden"><a href="#隐层-hidden" class="headerlink" title="隐层    $hidden$"></a>隐层    $hidden$</h3><p>隐层的N维向量就可以看作是单词的Word Vector,其实也就是$W_{input}$对应的一行</p>
<h3 id="隐层到输出层-W-output"><a href="#隐层到输出层-W-output" class="headerlink" title="隐层到输出层    $W_{output}$"></a>隐层到输出层    $W_{output}$</h3><p>实际上$W_{output}$​矩阵也是一个单词向量的编码，每一列是一个N维的单词向量Word Vector。但是这里的单词向量编码和前面的单词向量的编码是不同的，也就是说每个单词都有两种Word Vector表示：</p>
<ul>
<li><strong>当单词是center Word时，该单词的向量表示是$W_{input}$​​中的一行</strong></li>
<li><strong>当单词是context Word时，该单词的向量表示是$W_{output}$​中的一列</strong></li>
</ul>
<p>centerWord Vectors是输入到隐层的矩阵</p>
<p>outsideWord Vectors是隐层到输出层的矩阵（context Words的矩阵）</p>
<p>对于这里的理解，要<strong>结合softmax函数预测的概率来理解</strong></p>
<p><img src="https://i.loli.net/2021/08/20/VCHSsgFDORTzAXq.png"></p>
<p>虽然是矩阵表示$W_{output}$，但是其原理跟BP神经网络并不一样，也就是该矩阵不是单单跟隐藏层的向量相乘得到输出层的输入，而是用上面的softmax函数，用该矩阵与隐藏层一起作为softmax函数的输入来进行预测</p>
<h3 id="输出层-y-pred"><a href="#输出层-y-pred" class="headerlink" title="输出层 $y_{pred}$"></a>输出层 $y_{pred}$</h3><p>输出层就如上面所述，使用softmax函数作为激活函数，输出一个$V$维的向量</p>
<p>训练时用于与真实的one-hot向量作比较，得到误差</p>
<p>测试时该$V$维向量的最大的一个维度就是对应的所预测的向量</p>
<h2 id="训练-train"><a href="#训练-train" class="headerlink" title="训练 train"></a>训练 train</h2><p>我们需要确定机器学习的目标函数，也就是loss。机器学习的目的就是在测试集上达到loss的最小化，而训练的方向就是在训练集上达到loss的最小化，认为在训练集上达到loss最小化的模型在测试集上面也能达到一个比较好的效果。</p>
<p>首先，我们需要训练的参数就是$\theta=(W_{input},W_{output})$​​,分别表示中心单词的矩阵和文本单词的矩阵<br>$$<br>loss = J(\theta)=-\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>其中$T$​为整个语料库的大小，$w$​为设置的窗口的大小</p>
<p>显然当context Word出现在center Word附近的时候，我们需要让它的条件概率最大化，当上面loss最小化的时候，就达到了条件概率的最大化</p>
<p>整个过程就是遍历整个语料库，然后对每一个语料库中遍历到的单词$W_t$，计算<br>$$<br>J(\theta,W_t) = \sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>而对于每一个给定的context word$W_{t+j}$,显然所需要计算的就是<br>$$<br>\log(P(W_{t+j}|W_t;\theta))<br>$$<br>对于概率$P(W_{t+j}|W_t;\theta)$的计算，我们暂时有两种计算方法</p>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>把$W_{t+j}$也就是context word记为$u_o$​, 就是$\theta$的$W_{output}$中的一列单词向量</p>
<p>把$W_t$也就是center word记为$v_c$，就是$\theta$的$W_{input}$中的一行单词向量​</p>
<p>用softmax的公式<br>$$<br>P(W_{t+j}|W_t)=\frac{exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)}<br>$$<br>不难看出，这种方法概率的最后的和为1，也就是$\sum\limits_{i=1}^TP(Wi|W_t)=1$​</p>
<p>根据该损失函数，用梯度下降法，计算$\frac{\partial J}{\partial \theta}$<br>$$<br>\frac{\partial }{\partial v_c}(\frac{exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)})=W_{output}(y_{pred}-y_{true})<br>$$<br>对于$W_{input}$矩阵，我们只需要更新当前的中心单词的向量，因为其他单词向量的中心单词编码形式并没有出现在当前的损失函数中，因此并不会有其他单词向量的梯度更新</p>
<p>这里的$y_{true},y_{pred}$​都是V维的向量，</p>
<ul>
<li><p>$y_{pred}$​的每一个维度表示<strong>每一个单词的是当前中心单词的文本单词的概率</strong></p>
</li>
<li><p>$y_{true}$​​​​是语料库单词的one-hot编码</p>
</li>
</ul>
<p>$$<br>\frac{\partial}{\partial W_{output}}(\frac{\exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)})=v_c(y_{pred}-y_{true})^T<br>$$</p>
<p>$$<br>y_{pred}=softmax(W_{output}\cdot v_c )<br>$$</p>
<p>对于$W_{output}$矩阵，我们就需要对整个矩阵进行更新了，因为在softmax函数中，可以看到语料库中的所有单词都是出现在了该函数的分母中的，因此需要对整个矩阵求梯度并更新</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naiveSoftmaxLossAndGradient</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    centerWordVec,</span></span></span><br><span class="line"><span class="params"><span class="function">    outsideWordIdx,</span></span></span><br><span class="line"><span class="params"><span class="function">    outsideVectors,</span></span></span><br><span class="line"><span class="params"><span class="function">    dataset</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Naive Softmax loss &amp; gradient function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the naive softmax loss and gradients between a center word&#x27;s </span></span><br><span class="line"><span class="string">    embedding and an outside word&#x27;s embedding. This will be the building block</span></span><br><span class="line"><span class="string">    for our word2vec models. For those unfamiliar with numpy notation, note </span></span><br><span class="line"><span class="string">    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which</span></span><br><span class="line"><span class="string">    you can effectively treat as a vector with length x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    centerWordVec -- numpy ndarray, center word&#x27;s embedding</span></span><br><span class="line"><span class="string">                    in shape (word vector length, )</span></span><br><span class="line"><span class="string">                    (v_c in the pdf handout)</span></span><br><span class="line"><span class="string">    outsideWordIdx -- integer, the index of the outside word</span></span><br><span class="line"><span class="string">                    (o of u_o in the pdf handout)</span></span><br><span class="line"><span class="string">    outsideVectors -- outside vectors is</span></span><br><span class="line"><span class="string">                    in shape (num words in vocab, word vector length) </span></span><br><span class="line"><span class="string">                    for all words in vocab (tranpose of U in the pdf handout)</span></span><br><span class="line"><span class="string">    dataset -- needed for negative sampling, unused here.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    loss -- naive softmax loss</span></span><br><span class="line"><span class="string">    gradCenterVec -- the gradient with respect to the center word vector</span></span><br><span class="line"><span class="string">                     in shape (word vector length, )</span></span><br><span class="line"><span class="string">                     (dJ / dv_c in the pdf handout)</span></span><br><span class="line"><span class="string">    gradOutsideVecs -- the gradient with respect to all the outside word vectors</span></span><br><span class="line"><span class="string">                    in shape (num words in vocab, word vector length) </span></span><br><span class="line"><span class="string">                    (dJ / dU)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~6-8 Lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Please use the provided softmax function (imported earlier in this file)</span></span><br><span class="line">    <span class="comment">### This numerically stable implementation helps you avoid issues pertaining</span></span><br><span class="line">    <span class="comment">### to integer overflow. </span></span><br><span class="line">    y_pred = softmax(outsideVectors @ centerWordVec)</span><br><span class="line">    y = np.zeros(<span class="built_in">len</span>(outsideVectors))</span><br><span class="line">    y[outsideWordIdx] = <span class="number">1</span></span><br><span class="line">    gradCenterVec = outsideVectors.T @ (y_pred - y)</span><br><span class="line">    gradOutsideVecs = np.multiply(centerWordVec, np.expand_dims(y_pred - y, axis = <span class="number">1</span>))</span><br><span class="line">    loss = -(outsideVectors[outsideWordIdx].T @ centerWordVec) + np.log(np.<span class="built_in">sum</span>(np.exp(outsideVectors @ centerWordVec)))</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</span><br></pre></td></tr></table></figure>



<p>显然，这种方法的分母的计算开销太大，我们需要计算整个语料库的所有单词向量与中心向量相乘,因此在实际应用中，通常会采用negative sample的方法来减少计算开销</p>
<h3 id="negative-sample"><a href="#negative-sample" class="headerlink" title="negative sample"></a>negative sample</h3><p>回顾一下上面的损失函数的计算<br>$$<br>loss = J(\theta)=-\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>可以看到$-\frac{1}{T}\sum\limits_{t=1}^T$，为了消去该项，我们采用负采样的方法</p>
<p>与此同时，在负采样中，概率的计算也和上面的softmax函数是不同的，不会有遍历整个语料库的分母计算</p>
<p>对于每一个单个的中心单词，我们的目标函数为：<br>$$<br>\arg\max\limits_{\theta}p(D=1|w,c_{pos;\theta})\prod\limits_{c_{neg}\in W_{neg}}p(D=0|w,c_{neg;\theta})<br>$$<br>$W_{neg}$为所有负采样得到的样本，负采样就是那些噪声样本，这些样本没有出现在中心单词的附近</p>
<p>其中$p(D=1|w,c_{pos;\theta})$表示在语料库中出现了$(w,c_{pos})$​​​这一单词对在窗口中的概率</p>
<p>$p(D=0|w,c_{neg};\theta)$表示在语料库中没有出现$(w,c_{neg})$这一单词对在窗口中的概率</p>
<p>$c_{pos}$就是正样例，表示确实在语料库中在中心单词w的窗口内出现过$c_{pos}$这一单词</p>
<p>$c_{neg}$就是负样例，表示语料库中中心单词w的窗口内没有出现过$c_{neg}$这一单词</p>
<p>把(14)式进一步化简<br>$$<br>\arg\max\limits_{\theta}\log p(D=1|w,c_{pos};\theta)+\sum\limits_{c_{neg}\in W_{neg}}(1-\log p(D=1|w,c_{neg};\theta))<br>$$<br>对于该概率$p(D=1|w,c;\theta)$​，我们不采用softmax的计算方法，而是用sigmoid函数来计算<br>$$<br>p(D=1|w,c;\theta)=\frac{1}{1+exp(-c_{output}\cdot w)}<br>$$<br>$c_{output}$就是该单词在$W_{output}$中的对应的单词向量</p>
<p>因此(10)式可以改为<br>$$<br>\arg\max\limits_{\theta}\log \frac{1}{1+exp(-c_{pos}\cdot w)}+\sum\limits_{c_{neg}\in W_{neg}}(1-\log \frac{1}{1+exp(-c_{neg}\cdot w)})<br>$$</p>
<p>$$<br>\arg\max\limits_{\theta}\log \frac{1}{1+exp(-c_{pos}\cdot w)}+\sum\limits_{c_{neg}\in W_{neg}}\log \frac{1}{1+exp(c_{neg}\cdot w)}<br>$$</p>
<p>设$\sigma(x)=\frac{1}{1+exp(-x)}$​<br>$$<br>\arg\max\limits_{\theta}\log\sigma(-c_{pos}\cdot w)+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w)<br>$$<br>从上式可以看出，对每一个中心单词进行损失计算的话，不需要遍历整个语料库进行计算，而只需要计算负样本和正样本就可以了。因此计算梯度并更新参数的时候，我们只需要更新$w,c_{pos},W_{neg}$</p>
<p>正常来讲，损失函数应该是<br>$$<br>J(\theta)=-\frac{1}{T}\sum\limits_{i=1}^T\sum\limits_{-c&lt;j&lt;c}(\log\sigma(-c_{pos}\cdot w)+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w))<br>$$<br>这种方法在遍历完一次整个语料库后对参数进行更新，叫做<strong>batch gradient descent</strong></p>
<p>但是因为它的高计算开销，我们基本不会在实际应用中使用到</p>
<p>一般实际应用中，采用的是随机梯度下降法 <strong>stochastic gradient descent</strong></p>
<p>我们对每一对正训练对$(w,c_{pos})$进行一次梯度的计算并对其相对应的单词向量进行更新<br>$$<br>J(\theta;w,c_{pos})=-\log\sigma(c_{pos}\cdot w)-\sum\limits_{c_{neg}\in W_{neg}} \log\sigma(-c_{neg}\cdot w)<br>$$<br><strong>在更新的时候，我们仅仅更新当前的中心单词的$W_{input}$里的单词编码，以及$c_{pos},c_{neg}\in W_{neg}$在$W_{output}$里面的单词编码。</strong></p>
<p>显然这种做法大大减小了计算开销，训练速度较快。</p>
<p>对于batch gradient descent和stochastic gradient descent，可以参考标准BP算法和累计BP算法的区别。标准BP算法每一更新只针对一个样例，参数的更新非常频繁，而且对不同的样例更新的时候可能会出现“抵消”的现象，同时为了达到<strong>累计误差</strong>极小值点，标准BP算法需要进行更多次数的迭代。累计BP算法直接针对<strong>累计误差</strong>进行最小化，它在读取整个训练集一次之后才更新参数，参数更新的频率要低得多。但在很多任务中，累计误差在下降到一定程度后，进一步的下降就会非常的慢，这时标准BP算法会获得更好的解，尤其是在训练集较大的时候更为明显</p>
<p>对于每一个正训练对$(w,c_{pos})$​​,梯度计算如下：$w$是$W_{input}$的编码，$c_{pos}$是$W_{output}$的编码<br>$$<br>\frac{\partial J}{\partial c_{pos}}=(\sigma(c_{pos}\cdot w)-1)\cdot w<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial c_{neg}}=\sigma(c_{neg}\cdot w)\cdot w<br>$$</p>
<p>$$<br>\frac{\partial J}{\partial w}=(\sigma(c_{pos}\cdot w)-1)\cdot c_{pos}+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w)\cdot c_{neg}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">negSamplingLossAndGradient</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    centerWordVec,</span></span></span><br><span class="line"><span class="params"><span class="function">    outsideWordIdx,</span></span></span><br><span class="line"><span class="params"><span class="function">    outsideVectors,</span></span></span><br><span class="line"><span class="params"><span class="function">    dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">    K=<span class="number">10</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Negative sampling loss function for word2vec models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Implement the negative sampling loss and gradients for a centerWordVec</span></span><br><span class="line"><span class="string">    and a outsideWordIdx word vector as a building block for word2vec</span></span><br><span class="line"><span class="string">    models. K is the number of negative samples to take.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: The same word may be negatively sampled multiple times. For</span></span><br><span class="line"><span class="string">    example if an outside word is sampled twice, you shall have to</span></span><br><span class="line"><span class="string">    double count the gradient with respect to this word. Thrice if</span></span><br><span class="line"><span class="string">    it was sampled three times, and so forth.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Negative sampling of words is done for you. Do not modify this if you</span></span><br><span class="line">    <span class="comment"># wish to match the autograder and receive points!</span></span><br><span class="line">    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)</span><br><span class="line">    indices = [outsideWordIdx] + negSampleWordIndices</span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~10 Lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Please use your implementation of sigmoid in here.</span></span><br><span class="line">    loss = -np.log(sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec)) - np.<span class="built_in">sum</span>(np.log(sigmoid(-outsideVectors[negSampleWordIndices] @ centerWordVec)))</span><br><span class="line">    posGrad = (sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec) - <span class="number">1</span>) * centerWordVec</span><br><span class="line">    negGrad = np.expand_dims(sigmoid(outsideVectors[negSampleWordIndices] @ centerWordVec),axis =<span class="number">1</span>) @ np.expand_dims(centerWordVec,axis = <span class="number">0</span>)</span><br><span class="line">    gradOutsideVecs = np.zeros(outsideVectors.shape)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        gradOutsideVecs[negSampleWordIndices[i]] += negGrad[i]</span><br><span class="line">    gradOutsideVecs[outsideWordIdx] += posGrad</span><br><span class="line">    gradCenterVec = (sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec) - <span class="number">1</span>) * outsideVectors[outsideWordIdx] +\</span><br><span class="line">                    np.<span class="built_in">sum</span>(np.expand_dims(sigmoid(outsideVectors[negSampleWordIndices] @ centerWordVec),axis=<span class="number">1</span>) * outsideVectors[negSampleWordIndices],axis = <span class="number">0</span>)</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</span><br></pre></td></tr></table></figure>


</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0linux/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2021/08/14/An%20Intuitive%20Explanation%20of%20Field%20Aware%20Factorization%20Machines/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://hexo.io/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
