<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>《日本语》摘录</title>
    <link href="/2021/09/28/%E6%97%A5%E6%9C%AC%E8%AF%AD%E6%91%98%E5%BD%95/"/>
    <url>/2021/09/28/%E6%97%A5%E6%9C%AC%E8%AF%AD%E6%91%98%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="のだ"><a href="#のだ" class="headerlink" title="のだ"></a>のだ</h1><p>日本人在进行言语表达时，喜欢只讲述缘由而让听话人领悟到结论，所有才经常使用”のだ““のです”这一表达形式。上一节中的“あの子は寂しいんだ”，实际上想表达的可能是“あの子は寂しいので口をきかない”的意思，而“僕は知らなかったのです”中可能隐含着“僕は知らなかったので、先生にお辞儀をしませんでした”的含义</p><p>另外，“のだ”中还包含着“それが事実（真実）だ”的语义，进而派生出很多的用法</p><ul><li>（决心）何と言っても、俺はやり抜くのだ</li><li>（催促）おい、どうした、起きるのだ</li></ul><h1 id="部分否定"><a href="#部分否定" class="headerlink" title="部分否定"></a>部分否定</h1><p>日语中能够变为否定形的只有句末的动词，因此句末出现否定形时，并不能清楚地看出否定的是前面地哪个词，例如：</p><blockquote><p>車は急にとまれない</p></blockquote><p>其中的「ない」的否定对象既可以是「急に」，也可以是「とまれる」</p><p>为了解决这个问题，前辈们想了很多办法。例如在需要否定的部分插入「は」「しも」「と」等助词，如：</p><blockquote><p>伯楽は常には有らず</p></blockquote><p>其中，「ず」否定的是 「常に」，意思是：</p><blockquote><p>常にいるとは限らない、いないこともある</p></blockquote><p>这与「常に有らず」=「いつもいない」不同</p><p>这里的表达的意思和中文中的“一直不是”和“不是一直”的区别差不多，就是小学学的部分否定</p><p>英语中就是’not all’ and ‘all not’</p><h1 id="人称代词"><a href="#人称代词" class="headerlink" title="人称代词"></a>人称代词</h1><h2 id="使用频率"><a href="#使用频率" class="headerlink" title="使用频率"></a>使用频率</h2><p>英语人称代词的使用频率，占全体词汇的十分之一，其中“I”和“you”用得很多。日语中频繁地说「私が」「私が」<strong>好像是过分主张自己，会给别人带来不快地感觉</strong>。不必使用时不用，通过上下文让对方区去揣摩，这种方式更受欢迎。</p><p>英语的“I” “you”只要不是后续关系代词构成复句，一般不能附加修饰它的词语，但日语中，落语“粗忽者”的台词可以这样说：</p><blockquote><p>ここに死んでいる奴はたしかに俺だが、<strong>それを担いでいるおれ</strong>は一体誰だろう</p></blockquote><p>川端康成应邀到斯德哥尔摩去领取诺贝尔文学奖时，曾做过“美しい日本の私”的演讲，这个演讲题目应该很难译成欧洲语吧。赛登斯迪克在英语里艰难地把它译成了“美しい日本と私”</p><h2 id="词汇之多"><a href="#词汇之多" class="headerlink" title="词汇之多"></a>词汇之多</h2><p>日语中的第一人称代词就有</p><p>わたくし、わたし、わし、あたし、ぼく、おれ、わがはい</p><p>在电话中被问到“どなたですか”时，还有人回答：</p><blockquote><p>あたしよ、あたし</p></blockquote><p>这样回答，除了期待能让对方通过声音特点知道是谁之外，还似乎有这样的心理在起作用：<strong>对于那个人，称呼自己为「あたし」的人应该很少</strong></p><p>なんだかアニメのキャラが出できますが、どうにも思い出せない</p><p>格林童话“狼和七只小山羊”中有这样一段，狼扮母山羊的样子，在门外向小山羊们喊道：</p><blockquote><p>Eure Mutter ist da（お前たちのおかあさんだよ）</p></blockquote><p>笔者小时候读过的日语版的这则故事，喊的是：“<strong>わたしだよ</strong>“那才是日本式的读法</p><h1 id="「は」と「が」"><a href="#「は」と「が」" class="headerlink" title="「は」と「が」"></a>「は」と「が」</h1><p>日语中的“述定句”和“述定+传达句”这样的句子可以分成</p><ul><li>定品句</li><li>物语句</li></ul><p>按我个人的理解，定品句就是「は」的语句，物语句则为「が」，两种句子的中心是不同的</p><p>定品句中的中心是は前面的提示的主题</p><blockquote><p>あれは富士山だ</p></blockquote><p>这个句子的中心就是あれ</p><p>但是在物语句中，句子是谓语的单极结构</p><div class="note note-success">            <p>注意：和中文英文的主谓宾结构不同，日语中物语句的主语实际上是“<strong>主格补语</strong>”，它的地位和“目的格补语”以及其他附加上所谓格助词的“名词”相同，它们都从属于句子中的谓语</p>          </div><div class="note note-primary">            <p>甲が</p><p>乙に　　$\to$      紹介する</p><p>丙を</p>          </div><p>也就是说「が」前面的主语的地位并没有汉语和英语那么高</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>近端梯度下降</title>
    <link href="/2021/09/23/%E8%BF%91%E7%AB%AF%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <url>/2021/09/23/%E8%BF%91%E7%AB%AF%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>为了防止过拟合的现象，我们通常会对目标函数引入正则项，由于我们的重点是近端梯度下降，所以我们以最简单的线性回归模型为例，以平方误差为损失函数<br>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2<br>$$<br>若希望参数$w$的分量均衡，也即非零分量个数尽量稠密，则用$L_2$范数<br>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2+\lambda ||w||_2^2<br>$$</p><p>若希望非零分量个数尽量少，则使用$L_1$范数<br>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2+\lambda ||w||_1<br>$$<br>上面的这些都是西瓜书里面直接列出的陈述，我们再用画图的方法来观察一下为什么使用$L_1$范数的时候，得到的参数会比较稀疏</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20210923191125.png"></p><p>假设x仅有两个特征属性，那么$w$就只有两个分量$w_1, w_2$。</p><p>图中的平方误差项等值线也就对应着经验误差<br>$$<br>\sum\limits_{i=1}^m (y_i-w^Tx_i)^2<br>$$<br>该项看作二次函数，则可以画出对应的椭圆，实际上相当于标准椭圆移位<br>$$<br>\frac{w_1^2}{a^2}+\frac{w_2^2}{b^2}=1<br>$$<br>于是就得到了平方误差等值线，越靠近中心，则说明经验误差越小</p><p>同样的，若是$L_1$范数，等值线则为下，k越大说明离中心越远，结构误差项越大，而且显然该方程对应图中的正方形<br>$$<br>|w_1|+|w_2|=k<br>$$<br>若是$L_2$范数，等值线为下，k越大说明离中心越远，结构误差项越大，该方程对应图中的同心圆<br>$$<br>w_1^2+w_2^2=k<br>$$<br>我们引入正则项，在图中表现为，本来我们只需要取到一个较小的椭圆上面的点，但现在需要<strong>既使得该点靠近经验误差对应的椭圆的中心，又靠近结构误差对应的正方形的中心</strong></p><p>而在图中可以直观看到若选择$L_1$范数作为正则项，交点一般出现在正方形的顶点处，而这些顶点就是其中的某一个分量为0的点，因此可以使得参数的分量尽量的稀疏</p><h1 id="L1正则化问题"><a href="#L1正则化问题" class="headerlink" title="L1正则化问题"></a>L1正则化问题</h1><p>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2+\lambda ||w||_1<br>$$</p><p>$L_1$范数正则化的问题和$L_2$范数正则化的问题求解是不同的，因为$L_2$范数是连续可导的，因此可以使用随机梯度下降法（SGD）- 通过对当前所处点求导数然后沿着导数的方向下降，若是凸优化问题，则最后可以得到一个最优解。</p><p>但是$L_1$ <strong>范数不是一直可导的</strong>，因为绝对值的存在，它在那些0点都是不可导的，从上面的图就可以很清楚直观看出来。因此不能直接求导然后更新参数</p><h1 id="近端梯度下降（PGD）"><a href="#近端梯度下降（PGD）" class="headerlink" title="近端梯度下降（PGD）"></a>近端梯度下降（PGD）</h1><blockquote><p>与经典的梯度下降法和随机梯度下降法相比，近端梯度下降法的适用范围相对狭窄。对于凸优化问题，当其目标函数存在不可微部分（例如目标函数中有 L1-范数或迹范数）时，近端梯度下降法才会派上用场。</p></blockquote><p>近端梯度下降所适用的函数一般如下：<br>$$<br>h(x)=f(x)+g(x)<br>$$<br>其中$f(x)$为可微的凸函数，$g(x)$为不可微或局部不可微的凸函数</p><p>这里换一个记号，把例子中的样本数据x看作常数，把w看作函数的参数<br>$$<br>f(w)=\sum\limits_{i=1}^m (y_i-w^Tx_i)^2<br>$$</p><p>$$<br>g(w)=\lambda ||w||_1<br>$$</p><ul><li><p>写出$f(w)$的二阶泰勒展式<br>$$<br>f(w)=f(w_k)+&lt;\nabla f(w_k), w-w_k&gt;+\frac{1}{2}(w-w_k)^T\frac{\partial^2 f(w_k)}{\partial w_k^2}(w-w_k)<br>$$</p></li><li><p>假设$f(x)$满足L-Lipschitz条件，即存在常数$L&gt;0$，使得<br>$$<br>||\nabla f(x’)-\nabla f(x) ||\leq L|||x’-x||^2_2<br>$$</p></li><li><p>将上式代入到二阶泰勒展式，然后进行配方</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20210923231359.png"></p></li><li><p>代入到原问题中，可以得到（求和项的每个$f$函数都是对应样本不同的）<br>$$<br>\min\limits_w\sum\limits_{i=1}^m\frac{L}{2}||w-(w_k-\frac{1}{L}\nabla f(w_k))||^2+\lambda||w||_1<br>$$</p></li><li><p>每次在$w_k$的附近寻找最优点，不断迭代<br>$$<br>x_{k+1}=\min\limits_w\sum\limits_{i=1}^m\frac{L}{2}||w-(w_k-\frac{1}{L}\nabla f(w_k))||^2+\lambda||w||_1<br>$$</p></li><li><p>假设$z=w_k-1/L\nabla f(w_k)$，上式有闭式解（实际上就相当于对一个有多种情况的二次函数进行分类讨论，每种情况都会有其闭式解，只需要得到最小的就行）</p></li></ul><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20210923231312.png"></p><p>从上面的式子画图，我们可以发现，正则项相当于把$x^i_{k+1}=z^i$这个线性函数向中间拉扯了</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesE77E9F23B1777EE772381A6E7E467C9A.png"></p><p>这也符合了$L_1$正则项能使得参数的分量尽量稀疏的说法</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动漫台词抄录</title>
    <link href="/2021/09/21/%E5%8A%A8%E6%BC%AB%E5%8F%B0%E8%AF%8D%E6%8A%84%E5%BD%95/"/>
    <url>/2021/09/21/%E5%8A%A8%E6%BC%AB%E5%8F%B0%E8%AF%8D%E6%8A%84%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>感觉到了大三开始变得闲了下来，总要找点事干干。这个将会是一个持续更新的系列，排名不分先后，想到哪个就会更新，算是记录笔者的二次元轨迹吧。当然是用神番赛马娘东海帝皇（トウカイテイオー）镇楼</p><p>ねえねえねえ、どうしてみんな見てくれないの、こんなに真面目なのに</p><p><img src="https://i.loli.net/2021/09/21/yxD46jrE3klfRgS.png"></p><h1 id="恋爱小行星"><a href="#恋爱小行星" class="headerlink" title="恋爱小行星"></a>恋爱小行星</h1><p>这部番是我第一步追着更新看的番，真的是老兵回忆录了，看到完结撒花的时候真的很舍不得</p><p>从那时候完全不懂她们在说什么到现在能听译下来</p><div class="note note-success">            <p>みんな　好きなものや得意なもの　その人の世界を持ってる</p><p>ひとりでいたら　世界はひとつだけと</p><p>それが繋がったら　たくさんの可能性がどんどん広がって</p><p>大きくて　未知数で</p><p>宇宙みたい</p>          </div><div class="note note-primary">            <p>大家热爱的事物，擅长的事物都不同，都拥有自己的世界</p><p>一个人的话只有一个世界</p><p>这些互相连接的话</p><p>就会展开许多的可能性</p><p>广大 未知</p><p>就像是宇宙一样</p>          </div><p><img src="https://i.loli.net/2021/09/18/clP5r6pnWGstjDL.png"></p><p><img src="https://i.loli.net/2021/09/18/hOKpePdcbrQV1xF.png"></p><p><img src="https://i.loli.net/2021/09/18/KXYB69nemGhw5Vs.png">     <img src="https://i.loli.net/2021/09/18/LwTG9x4ioCHXNqW.png"></p><p>这段话配上背景的气氛和配乐真的很有感觉啊，番里面出现了几个只出现了很短时间的配角，但是每个配角都没有强行插入的感觉，只是日常很普通的相遇，然后交流，到分开，每个人都给这故事的日常的感觉增色许多，每个人都会有自己的世界，和主角团她们相遇，让故事里的世界更加丰富多彩。因为是一部日常番，所以并没有必要插入配角来推动情节发展，加入的这些配角或许只是让结尾的这段话更有含义吧。每个人相遇在一起，所能创造的世界就和那星空一样，壮美，深邃。还有米拉的眼睛，画的也很像宇宙一样漂亮</p><hr><h1 id="紫罗兰永恒花园"><a href="#紫罗兰永恒花园" class="headerlink" title="紫罗兰永恒花园"></a>紫罗兰永恒花园</h1><p>京紫前几集看的其实是有点血压上升，隔了很久才重新拾起，但是到了4集以后每集都是爆好看啊，我觉得这个可以更很多个场景台词有没有</p><h2 id="在某处的星空下"><a href="#在某处的星空下" class="headerlink" title="在某处的星空下"></a>在某处的星空下</h2><p><img src="https://i.loli.net/2021/09/21/92ZKNQAtS4uai5G.png"></p><div class="note note-success">            <p>旅先で再び彼女と会える可能性がどのくらいあるんだろうか</p><p>もう一度あの彗星を見上げる確率だろうか</p><p>それでも俺はもう躊躇うことはないだろう</p><p>閉じられていた扉の向こうに</p><p>歩き出す勇気を</p><p>彼女がくれたんだから</p><p>いつかきっと</p><p>どこかの星空の下で</p>          </div><p><img src="https://i.loli.net/2021/09/21/qw7M6XhIkiUtrpP.png"></p><div class="note note-primary">            <p>旅途中能再次遇到她的可能性会有多大呢</p><p>是否就像再次目击那颗彗星一样微乎其微</p><p>尽管如此我也不再会犹豫</p><p>因为她已经赐予了我</p><p>走出这扇门的勇气</p><p>有朝一日</p><p>在某处的星空下</p>          </div><p><img src="https://i.loli.net/2021/09/21/TwrNlH9LfkdGvZ4.png"></p><p>实际上这一集的剧情没有其他的催泪，不过它在最后展现的男主的期待的感觉还是挺有感染性的，由原来的自闭封锁，变成对未知的未来充满希望的感觉，配上清晨的蓝天白云的背景，让人感受到了少年心中的希望和激情因为薇妹终于回来了。最后的「どこかの星空の下で」跟中国古代的以意境为结尾的手法很像</p><blockquote><p>孤帆远影碧空尽，唯见长江天际流。</p></blockquote><p>以意境结尾，很有日本人的那种委婉的感觉</p><p>实际上这句话肯定是</p><p>「いつかきっとどこかの星空の下で（彼女と一緒に星空を見上げる）」</p><p>但是把全部说出来倒变得很没意思了，留有一点想象空间，以意境结尾重点就放在了那ほしぞら上面而不是人物的动作上面了，更有一种美的感觉，和「今晩の月は綺麗です」大概是一个道理吧</p><p><img src="https://i.loli.net/2021/09/21/oWOsVZ5mSjdR4Fc.png"></p><p>最后还要再吹一句，每次ED都是神插入啊，真就「星空」啊</p><hr><h1 id="赛马娘二"><a href="#赛马娘二" class="headerlink" title="赛马娘二"></a>赛马娘二</h1><p>这真的是神番啊，有些场面看一次就泪目一次，本来想开一个新的文章来谈谈的，但是反正都是只有自己看，就在这里把想写的写下来吧</p><h2 id="绝对，一定"><a href="#绝对，一定" class="headerlink" title="绝对，一定"></a>绝对，一定</h2><div class="note note-primary">            <p>追いつけない背中を</p><p>認めるのは怖かった</p><p>本気の夢だったから</p>          </div><p>トウカイテイオー　これがあきらめないんだ！</p><h2 id="秒表"><a href="#秒表" class="headerlink" title="秒表"></a>秒表</h2><p>（未完待续，持续更新中…）</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网络层</title>
    <link href="/2021/09/18/%E7%BD%91%E7%BB%9C%E5%B1%82/"/>
    <url>/2021/09/18/%E7%BD%91%E7%BB%9C%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="路由和转发"><a href="#路由和转发" class="headerlink" title="路由和转发"></a>路由和转发</h1><h2 id="英文"><a href="#英文" class="headerlink" title="英文"></a>英文</h2><p>路由 – routing/switch</p><p>转发 – forward</p><p>分组 – packet</p><p>多说一句，感觉【路由】这个翻译的真的不错，又有那么点音译的意味（粤语中的“路”和routing的rou的发音基本是一样的，但是又兼顾了汉字的意思–“由”哪条“路”到哪条“路”</p><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ul><li>路由是指决定分组从发送节点到目的节点所经过的路径，也就是选择经过哪些路由器交换机转发<ul><li>通常路由就是选择最短路径，或者最小成本路径</li></ul></li></ul><img src="https://i.loli.net/2021/09/18/xrSEUnHDFmYpC7f.png" style="zoom:67%;" /><ul><li>转发是指将分组从一个输入链路接口转移到适当的输出链路接口的路由器本地动作<ul><li>包含错误处理，排队，调度等过程</li></ul></li></ul><img src="https://i.loli.net/2021/09/18/7NbQwcAq6xKsBCD.png" style="zoom: 67%;" /><h2 id="和其他层关系"><a href="#和其他层关系" class="headerlink" title="和其他层关系"></a>和其他层关系</h2>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>信号的时域分析</title>
    <link href="/2021/09/17/%E4%BF%A1%E5%8F%B7%E7%9A%84%E6%97%B6%E5%9F%9F%E5%88%86%E6%9E%90/"/>
    <url>/2021/09/17/%E4%BF%A1%E5%8F%B7%E7%9A%84%E6%97%B6%E5%9F%9F%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这门课应该是电子专业的必修课，是在微积分，复变函数，信号与系统的基础之上建立起来的一个学科。但是我自己并不是电子专业的，学的是人工智能专业，应该侧重点会和电子专业学的时候会不同。</p><h1 id="信号的分类"><a href="#信号的分类" class="headerlink" title="信号的分类"></a>信号的分类</h1><h2 id="周期性"><a href="#周期性" class="headerlink" title="周期性"></a>周期性</h2><p>对于一个给定的解析式的信号，会有一些判定该信号是否为周期信号的定理</p><ul><li>周期信号$x(t)$，$y(t)$的周期为$T_1,T_2$，若周期之比$T_1/T_2$为有理数，则其<strong>和信号</strong>$x(t)+y(t)$仍为周期信号，其周期为$T_1,T_2$的最小公倍数</li></ul><div class="note note-success">            <p>周期信号可能分解为周期信号，</p><p><strong>非周期信号也可能分解为周期信号</strong></p>          </div><ul><li>离散信号可视为连续信号和周期脉冲的乘积，因此可用相关积信号的性质对其分析</li></ul><p><img src="https://i.loli.net/2021/09/17/LgawsyWuIYC4xQm.png"></p><div class="note note-success">            <p>两连续周期信号之和不一定是周期信号，而两周期序列之和一定是周期序列</p>          </div><h2 id="能量信号与功率信号"><a href="#能量信号与功率信号" class="headerlink" title="能量信号与功率信号"></a>能量信号与功率信号</h2><p><img src="https://i.loli.net/2021/09/17/Js4cebhZPaVlRHo.png"></p><div class="note note-success">            <p>信号𝑥(𝑡)可以是一个既非功率信号，又非能量信号。但一个信号不可能同时既是功率信号，又是能量信号。</p>          </div><h2 id="因果信号"><a href="#因果信号" class="headerlink" title="因果信号"></a>因果信号</h2><p>非因果信号指的是在时间零点前有非零值</p><h1 id="典型的信号"><a href="#典型的信号" class="headerlink" title="典型的信号"></a>典型的信号</h1><h2 id="基本连续信号"><a href="#基本连续信号" class="headerlink" title="基本连续信号"></a>基本连续信号</h2><h3 id="直流信号"><a href="#直流信号" class="headerlink" title="直流信号"></a>直流信号</h3><h3 id="正弦信号"><a href="#正弦信号" class="headerlink" title="正弦信号"></a>正弦信号</h3><h3 id="实指数信号"><a href="#实指数信号" class="headerlink" title="实指数信号"></a>实指数信号</h3><h3 id="虚指数信号"><a href="#虚指数信号" class="headerlink" title="虚指数信号"></a>虚指数信号</h3><ul><li><p>解析式：$x(t)=Ae^{\alpha t}$，且其中$\alpha$为纯虚数</p></li><li><p>周期性：$x(t)=x(t+T_0)=e^{jw_0}=e^{jw_0(t+T_0)}$</p></li><li><p>$T_0=\frac{2\pi}{|w_0|}$</p></li><li><p>虚指数信号由欧拉公式可分解为正弦信号和余弦信号相加的形式<br>$$<br>e^{j\omega_0t}=\cos\omega_0t+j\sin w_0t<br>$$</p></li></ul><h3 id="复指数信号"><a href="#复指数信号" class="headerlink" title="复指数信号"></a>复指数信号</h3><p>要先搞清楚虚数与极坐标的关系</p><p>虚数z可以写成笛卡尔坐标的形式<br>$$<br>z=x+iy<br>$$<br>也可以写成极坐标的表达形式<br>$$<br>z = |z|e^{i\theta}<br>$$<br>两种表达形式可以互相转化（欧拉公式）<br>$$<br>|z|=\sqrt{x^2+y^2}<br>$$</p><p>$$<br>\theta = \arctan (y/x)<br>$$</p><p><img src="https://i.loli.net/2021/09/17/zpIBK3Md2iGRV7t.png"></p><h3 id="抽样信号"><a href="#抽样信号" class="headerlink" title="抽样信号"></a>抽样信号</h3><ul><li>解析式：$Sa(t)=\frac{\sin t}{t}$</li><li><img src="https://i.loli.net/2021/09/17/HmJOxCzNi8nhXcq.png"></li><li><img src="https://i.loli.net/2021/09/17/EKLj7TkutAVUMxd.png"></li><li></li></ul><h1 id="信号的分解"><a href="#信号的分解" class="headerlink" title="信号的分解"></a>信号的分解</h1>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DSP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>信息熵的解释</title>
    <link href="/2021/09/16/%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E8%A7%A3%E9%87%8A/"/>
    <url>/2021/09/16/%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E8%A7%A3%E9%87%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="数学定义"><a href="#数学定义" class="headerlink" title="数学定义"></a>数学定义</h1><p>给定数据集$D$，D中第$i$类样本所占比例为$p_i(i=1,2,…,|Y|)$</p><p>则该数据集的信息熵定义为<br>$$<br>Ent(D)= -\sum\limits_{i=1}^{|Y|}p_klog_2p_k<br>$$</p><h1 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h1><p>信息熵可以理解为**$\frac{1}{p_k}$​的二进制编码长度的数学期望<strong>，更进一步，可以理解为</strong>数据集$D$的各个类别的稀有度的数学期望**</p><h2 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h2><p>对于信息熵的数学定义，可以看成是如下数学期望<br>$$<br>E[log_2\frac{1}{p_k}]=\sum\limits_{i=1}^{|Y|}p_klog_2\frac{1}{p_k}<br>$$<br>$X_k = log_2\frac{1}{p_k}$表示随机变量X的分类为k时，其值$x_k$为$log_2\frac{1}{p_k}$<br>$$<br>E[X]=\sum\limits_{i=1}^{|Y|}p_kx_k<br>$$</p><h2 id="稀有度"><a href="#稀有度" class="headerlink" title="稀有度"></a>稀有度</h2><p>$1/p_k$ 就表示该类别的稀有度，该值越大，则表示稀有程度越高</p><p>而$log_2$则表示对稀有度进行二进制编码的长度，$log_21/p_k$​​越大，表示所需编码长度就越长。这个也不难理解，<strong>越稀有的事物，则描述难度就越高</strong>，而越常见的事物，描述就越简单。比如说，在二次元里，黄毛是很常见的，我们只需要用两个字就可以表示出来，但是最后输了的傲娇黄毛是比较稀缺的，我们需要用9个字才表示出来这个意思。在上面的自然语言例子里面，换一个角度看，我们只描述一个普遍的东西是比较简单的，只需要用一个对应的名词就可以表示出来，但是若在名词前面加上定语之类的，那么显然该事物的稀有程度就上升了</p><p><img src="https://i.loli.net/2021/09/16/Ulr2NmF4fiQwW6b.png"></p><p>因此$log_21/p_k$同样表示类别的稀有程度</p><h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>$Ent(D)$越大，表示该数据集的存在的不同的类别越多，而且稀有类别的类别数也越多。</p><p>极端地想，$D$数据集如下</p><p>${1,2,3,4,5}$</p><p>该数据集的每个类别的稀有程度都一样高，都只有$1/5$​，而且类别的数目也很多，则信息熵也会比较高</p><p>而换一种比较高的信息熵的数据集，还可以像是下面这样</p><p>${1,1,1,1,1,5}$</p><p>该数据集存在一个稀有类别5，该稀有类别所占的期望会很大，导致信息熵比较高</p><p>我们在决策树中做特征集划分的时候，就是要取出一个特征，使得用该特征可以把那些稀有类别都剔除出去，稀有类别为一个新的数据子集，还是用上面的例子，就是希望取出一个特征，能把上面的数据集划分成两个数据子集</p><p>${1,1,1,1}$和${5}$</p><p>如此，两个子集都不存在稀有类别了，在子集的范围内，1和5都是常见类别了，两个集合的信息熵都变为了0</p><h1 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h1><p>$$<br>Gain(A)=Ent(D)-\sum\limits_{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)<br>$$</p><p>极端地想，若我们划分出来的子集全部都不存在稀有类别，则$Ent(D^v)$​全都等于0，则该特征选取是最好的</p><p>稀有程度的期望值越小，则$Gain(A)$的值就越大，该特征选取就越好</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>降维与度量学习</title>
    <link href="/2021/09/16/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    <url>/2021/09/16/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这篇博客并不打算把下面的所有内容都推导整理一遍，而只是把上课时老师讲的比较有意思的记录下来，也结合一些自己的理解吧。毕竟如果把所有内容都推导一遍那就相当于抄一遍书了</p><h1 id="k近邻学习"><a href="#k近邻学习" class="headerlink" title="k近邻学习"></a>k近邻学习</h1><p>k近邻学习是懒惰学习的代表。指的是其在得到训练集的时候不学习，而是在得到了测试样本时再运行，实际上这个大概不属于传统意义上的机器学习，因为它没有模型参数之类的需要用训练集训练得到的，最简单的k近邻学习的唯一一个参数就是k，还是超参数</p><p>注意区分k近邻学习和k-means聚类学习，前者是监督学习算法，而后者是无监督学习</p><p>k近邻学习看起来好像和后面的内容没什么关系，实际上k近邻的一个重要部分就是距离计算</p><ul><li>选择不同的距离计算也就和后面的度量有关</li><li>如何简化距离的计算和降维有关</li></ul><h2 id="NCA"><a href="#NCA" class="headerlink" title="NCA"></a>NCA</h2><p>NCA是k近邻学习的进阶版，它不是懒惰学习，而是需要学习马氏距离中的半正定矩阵$M$，这个是后面的度量学习的内容，这里只分析NCA的意义</p><p>若NCA只关心k近邻学习的距离计算，那么其学习的度量矩阵和样本的维度$d$，相对应为$d\times d$​​。由于k近邻的距离计算就是普通的欧氏距离，所以有可能对于一些非线性空间的数据集的分类效果不是很好</p><p><img src="https://i.loli.net/2021/09/16/dwZgTptXSPGDsvH.png"></p><p>如上图，要学习的$d(x_i,x_j)=\sqrt{(x_i-x_j)^TM(x_i-x_j)}$，能使得两个点的距离并不是欧氏距离而是测地线距离，才能使得分类效果较好</p><p>若NCA还同时希望能使得距离计算的复杂度下降，那么需要如下转换一下$M$<br>$$<br>M = A^TA<br>$$<br>其中$A$为$d’\times d$​的矩阵，如此计算距离的式子可以变为如下<br>$$<br>d(x_i, x_j)=\sqrt{(Ax_i-Ax_j)^T(Ax_i-Ax_j)}<br>$$<br>就相当于把样本的维度降到了$d’$</p><p>这样就解决了k近邻的维数灾难问题</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>k近邻分类器在判别分类时通常用多数投票法，但是NCA为了使得目标函数方便优化，替换为概率投票法，这样容易写出目标函数，也容易对目标函数进行优化，因为这样目标函数就是连续函数，NCA的优化目标：<br>$$<br>\min\limits_P\quad1-\sum\limits_{i=1}^m\sum\limits_{j\in \Omega_i}\frac{exp(-||A^Tx_i-A^Tx_j||^2_2)}{\sum_lexp(-||A^Tx_i-A^Tx_l|^2_2|)}<br>$$<br>目标函数的里层实际上类似于一个softmax函数，计算的是一种概率，表示任意样本$x_j$对$x_i$分类结果影响的概率</p><h1 id="低维嵌入"><a href="#低维嵌入" class="headerlink" title="低维嵌入"></a>低维嵌入</h1><p><strong>低维嵌入和主成分分析一样，都是一种对数据降维的方法，但是其目标和PCA是不同的</strong></p><p>它的目标是寻找一个子空间，使得子空间的距离和样本在原来的空间的距离是尽量保持一致的</p><p>它可以转换为另一个问题：<strong>如何在低维子空间和高维空间之间保持样本之间的内积不变</strong>？</p><p>设样本维度为$d$，样本个数为$m$，样本之间的内积矩阵在低维和高维空间都是$B(m\times m)$​，可以对其进行特征值分解<br>$$<br>B= V\Lambda V^T<br>$$<br>$V$为$m\times d$矩阵，$\Lambda$为$d\times d$矩阵</p><p>然后只取$\Lambda$​中较大的$d’$个特征值，组成新的矩阵$\Lambda_1$，同时$V$也截断部分，变为$V_1$，$V_1$为$m\times d’$矩阵<br>$$<br>B=V_1\Lambda_1V^T_1<br>$$<br>这样操作后，内积矩阵$B$基本不会有太大变化，但是样本的维度由$d$降维$d’$</p><p>样本在低维空间中组成的矩阵为$Z$,<br>$$<br>Z =\Lambda_1^{1/2}V_1^T\in R^{d’\times m}<br>$$</p><p>$$<br>B=Z^TZ\in R^{m\times m}<br>$$</p><p>关键是要记得，<strong>低维嵌入MDS的目标是保持距离不变，转换为保持内积不变，其做法是对样本的内积矩阵进行特征值分解</strong></p><h1 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h1><p>关于PCA，主要就是想说，注意其和低维嵌入的区别</p><ul><li>它是对样本的协方差矩阵进行特征值分解，而不是内积矩阵</li><li>目标是找到超平面，使得满足<ul><li>最近重构性：样本点到这个超平面的距离都足够近</li><li>最大可分性：样本点在这个超平面上的投影能尽可能分开</li></ul></li></ul><h2 id="内积矩阵和协方差矩阵"><a href="#内积矩阵和协方差矩阵" class="headerlink" title="内积矩阵和协方差矩阵"></a>内积矩阵和协方差矩阵</h2><p>内积矩阵是描述两个向量相似度的东西，若样本个数为m个，样本维度为n维，则内积矩阵会是$m\times m$维</p><p>而协方差矩阵是从特征的角度研究向量，和上面正好是相反的，若样本个数为m个，样本维度为n维，则内积矩阵会是$n\times n$​维</p><p>内积矩阵的计算是<br>$$<br>P=XX^T<br>$$<br>协方差矩阵的计算是<br>$$<br>C=X^TX<br>$$</p><h1 id="流形学习"><a href="#流形学习" class="headerlink" title="流形学习"></a>流形学习</h1><p>关键：<strong>测地线距离（近似），保距</strong></p><p>流形学习用一种类似于树的最短路径搜索的方法来近似得到点与点之间的测地线距离</p><h1 id="度量学习"><a href="#度量学习" class="headerlink" title="度量学习"></a>度量学习</h1><p>降维的主要目的是希望找到一个“合适的”低维空间，而机器要学习的就是学出合适的距离的度量</p><p>而一般来说，就是要学习马氏距离的矩阵$M$</p><p>还要知道对M的学习的目标是什么，优化的是什么目标函数</p><ul><li><p>某种分类器的性能</p><ul><li>如以近邻分类器的性能为目标，得到NCA</li></ul></li><li><p>该距离希望结合领域知识</p><p><img src="https://i.loli.net/2021/09/16/KhTwcW467sZLDMk.png"></p></li></ul><p>这里有个有意思的地方，就是我们是最小化同类之间的距离，同时令异类之间的距离大于1，但是没有用最大化异类之间的距离，同时令同类之间的距离小于某个值，因为最大化是趋向无穷的，如果优化时目标为最大化，很容易导致M的结果有很多问题，而最小化只能到0，一般来说优化都是最小化而没有最大化的</p><p><img src="https://i.loli.net/2021/09/16/ZHKPqEAzyXrxv59.png"></p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>词法分析</title>
    <link href="/2021/09/07/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <url>/2021/09/07/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="词法分析相关概念"><a href="#词法分析相关概念" class="headerlink" title="词法分析相关概念"></a>词法分析相关概念</h1><p><strong>注意区分词法单元的单元名和词素</strong>，词法单元是抽象层次更高的，词素则是具体的实例</p><p>词法单元包含两个部分，单元名和属性，其中属性值是一个结构化的数据，其中包含词素，类型，第一次出现的位置，….编译时能够识别undefined variable，就是靠属性值中记录的第一次出现的位置来判定的</p><p><img src="https://i.loli.net/2021/09/07/BLNioCHqZO5Qf82.png"></p><p>如上图，一般词素就是具体的定义，比如说人定义的变量a,b,c等等，都是词素，而它们都可以抽象成词法单元中的id</p><hr><p>识别词法单元时，源程序会有一个固定的Pattern，这个Pattern需要有精确的数学描述定义，而且计算机能够运行识别，我们使用正则表达式来作为该模式的数学描述</p><p>在正则表达式之前，需要了解一些相关概念</p><h2 id="字母表"><a href="#字母表" class="headerlink" title="字母表"></a>字母表</h2><p>一个<strong>有限</strong>的符号集合</p><p>比如英语的字母表就是${a,b,..,z,A,B,..,Z}$</p><h2 id="串"><a href="#串" class="headerlink" title="串"></a>串</h2><p>字母表中符号组成的一个有穷序列</p><p>比如英语字母表的串，就是一个英语单词</p><h2 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h2><p>给定字母表上一个任意的可数的串的集合</p><ul><li>语法正确的C程序的集合，英语，汉语，日语</li></ul><p>如英语就是由所有英语单词（串）组成的集合，这里的语言不包含语言的语法，仅仅是一个单词的集合的意思</p><h2 id="语言的运算"><a href="#语言的运算" class="headerlink" title="语言的运算"></a>语言的运算</h2><p><img src="https://i.loli.net/2021/09/07/PuWX9v1VIRThyoZ.png"></p><img src="https://i.loli.net/2021/09/07/FMonZEIW1Ubt6ya.png" style="zoom: 67%;" /><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p>正则表达式是一种形式的描述语言，它可以抽象地表示语言的模式类型，表示语言中的具有某种固定形式的子集。</p><p>可以先看一个正则表达式的例子，在C语言中的定义的标识符规则，可以用正则表达式表示<br>$$<br>letter_(letter_|digit)^*<br>$$<br>其中的**letter_**表示任一字母或者下划线，竖线相当于语言的并运算，表示可以是字母或者下划线或者数字，星号的定义和语言的闭包的定义一样，表示可以由任意长度的括号内的表达式连接起来。而letter_直接与后面的括号连接相当于语言的连接运算。该定义是与自然语言的表述规则是一样的：</p><ul><li>标识符的开头需要是字母或者下划线，不能是数字等其他东西</li><li>标识符中可以包含数字，字母，下划线</li><li>标识符的长度可以是大于1的任意长度</li></ul><p>从该例子可以体会到</p><ul><li>正则表达式自身是一种语言，它也有自己定义的运算</li><li>正则表达式的运算可以映射到其描述的语言的运算</li><li>正则表达式是一种简洁的，精确的描述<strong>语言</strong>的固有模式（或说规则）的<strong>语言</strong></li></ul><h2 id="正则表达式的形式定义及到语言上的映射"><a href="#正则表达式的形式定义及到语言上的映射" class="headerlink" title="正则表达式的形式定义及到语言上的映射"></a>正则表达式的形式定义及到语言上的映射</h2><ul><li><p>字母表$\Sigma$上的正则表达式的定义</p></li><li><p>基本部分</p><ul><li>$\varepsilon$ 是一个正则表达式，$L(\varepsilon)= {\varepsilon}$</li><li>如果a是$\Sigma$上的一个符号，那么a是正则表达式，$L(a)={a}$</li></ul></li><li><p>归纳步骤</p><ul><li>选择：$(r)|(s)$​，$L((r) | (s))=L(r) \cup L(s)$​；</li><li>连接：$(r)(s)，L((r)(s))=L(r)L(s) $；</li><li>闭包：$(r)<em>，L((r)^</em>)=(L(r))^*$​​​​；</li><li>括号：$(r)，L((r))=L(r)$</li></ul></li><li><p>运算的优先级：$* &gt;$ 连接符$&gt;$ |</p></li></ul><p>该节内容类似于<strong>数理逻辑</strong>中的内容，实际上正则表达式是一个抽象的东西，其自身由正则表达式（元素）及正则表达式的运算（运算）组成，但是可以定义映射$L$​​来把正则表达式的元素映射到语言上，也可以把正则表达式的运算映射到语言的运算（<strong>语言的运算实际上是集合的运算</strong>）中</p><h2 id="正则表达式的例子"><a href="#正则表达式的例子" class="headerlink" title="正则表达式的例子"></a>正则表达式的例子</h2><p><img src="https://i.loli.net/2021/09/14/etjpvGn4Xka5Vr8.png"></p><h2 id="正则表达式的性质"><a href="#正则表达式的性质" class="headerlink" title="正则表达式的性质"></a>正则表达式的性质</h2><ul><li><p>等价性：如果两个正则表达式$r$和$s$表示同样的语言,也就是$L(r)=L(s)$，则$r=s$</p></li><li><p>代数定律</p><p><img src="https://i.loli.net/2021/09/14/8PSHAJiIgMjWmEL.png"></p></li></ul><h2 id="正则定义"><a href="#正则定义" class="headerlink" title="正则定义"></a>正则定义</h2><p>通过正则定义，可以使得正则表达式更加贴近人类可读的语言，且更为简洁</p><p>例如：C语言的标识符集合</p><p>$letter_\rightarrow A|B|…|Z|a|b…|z|_$</p><p>$digit\to 0|1|…|9$</p><p>$id\to letter_(letter_|digit)^*$</p><p>通过定义letter和digit，使其更接近人类语言，且用这两个正则定义可以继续定义$id$​，这使得id的定义十分简洁。否则可以把id的定义拆开，会写的十分的冗长且不d可读 </p><h1 id="状态转换图"><a href="#状态转换图" class="headerlink" title="状态转换图"></a>状态转换图</h1><p>上面的正则表达式始终还是形式的抽象定义，而词法分析器需要的是计算机能处理的表达，因此我们需要一个更为具象的实现正则表达式的方式，那就是状态转换图。</p><p><strong>根据正则表达式，可以定义出状态转换图，而状态转换图可以由计算机处理，且返回识别出来的词法单元</strong></p><h2 id="词法单元模式"><a href="#词法单元模式" class="headerlink" title="词法单元模式"></a>词法单元模式</h2><p>下面的讨论都会围绕这里列出的词法单元进行</p><p><img src="https://i.loli.net/2021/09/14/VdLlinS3fxZ8qy2.png"></p><p>ws表示空白符</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>状态转换图就是一个图，有节点和边组成</p><ul><li>状态（节点）：表示在识别词素的过程中可能出现的情况<ul><li>状态看作是已处理部分的总结</li><li>一般的状态的节点就是一个圈</li><li>某些状态作为接受状态，表示已经找到词素，在图中用两个圈表示</li><li>加上*的接收状态表示最后读入的符号不在词素中</li><li>开始状态用start边表示</li></ul></li><li>边：从一个状态到另一个状态<ul><li>如果当前状态为s，下一个输入符号为a，就沿着从s离开，标号为a的边到达下一状态</li></ul></li></ul><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><img src="https://i.loli.net/2021/09/14/oSaJB5dPe2Zz76U.png" style="zoom: 80%;" /><p><img src="https://i.loli.net/2021/09/14/NPgjATwifarKLZ1.png"></p><p>图3-16可以表示无符号数字的识别，若状态到达19，20，21这三个的任意一个，则表示该数字识别成功，返回数字。该图可以解释$10.$不可以作为最终状态，也就是若程序中写数字时，写成$10.$的话，编译时就会报错</p><p>初始状态为12，然后读取1，进入13状态，再读取0，进入13状态，读取.，进入14状态，最后没有可读取的了，于是最终会停留在14状态，但是14状态不是接受状态，因此10.并不是一个正确的数字的写法</p><h2 id="构造词法分析器"><a href="#构造词法分析器" class="headerlink" title="构造词法分析器"></a>构造词法分析器</h2><p>用程序实现词法分析器有很多方法，当然可以在每个状态都用if实现，但是若语言支持switch的话，可以比较简洁地写出来</p><ul><li>用state记录当前状态</li><li>一个switch根据state地值转到相应的代码</li><li>进入到某个接受状态时，返回相应的词法单元</li></ul><img src="https://i.loli.net/2021/09/14/wi9sZnJKVDyxXWg.png" style="zoom:67%;" /><p>该段代码对应图3-13的状态转换图</p><h2 id="多个模式集成到词法分析器"><a href="#多个模式集成到词法分析器" class="headerlink" title="多个模式集成到词法分析器"></a>多个模式集成到词法分析器</h2><p>在解析源程序的时候，肯定是要有多种正则表达式匹配的，比如说，digits正则表达式匹配数字，id正则表达式匹配标识符，但是一个状态转换图只能对应一个正则表达式，因此我们识别词法单元时，需要用多个状态转换图来识别</p><ul><li>可以顺序匹配多个模式，若引发fail，则回退并启动下一个状态转换图</li><li>可以并行的运行各个状态转换图</li><li>所有的状态转换图合并为一个图</li></ul><h1 id="有穷自动机"><a href="#有穷自动机" class="headerlink" title="有穷自动机"></a>有穷自动机</h1><ul><li><p>有穷自动机在本质上等价于状态转换图</p></li><li><p><strong>区别在于</strong>，自动机是识别器，对每个输入串回答yes or no</p></li><li><p>分为两类</p><ul><li>不确定的有穷自动机（NFA）</li><li>确定的有穷自动机（DFA）</li></ul></li></ul><h2 id="不确定与确定"><a href="#不确定与确定" class="headerlink" title="不确定与确定"></a>不确定与确定</h2><p>不确定的意思就是对于一个状态，读取下一个符号后，在不同的情况下可能跳转到不同的状态。以debug作类比的话，就是同一个程序，同样编译两次，但是两次的出错的地方都不同的感觉，做PA时就深感痛苦，同一个程序，有时出错，有时没事，有时出错的地方和上次不同….</p><p>一共有两点不同</p><ul><li><strong>不确定的自动机的边可以是空串$\varepsilon$</strong></li><li><strong>不确定自动机在一个状态下的多条边可以是同一个标记</strong></li></ul><p>第二点很好理解为什么是不确定的</p><img src="https://i.loli.net/2021/09/14/bgrVflPF8WCIHBQ.png" style="zoom: 25%;" /><p>对于状态S1，读取到同一个符号a，但是有可能跳转到不同的状态，这就是不确定</p><p>对于第一点，也很好理解，下图中，S1状态读取了a后，同样也是可以跳转到S3或者S4，这是因为边的符号可以是空串$\varepsilon$导致的</p><img src="https://i.loli.net/2021/09/14/GhHa57lrfJkA6SL.png" style="zoom:33%;" /><h2 id="不确定的有穷自动机"><a href="#不确定的有穷自动机" class="headerlink" title="不确定的有穷自动机"></a>不确定的有穷自动机</h2><p><img src="https://i.loli.net/2021/09/14/JqLdTOmXfjrZwos.png"></p><p><img src="https://i.loli.net/2021/09/14/LD9hX61QmVksedf.png"></p><h2 id="自动机对输入字符串的接受"><a href="#自动机对输入字符串的接受" class="headerlink" title="自动机对输入字符串的接受"></a>自动机对输入字符串的接受</h2><p>这里有一点需要注意的，对于不确定的自动机，只要存在一条从开始状态到接受状态的路径，就认为符号串可以被NFA接受。如字符串aabb，然后转换图如下</p><p><img src="https://i.loli.net/2021/09/14/JtswoRlOIfzrDiB.png"></p><p>aabb字符串显然可以到达接受状态3，但是也可能一直在0状态那里。我们不能认为有一条路没走到接受状态就认为aabb是不能被接受的串</p><h2 id="自动机与语言"><a href="#自动机与语言" class="headerlink" title="自动机与语言"></a>自动机与语言</h2><p>由一个NFA <em>A</em>定义（接受）的语言是从开始状态到某个接受状态的所有路径上的符号串集合，称为*L(A)*。</p><p><img src="https://i.loli.net/2021/09/14/AVdy1rkQgaPOFhT.png"></p><h2 id="确定有穷自动机"><a href="#确定有穷自动机" class="headerlink" title="确定有穷自动机"></a>确定有穷自动机</h2><p>正则表达式容易被人工翻译成不确定有穷自动机，但是确定有穷自动机才容易被计算机实现，因为这是一个step by step的过程，实现起来高效简洁</p><p><strong>每个NFA都有一个等价的DFA，即它们接受同样的语言</strong></p><p><img src="https://i.loli.net/2021/09/14/L8uirxAokmeI7Jn.png"></p><p>上图中，如果是NFA，move就是返回一个状态的集合而不是一个状态，因此就可能需要计算机并行地计算下一步，实现起来会很麻烦，但是如果是DFA，move仅仅返回一个状态给s，运行简单，所以一般词法分析器的模式识别最终是用DFA的</p><h1 id="正则表达式到自动机"><a href="#正则表达式到自动机" class="headerlink" title="正则表达式到自动机"></a>正则表达式到自动机</h1><p><strong>正则表达式可以简洁、精确地描述词法单元的模式但是在进行模式匹配时需要模拟DFA的执行。</strong></p><p>将正则表达式转换为DFA需要两步：</p><ul><li>正则表达式到NFA</li><li>NFA到DFA</li></ul><h2 id="NFA到DFA-子集构造法"><a href="#NFA到DFA-子集构造法" class="headerlink" title="NFA到DFA-子集构造法"></a>NFA到DFA-子集构造法</h2><p><strong>基本思想：</strong></p><ul><li>NFA的状态子集合并为DFA的一个状态</li><li>用$\varepsilon$-closure方法把NFA中的空串边消除</li></ul><p>理论上，最坏情况下DFA的状态个数会是NFA状态个数的指数多个。但是对于大部分应用，NFA和相应的DFA的状态数量大致相同。</p><p>操作：</p><p><img src="https://i.loli.net/2021/09/14/7qGFVWP1YaAkI2J.png"></p><p><img src="C:\Users\11056\AppData\Roaming\Typora\typora-user-images\image-20210914211606289.png" alt="image-20210914211606289"></p><p>对于$\varepsilon$​-closure,一般采用图搜索的方法，就是一种深度优先搜索</p><p><img src="https://i.loli.net/2021/09/14/bvDKr1z25UPF7Mh.png"></p><p><img src="https://i.loli.net/2021/09/14/xz7Iivrhjs3OY9e.png"></p><p>可以看到，有很多个NFA的状态合成了一个DFA状态，而且一个NFA状态可以出现在不同的DFA状态里，像1状态同时在B状态和C，D状态里</p><hr><p>未完待续</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Compile</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Direct Link Networks</title>
    <link href="/2021/09/07/Direct%20Link%20Networks/"/>
    <url>/2021/09/07/Direct%20Link%20Networks/</url>
    
    <content type="html"><![CDATA[<h2 id="链路层服务和帧"><a href="#链路层服务和帧" class="headerlink" title="链路层服务和帧"></a>链路层服务和帧</h2><ul><li><p>帧(Framing)：在每个网络层数据报经过链路传送前，几乎所有的链路层协议都要用链路层帧封装起来。一个帧由一个数据字段和若干首部字段组成，网络层的数据报就插在数据字段中。帧的结构由链路层协议决定</p><p>为了实现链路层的功能，不同的链路层协议会用不同的方法，由此会有不同的帧结构，下面还会继续说明，但是在这里简要看一下，HDLC协议为了链路层的差错检测功能采用循环冗余纠错码，而PPP则采用了Check Sum的方法，因此两个协议的帧结构是不同的</p></li></ul><img src="https://i.loli.net/2021/09/06/wzPeg7VHbB9XKOa.png" style="zoom: 67%;" /><p><img src="https://i.loli.net/2021/09/06/xNrRIto5CAlF3dh.png"></p><hr><ul><li><strong>链路接入</strong>，媒体控制访问（<strong>MAC</strong>）协议规定了帧在链路上传输的规则</li></ul><p>因为链路上面可能会有其他帧在传输而引起冲突等问题，所以需要规定帧的传输规则</p><p>对于链路的一端仅有一个发送方，另一端只有一个接收方的点对点链路，则MAC协议很简单，只要规定谁接受谁发送帧就可以，无论何时链路空闲就可以发送帧</p><p>但是大多数时候需要的都是多个节点共享单个广播链路的情况，MAC用于协调多个节点的帧运输</p><hr><ul><li><strong>可靠交付</strong></li></ul><p>当链路层协议提供可靠交付服务时，它保证无差错地经链路层移动每个网络层数据报</p><p>该服务主要通过<strong>确认和重传</strong>实现</p><hr><ul><li><strong>差错检测和纠正</strong></li></ul><p>链路层的差错检测和纠正与数字电路设计里面的类似，基本都是靠硬件实现的，但实际上计算机网络应用的差错检测和纠错不多，主要都是在硬件设计里面用的</p><hr><ul><li><strong>流量控制</strong></li></ul><p>流量控制表示在接收方接受到并处理帧之前，发送方被限制只能发送多少帧</p><p>流量控制用于让发送方能以较高的速率发送帧，并保证接收方能及时接受和处理接收到的帧</p><p>更具体地，因为接收方接受是通过读取接收方的缓冲区的数据来接收到信息的，但是其缓冲区的大小并不是无限大的，所以需要限制数据报进入缓冲区的速率，避免缓冲区溢出</p><h2 id="链路层的实现"><a href="#链路层的实现" class="headerlink" title="链路层的实现"></a>链路层的实现</h2><ul><li>链路层的主体就是网络适配器，通俗点就是网卡，该芯片实现了很多的链路层服务，包括成帧，链路接入，还有差错检测等</li><li>链路层的大部分是在硬件中实现，但是部分的链路层是在运行在主机CPU上，的软件中实现的，链路层的软件组件实现了高层的链路层功能，如组装链路层寻址信息和激活控制器硬件，而在接收方，链路层软件响应控制器终端，处理差错条件并将数据报向上传递给网络层，所以<strong>链路层是软件和硬件的结合体</strong></li></ul><p><img src="https://i.loli.net/2021/09/06/X1tcrnm5uGSM6Wf.png"></p><h2 id="差错检测和可靠传输"><a href="#差错检测和可靠传输" class="headerlink" title="差错检测和可靠传输"></a>差错检测和可靠传输</h2><p><img src="https://i.loli.net/2021/09/06/vVlXq3KRpWtYyIu.png"></p><ul><li>D是数据报本身</li><li>EDC是为了进行差错检测，链路层中所添加的纠错码</li><li>越长的EDC域会有更好的检测和纠错效果</li></ul><p>主要的差错检测和纠正有三种方法</p><ul><li>奇偶校验</li><li>循环冗余检测</li><li>检验和</li></ul><h2 id="控制流速"><a href="#控制流速" class="headerlink" title="控制流速"></a>控制流速</h2><p>方法：</p><ul><li>停止等待</li><li>滑动窗口</li></ul><h3 id="停止等待"><a href="#停止等待" class="headerlink" title="停止等待"></a>停止等待</h3><ul><li>源节点发送一个帧</li><li>目的节点接受帧，并返回ACK信号</li><li>源节点等待返回的ACK信号</li><li>若源节点收到返回的ACK信号，则继续发送新的帧</li><li>目的节点可以通过不发送ACK信号来停止流</li></ul><p>可以很容易看出来，这种方法效率很低，只适用于每个帧都比较大的情形</p><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><ul><li>这种方法允许源节点每次发送多个帧</li><li>接收方的缓冲区大小设为Win</li><li>发送方可以最多一次发送Win个帧，不需要接受到ACK信号</li><li>每个帧都有自己的编号</li><li>ACK信号包括下一个接收方希望要的帧</li></ul><p><img src="https://i.loli.net/2021/09/07/tRrO6PjTYGZEqgw.png"></p><p>如上图，窗口大小为7个帧，A第一次发送3个帧，然后B接受到3个帧后窗口向右移动到3号开始的帧，然后发送ACK信号给A，RR3表示下一个帧希望是3号帧，于是A再从3号帧开始发送4个帧给B，但是这次B只接受到了3号帧，因此ACK要求下一个帧是4号帧，于是A重新发送4，5，6帧给B，B接收到了就变成右下角的形式</p><h4 id="传输出错时的方案"><a href="#传输出错时的方案" class="headerlink" title="传输出错时的方案"></a>传输出错时的方案</h4><ul><li>Go Back N</li><li>选择拒绝</li></ul><p>实际上，一般都会用Go Back N的方案，因为选择拒绝的方案所需要的成本高，技术比较复杂</p><p>实现上，假设3号帧出错，但是发送方一开始发送了4个帧，2，3，4，5号帧，若采用Go Back N的方案，则需要从3号帧全部重传，也就是需要发送方重传3，4，5号帧；若采用选择拒绝的方案，则需要重新单独发送3号帧。</p><p>但是，因为一般来说，在帧传输的过程中，会因为外部未知因素干扰而导致帧出错，而一般干扰会持续一段时间，也就是说3号帧出错，由于持续的干扰的影响，后面的帧也会出错，因此我们一般会在出错帧后面也选择重传</p><h2 id="以太网"><a href="#以太网" class="headerlink" title="以太网"></a>以太网</h2><h3 id="多路访问链路"><a href="#多路访问链路" class="headerlink" title="多路访问链路"></a>多路访问链路</h3><p>在这节开始之前，上面的都是讨论单个发送方和单个接收方在一个链路上面的问题，而从现在就是要考虑协调多个发送和接收节点对一个共享广播信道的访问</p><p>多个发送和接受节点都连接到相同的，单一的，共享的广播信道上，实际生活中有很多，如WIFI，卫星信号传播，电缆接入网</p><p><img src="https://i.loli.net/2021/09/07/89Zzewo7fQl2Anx.png"></p><p>在这种链路上面，我们主要需要解决的问题就是<strong>冲突</strong>问题</p><h3 id="链路组织的拓扑结构"><a href="#链路组织的拓扑结构" class="headerlink" title="链路组织的拓扑结构"></a>链路组织的拓扑结构</h3><p><img src="https://i.loli.net/2021/09/07/Kh4ZkwAQNfbxYce.png"></p><p>在上面的Bus结构中，显然会出现数据冲突的问题，Tree结构则稍微减少了冲突的现象但是显然还是需要协议来解决冲突问题，Ring结构可以规定数据流的流动方向从而减少冲突的现象，最后Star结构比较复杂，但是基本解决了冲突问题</p><h3 id="信道划分协议"><a href="#信道划分协议" class="headerlink" title="信道划分协议"></a>信道划分协议</h3><p>假设一个支持N个节点的信道且信道的传输速率为R bps</p><h4 id="时分多路复用"><a href="#时分多路复用" class="headerlink" title="时分多路复用"></a>时分多路复用</h4><p>该方法的想法很简单，实际上跟操作系统里面的分时提供服务差不多，就是把时间划分为时间帧(time frame)，并进一步把每个时间帧划分为N个时隙(slot)，帧的概念就相当于一个周期，在每个帧内，规定一个可以发送数据分组的发送节点，每个节点在一个帧内都有发送数据分组的机会。</p><p>但是这个方法的缺点就是，每个节点被限制于R/N bps 的平均速率，即使当它是唯一有分组要发送的节点</p><p><img src="https://i.loli.net/2021/09/07/7jTqVuKhYesMiJQ.png"></p><h4 id="频分多路复用"><a href="#频分多路复用" class="headerlink" title="频分多路复用"></a>频分多路复用</h4><p>FDM将R bps信道划分为不同的频段（每个频段具有R/N的带宽），并把每个频率分配给N个节点中的一个，因此FDM在单个较大的R bps信道中创建了N个较小的R/N bps信道，公平的划分了带宽，但是和时分多路复用有同样的缺点，就是每个节点只能使用R/N的带宽</p><p><img src="https://i.loli.net/2021/09/07/4r6uWX2gjM1hmAH.png"></p><h4 id="码分多址（CMDA）"><a href="#码分多址（CMDA）" class="headerlink" title="码分多址（CMDA）"></a>码分多址（CMDA）</h4><p>在课本里面，用鸡尾酒会作类比，一个CMDA协议类似于让聚会客人使用多种语言来谈论，人们善于锁定他们能听懂的语言的谈话，而过滤其余的对话</p><p>定义时隙等于要发送1个比特所需要的时间，比特为0或1，但是这里把0比特视为-1</p><p>每一个时隙都设计一个对应的CDMA编码的比特，设第i个比特时隙中的数据比特值为$d_i$，对于$d_i$比特传输时间的第$m$个微时隙，CDMA编码器的输出$Z_{i,m}$是$d_i$乘以分配的CDMA编码的第$m$比特$c_m$:<br>$$<br>Z_{i,m}=d_i \cdot c_m<br>$$<br><img src="https://i.loli.net/2021/09/07/CNgVHsSjEI3bv6m.png"></p><p>CMDA的工作中有一个假设，就是对干扰的传输比特信号是加性的，也就是在第i个比特时隙的第m个微时隙期间，接收方所受到的是在那个微时隙中所有N个发送方传输的比特的总和：<br>$$<br>Z_{i,m}^* = \sum\limits_{s=1}^NZ_{i,m}^s<br>$$<br>如果仔细的选择CMDA的编码，每个接收方通过下面式子使用发送方的编码，就能从聚合的信号中恢复一个给定的发送方发送的数据<br>$$<br>d_i=\frac{1}{M}\sum\limits_{m=1}^M Z_{i,m}^*\cdot c_m<br>$$</p><h3 id="随机接入协议"><a href="#随机接入协议" class="headerlink" title="随机接入协议"></a>随机接入协议</h3><h4 id="ALOHA"><a href="#ALOHA" class="headerlink" title="ALOHA"></a>ALOHA</h4><ul><li><p>发送方</p><ul><li>只要有要发送的帧，就立即发送</li><li>发送方在一段很短的时间内侦听回复</li><li>如果收到ACK，则没问题，如果没收到ACK，将以概率p立即重传该帧，否则该节点等待一个帧传输时间</li><li>如果重传了几次之后都没有ACK，则放弃</li></ul></li><li><p>接收方</p><ul><li>进行差错检测</li><li>如果帧没问题，则发送ACK给发送方</li></ul></li></ul><p>帧可能会被冲突破坏，该方法的线路最大利用率是$18%$，超过$18%$就会可能出现死锁的情况，因为一个冲突可能会导致后面的多个冲突，就如生态系统中的正反馈一样，冲突的问题有可能越来越严重最后导致整条线路都无法传输帧</p><img src="https://i.loli.net/2021/09/07/queX72zZBvAlSft.png" style="zoom: 50%;" /><p>如上图2站点发出的2帧的冲突会导致后面的冲突，可想而知，一个冲突可能不是一个冲突的问题，而是会引发连锁的冲突</p><h4 id="Slot-ALOHA"><a href="#Slot-ALOHA" class="headerlink" title="Slot ALOHA"></a>Slot ALOHA</h4><p>跟上面的简单的ALOHA不同的就是，每个站点需要发送帧的时候，需要等到每个时隙的开始才能发送</p><p><img src="https://i.loli.net/2021/09/07/GFnod5D2SvtCiH6.png"></p><p>在这种方案下，线路的利用率是$37%$,比简单的ALOHA高了一倍</p><h4 id="Nonpersistent-CSMA"><a href="#Nonpersistent-CSMA" class="headerlink" title="Nonpersistent CSMA"></a>Nonpersistent CSMA</h4><p>发送站点会保持监听线路是否空闲</p><ul><li>如果处于空闲状态，则传输帧，否则到下面的步骤</li><li>如果处于繁忙状态，则等待一个随机的时间，然后到上面的步骤</li></ul><p>这种方法是一种谦让的方案，但是因为谦让可能会导致即使线路处于空闲状态，但是也有几个站点在等待发送</p><h4 id="1-persistent-CSMA"><a href="#1-persistent-CSMA" class="headerlink" title="1-persistent CSMA"></a>1-persistent CSMA</h4><p>发送站点会保持监听线路是否空闲</p><ul><li>如果处于空闲状态，则传输，否则到下面的步骤</li><li>如果线路繁忙，则持续监听直到线路空闲，然后立即传输帧</li></ul><p>这种方案是一种自私的方案，如果同时有两个或以上的站点在等待发送，则会导致冲突</p><h4 id="p-persistent-CSMA"><a href="#p-persistent-CSMA" class="headerlink" title="p-persistent CSMA"></a>p-persistent CSMA</h4><p>该方案尝试折中上述的两种方案</p><ul><li>像NonPersistent那样避免冲突</li><li>同时像1-persistent那样减少线路空闲时间</li></ul><p>规则：</p><ul><li>(1) 如果线路空闲，则以$p$的概率传输帧，以$(1-p)$的概率推迟一个时间单元</li><li>一般时间单元会设置为最大传输延迟</li><li>(2) 如果线路繁忙，则监听直至线路空闲，然后跳到(1)步骤</li><li>当传输推迟了一个时间单元，则跳到步骤(1)</li></ul><p>有理论表明，$p$的最优值是$1/N$ （N是在等待发送帧的站点）</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编译原理入门</title>
    <link href="/2021/08/31/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8/"/>
    <url>/2021/08/31/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="编译器和解释器"><a href="#编译器和解释器" class="headerlink" title="编译器和解释器"></a>编译器和解释器</h1><h2 id="编译器"><a href="#编译器" class="headerlink" title="编译器"></a>编译器</h2><ul><li>效率高，一次编译，多次运行</li><li>通常目标程序是可执行的</li></ul><p>一个源代码文件，比如C文件，或者py文件，其内容并不能被机器识别运行，因此需要先编译为汇编语言，再转化为机器语言，在windows上的目标程序（可执行程序）就是exe文件，编译器只会编译一遍，生成一个可执行文件，以后用户只需要执行可执行文件，不需要再编译就可以运行，与源程序再也没有关系</p><p><img src="https://i.loli.net/2021/08/31/UQGviYcNXPy3Sel.png"></p><h2 id="解释器"><a href="#解释器" class="headerlink" title="解释器"></a>解释器</h2><ul><li>直接利用用户提供的输入，执行源程序中指定的操作。</li><li>不生成目标程序，而是根据源程序的语义直接运行。</li><li>边解释，边执行，错误诊断效果好。</li></ul><p><img src="https://i.loli.net/2021/08/31/7n9x4MAhIXFGkaQ.png"></p><p>其实仍然需要生成可执行的代码才能运行，但是是逐条转换，而且不会保留可执行的代码，因此下一次再有程序输入时仍然需要解释器再重新编译源程序</p><h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><p>如果举生活中的例子，编译器就相当于翻译一本英文书为中文书，只需要有一个人翻译出来，有了中文译本，用户就可以一直看中文译本而不需要再去看英文本了。解释器就相当于有一个人逐条翻译英文本给用户，用户可以一页页听着翻译来看完整本书，但是看完整本书后，不会留下译本，下次需要再看这本书的时候，还需要译者去逐条逐条地翻译才能看懂</p><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><p>Java的语言处理器结合了编译和解释的过程，一个Java源程序首先被编译成一个字节码(bytecode)的中间表示形式，然后由一个虚拟机对得到的字节码加以解释执行。这样会比每次都重新编译源程序要快，又能有解释的易进行错误诊断的效果</p><p>好处是再一台机器上编译得到的字节码可以再另一台机器上解释执行</p><p><img src="https://i.loli.net/2021/08/31/vQWGT6JYimoR4XA.png"></p><h1 id="从源程序到机器代码"><a href="#从源程序到机器代码" class="headerlink" title="从源程序到机器代码"></a>从源程序到机器代码</h1><p>这一部分是在计算机系统基础中的知识，在这里再复习下</p><ul><li>源程序$\to$ 预处理器$\to$​ 经过预处理的源程序（hello.c $\to$​ hello.i）</li></ul><p>预处理，比如宏定义的转换</p><ul><li>经过预处理的源程序$\to$ 编译器$\to$ 目标汇编程序 （helloc.i$\to$ hello.s）</li></ul><p>把高级语言的程序翻译为汇编语言程序</p><ul><li>目标汇编程序$\to$汇编器$\to$ 可重定位机器代码（hello.s $\to$ hello.o）</li></ul><p>生成可重定位的机器代码，也就是还没有经过链接的过程，变量的地址还不知道</p><ul><li>可重定位机器代码$\to$ 链接器/加载器$\to$ 目标机器代码（hello.o$\to$ hello.exe）</li></ul><p>链接库文件和可重定位对象文件，生成机器可以运行的机器代码，不同的操作系统的可执行文件不同</p><h1 id="编译器的结构"><a href="#编译器的结构" class="headerlink" title="编译器的结构"></a>编译器的结构</h1><h2 id="分析部分"><a href="#分析部分" class="headerlink" title="分析部分"></a>分析部分</h2><ul><li>把源程序分解为多个组成要素，并得到语法结构，用该语法结构创建源程序的一个中间表示</li><li>搜集源程序中的相关信息，放入符号表</li><li>分析，定位程序中可能存在的语法，语义错误</li><li>又称为编译器的前端，是与机器无关的部分</li></ul><h2 id="综合部分"><a href="#综合部分" class="headerlink" title="综合部分"></a>综合部分</h2><ul><li>根据符号表和中间表示构造目标程序</li><li>又称编译器的后端，是与机器相关的部分</li></ul><h1 id="编译器中的步骤"><a href="#编译器中的步骤" class="headerlink" title="编译器中的步骤"></a>编译器中的步骤</h1><p><img src="https://i.loli.net/2021/08/31/QbvJsqdoTrMU9uN.jpg"></p><p>除了上面标出来的，还有一个贯穿整个编译过程的东西，<strong>符号表</strong>，用于存放源程序的相关信息，可由各个步骤使用</p><p>下面的过程的例子将用一个简单的源程序语句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">position = initial + rate * <span class="hljs-number">60</span><br></code></pre></td></tr></table></figure><h2 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h2><p>编译器的第一个步骤是词法分析(lexical analysis)，它读入组成源程序的字符流，并将他们组织成有意义的词素的序列（词素是语言学中能表达意思的最小单位，在英语中是单词，在汉语中则是汉字）</p><p>对于每个词素，词法分析器产生如下的<strong>词法单元</strong>作为输出:</p><p>$&lt;token-name,attribute-value&gt;$​</p><p>其中token-name是一个抽象符号，相同类型的词素会使用相同的抽象符号</p><p>而attribute-value指向符号表中关于这个词法单元的条目，也就是相当于一个指针</p><p>该词法单元会传递给下一个步骤，也就是语法分析</p><ul><li>position，initial，rate，=，+，*，60 都是词素</li></ul><table><thead><tr><th>词素</th><th>词法单元</th></tr></thead><tbody><tr><td>position</td><td>&lt;id, 1&gt;</td></tr><tr><td>initial</td><td>&lt;id, 2&gt;</td></tr><tr><td>rate</td><td>&lt;id, 3&gt;</td></tr><tr><td>=</td><td>&lt;=&gt;</td></tr><tr><td>+</td><td>&lt;+&gt;</td></tr><tr><td>*</td><td>&lt;*&gt;</td></tr><tr><td>60</td><td>&lt;60&gt;</td></tr></tbody></table><p>字符流变为符号流<br>$$<br>&lt;id, 1&gt;&lt;=&gt;&lt;id,2&gt;&lt;+&gt;&lt;id,3&gt;&lt;*&gt;&lt;60&gt;<br>$$</p><h2 id="语法分析"><a href="#语法分析" class="headerlink" title="语法分析"></a>语法分析</h2><p><img src="https://i.loli.net/2021/08/31/D3zLGAaEYq4XQFu.png"></p><ul><li>生成语法树</li><li>指出了词法单元流的语法结构</li></ul><h2 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h2><p><img src="https://i.loli.net/2021/08/31/oNkTWFrRZ8pgeJq.png"></p><ul><li>得到语义，对于编译器来说比较难</li><li>语义分析<ul><li>使用语法树和符号表中的信息，检查源程序是否满足语言定义的语义约束（比如，在C语言中，当&lt;id,3&gt;与60相乘，会检查&lt;id, 3&gt;是否为定义的数字类型，否则报错）</li><li>同时收集类型信息，用于代码生成</li><li>类型检查，类型转换（如果int类型与float类型相乘，就需要先把int扩展为float类型）</li></ul></li></ul><h3 id="语义分析的例子"><a href="#语义分析的例子" class="headerlink" title="语义分析的例子"></a>语义分析的例子</h3><p>自然语言中代词的分析</p><ul><li>Jack said Jerry left his assignment at home.</li><li>Jack said Jack left his assignment at home?</li></ul><p>第一句中的his指代的his是Jerry，人很容易看出来，但是怎么能让机器看出来</p><p>第二句中两个Jack是不是同一个人，his又是指代的哪一个Jack</p><p>而在程序设计中，也会出现类似的情况</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C">&#123;<br>   <span class="hljs-keyword">int</span> Jack = <span class="hljs-number">3</span>;<br>   &#123;<br>      <span class="hljs-keyword">int</span> Jack = <span class="hljs-number">4</span>;<br>      <span class="hljs-built_in">cout</span> &lt;&lt; Jack;<br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>当中的Jack应该是哪一个值，机器需要怎么识别</p><h2 id="中间代码生成"><a href="#中间代码生成" class="headerlink" title="中间代码生成"></a>中间代码生成</h2><p><img src="https://i.loli.net/2021/08/31/Pnm3iFry4QZXJl5.png"></p><p>变成三地址代码：也就是每个指令最多包含3个运算分量</p><h2 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h2><p><img src="https://i.loli.net/2021/08/31/38eCmqhBGOVQUYL.png"></p><p>改进中间代码，使用更少行代码，运行的更快，占用更少内存</p><h2 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h2><p><img src="https://i.loli.net/2021/08/31/teQPw8fIJUOTNGd.png"></p><p>把中间表示形式映射到目标语言</p><ul><li>寄存器的分配</li><li>指令选择</li><li>内存分配</li></ul><h1 id="编译器的趟-Pass"><a href="#编译器的趟-Pass" class="headerlink" title="编译器的趟(Pass)"></a>编译器的趟(Pass)</h1><p>在一个特定的实现中，上面的多个步骤的活动可以被组合成一趟。每趟读入一个输入文件并产生一个输出文件</p><p>比如，前端步骤中的词法分析，语法分析，语义分析，以及中间代码生成可以被组合在一起成为一趟，然后可以有一个为特定目标机生成代码的后端趟</p><h1 id="人工智能编译器"><a href="#人工智能编译器" class="headerlink" title="人工智能编译器"></a>人工智能编译器</h1><p><img src="https://i.loli.net/2021/08/31/JZoOyiQK83mYatL.png"></p><p>现在有很多很火的人工智能深度学习框架，像是Pytorch和Keras，它们是不同的框架，但是最终都要运行在机器上，可以是CPU，也可以用GPU，TPU，如何让这些深度学习框架更快地运行在机器上面，有一部分就是编译器的工作</p><blockquote><p><strong>首先，由于前端接口和后端实现之间的差异，从一个人工智能框架切换到另一个AI框架是非常重要的</strong>。此外，算法开发人员可能会使用多个框架作为开发和交付流程的一部分。在AWS上，我们有客户希望在MXNet上部署他们的Caffe模型，以享受Amazon EC2上的高速性能。Joaquin Candela最近在博客中写道，用户可能会使用PyTorch进行快速开发，然后部署在Caffe2上。然而，在将模型从一个框架转换到另一个框架后，有人抱怨二者的调试结果存在差异。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Compile</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>表义文字的魅力</title>
    <link href="/2021/08/30/%E8%A1%A8%E4%B9%89%E6%96%87%E5%AD%97%E7%9A%84%E9%AD%85%E5%8A%9B/"/>
    <url>/2021/08/30/%E8%A1%A8%E4%B9%89%E6%96%87%E5%AD%97%E7%9A%84%E9%AD%85%E5%8A%9B/</url>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/08/30/ueUXYyxVpCNMrm7.png"></p><p>只是作为一个外行人，来谈谈自己对表义文字的看法</p><p>世界上的语言文字可以分为两种，表音文字与表义文字。而从它们的名字也可以立即知道两种文字的区别，前者的语言仅仅表示发音，世界上除了汉语外的文字全都是表音文字，如英语，德语，法语等等。而汉语，作为唯一的表义文字，现在仅仅在中国和日本，新加坡等地方使用，而只有中国以汉语作为国语全面使用的国家。我们在中国一直都在享受着汉语给我们带来的好处，但是我们自身却没有察觉，而且认为很多都是理所当然的。语言，是有表达效率的高低之分的。但是大多数人都仅仅限制在汉语和英语里面，没有接触过其他语言，所以很难知道汉语的表达力有多强</p><p>表义文字，顾名思义，就是每个字都有自己本身的意思。为了更好说明，可以与英语做对比，英语的文字是没有意义的。一个单个的字母不会有任何含义，而就算把一个单词拿出来，这个单词有它本身的意义，但是基本不会再用这个单词的本意来组合出新的单词而保留其原来的意思。比如说【色】这个字，在英文中，应该是【color】，而在表义文字中，我们而已用【色】这个字本身的意思来组合出新的单词【色素】而保留其原来的意思，而在英文中的色素则是【pigment】，一个和color完全没有关的单词。</p><p>从上面的例子就可以很容易看出来，汉语，只要学会了最基础的几千个字，就可以用这几千个字去组合出成千上万的新词，而这些新词尽管是第一次看到，也能通过文字的原意来大概知道意思，这就是作为表义文字的汉语的效率之高。这作用在我们的学习研究尤为明显。在初中开始，我们就需要学习很多的领域相关的词语了，像上面提到的色素。还有【叶绿体】，我们即便从来没有学到过这个知识，但却也能对其有一个大概的绿色的叶子的印象，而在英文中则是【chloroplast】，对于以英语为母语的人来说，刚接触到这个概念的时候应该是完全不知道这个是什么东西的吧。相比于英文，或许中文更适合于教学，只可惜中国的科学方面的水平不如外国，只能靠中国的科学家把外国的难以理解的词汇，都逐一找到对应的中文意思的汉字来翻译出来，让国人得以受益。</p><p>表义文字的效率还体现在可读性上面。在日语中，尽管他们有自己的文字，但是却仍然一直保留着汉字。原因有二，一是日语是不作间隔的，与英文不一样，日语的每个单词之间并没有空格隔开，所以如果没有汉字，日语就会相当于英语而且单词之间没有空格的情况，比如</p><p>IamJohnwhataboutyou</p><p>わたしはたなかですがおなまえは</p><p>可读性是很差的，因此日语的很多单词都会把汉字放在单词的开头，然后以他们的固有字结尾，比如「素晴らしい」「美しい」</p><p>これは|本当に|綺麗な|花|ですね</p><p>通过汉字，很容易就可以把句子断开，可读性大大提高。二则是汉字的表达能力强，只需要看到文章中的汉字，人们就大概能知道整篇文章说的是什么，这是表音文字不可能做到的。 </p><p>因为表义文字的存在，也让我们发展出来了自己的起名文化。在外国，因为只有表音文字，所以对于人名，一般都是用圣经里面的圣人的名字，像是什么John，James，实际上都是没有实际含义的，然后到Johnson，Robinson，实际上是用刚才提到的圣人的名字加上son，也就是儿子的意思，表示约翰的儿子…。人如其名这个成语也应该只能存在于汉语存在的地方吧。中国的名字，一般都能看出来其父母对儿女日后作为或性格的期许，像是【伟】【立名】【怡宁】之类，都是可以看出来的。而在一些已经弃用汉字的国家（韩国），都会在身份证件上面使用汉字来书写自己的名字，虽然流传的说法是韩国的同音异义词太多，如果用韩文的话会太多重名，所以会使用汉字来写，但或许他们也感受到了汉字的魅力才会继续沿用下去吧。</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Chinese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>N2之路</title>
    <link href="/2021/08/27/N2%E4%B9%8B%E8%B7%AF/"/>
    <url>/2021/08/27/N2%E4%B9%8B%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<p>在大二的暑假准备结束时，闲来无事，突然想写写自己的日语学习之路，不只是备考方面的经历，也想写一下中途的心路历程</p><p>现在想想学日语的初心，当时就是想看番能自己看懂不用字幕。大概这也是很多人的初心吧，隔着语言的墙壁始终觉得看番不够感觉，于是就在大一的寒假的时候开始了日语学习之路。想来那是多简单纯粹的初心，当时确实是单纯的热爱吧。对二次元世界的热爱让我有了学习日语的动力，这个动力也持续了很久，让我坚持学习了很久。</p><p>刚开始学习日语的时候，基本只认得汉字，看到片假名和平假名都觉得像看外星语一样，完全是看不懂的，感觉很隔阂，特别是片假名。我还记得第一个学的片假名单词是テレビ，现在看到这单词一看就知道意思和读音了，但是当时记这个单词的时候并没有那么容易，按着五十音的发音，逐个假名这样查，逐个记住，然后终于记住了第一个片假名单词。</p><p>慢慢地，寒假也学完了几篇课文，学了一些最基础的单词。那时候是反馈比较明显的时候，因为单词比较接近生活，也有很多明显日本特色的单词，像是什么本当に、けれど、せっかく、でも、しかし、あります、なんでもない之类的，很多都在番里面或者歌曲里面出现过，于是第一次“看到”，而不是“听到”，就会感觉很新鲜，觉得终于知道平时经常听到的单词或者短句是什么意思，会比较有成就感吧。那时候发现像是通常是连词，句尾语气词，动词之类的比较容易记住，很多都是在看番的时候看字幕就知道了这个发音对应的中文意思，因为通常这些词都比较独立，能表达独立的意思，翻译成中文也能很明显的分隔地看出来，但是像是名词通常是出现在句子的中间，看番即使一直有字幕，听过几遍，也很难直接把这个读音和意思联系起来，因为在这种情况，我们即使听到了这个发音，因为没有学过，大脑无法把这单词与单词附着的短语或者是其他语气词之类的分割开来，就会自动过滤掉，也就是说听到也反应不过来自己听到了，于是就相当于没听到过这个单词了，所以这种单词如果没“看到”过的话，是基本不可能学会的。而在那个时候，会学到这种单词，然后惊觉这个单词在哪个地方出现过。我印象比较深刻的就是「入り口　いりぐち」了，当我在课文中看到，并记住了这个单词的发音的时候，当我有一次不自觉唱起【相遇天使】，才发现这个单词在这首歌里面出现过，但是以前一直都是单听到什么就唱出来，完全不知道这个句子或者单词的意思</p><p><strong>明日の入り口に</strong></p><p><strong>置いてかなくちゃいけないのかな</strong></p><p>这种感觉很奇妙，会让学的时候更有动力，不断地学习，然后得到反馈，然后继续学习</p><p>逐渐的，很多歌的意思都能看懂，听懂了。有些以前只是觉得旋律好听的歌，细看一下歌词，更加惊叹于这首歌，这种感觉就像是高中的时候回看小学时候喜欢听的周杰伦和许嵩的歌，以前只是喜欢旋律，但是有了自己的经历，看过很多书，能明白很多艺术手法之后，才发现他们的歌不仅是旋律厉害，这歌词也是一流的。</p><p>背单词，学课文，过完大一的第二学期，标日初级也基本学完了，当时候就开始想更进一步地开发学习的乐趣，于是进军二次元轻小说了，在kindle上面看完了几本动漫化的轻小说，也学会了很多单词，也在轻小说里面巩固了自己的句节的分解的能力，说到底还是语法基础，但是那时候开始，学习日语的反馈没有一开始那么明显了，学来学去都是很多单词看不懂，开始觉得有点无聊，热情也开始消减。到大二时，简单的文章基本都能看懂，但是复杂的文章看不懂，也不想硬啃，因为啃完所有单词，单词也大多都不会出现在日常生活中，所以就算背完一次，没有后续的巩固，也很快会忘掉，和一开始学的简单的日常生活的单词不同，学完那些单词也很难在番剧或者歌曲里面得到反馈，很多都是低频词。感觉进入了瓶颈期，但是还是尽力啃完了标日中级两册。那时候单看课文也越来越枯燥。于是开始上网看youtube，twitter，看到真实的日本，发现真实的日本并没有多么美好，对我们中国的情感也是很奇怪的一致的厌恶。开始慢慢不知道为什么一开始要学日语，难道学习日语就是为了看这些吗。但是我过了一会儿还是不信邪，我还是觉得哪里都有脑残，还是没有放弃学日语。虽然看到了很多不好的东西，但是也看到了很多美好的东西，只要不涉及政治，很多时候看日本还是很美好的，有很多美的音乐，有很多精彩的动漫，有好看的樱花，有精美的和服，不得不说，他们的文化输出实在是太厉害了。</p><p>在那段时间，我看到了很多负面的东西，但同时也会看到美好的东西。越来越深刻地感受到一些日语单词的含义，像是「しあわせ」「辿り着いた」「あたたかい」「ひかり」「うれしい」「ほしぞら」「はなび」「いのり」。虽然这些单词都有对应的汉字，但是尽管只是写成表音文字的形式，我也能体会到这单词背后的意思，同时在我的脑海中这发音也与单词的美好本意紧紧地联系在了一起。小小的一个反馈，但也能支持我继续学习下去</p><p><img src="https://i.loli.net/2021/08/27/sWkiVBHw6lRtTY8.png"></p><p>尽管在接触日本文化后，我就大概知道日本的社会的压抑感，但是我还是想去看落樱，想去感受京都的唐朝留下来的文化风貌，想去看烟火大会，想去祭り</p><p><img src="https://i.loli.net/2021/08/27/oFGnQtBe6uXmClP.png"></p><p>一路下来，也坚持了不久了。于是想去日本留学。当时候看到学校有交流项目，如果我想大三去日本交流的话，就要在今年的7月份把N2考了，于是21年的寒假就开始复习备考了，整个寒假都是在学日语（最后在练车），每天6，7个小时，感觉算是高考之后少有的高强度学习了，每天红宝书，蓝宝书，一个单元单词，一个单元语法，WPS，OneNote，各种软件都用上。</p><p><img src="https://i.loli.net/2021/08/27/OqfaWgpQTivUYVR.png">现在想想自己的毅力还是有点强的，中间也会觉得累，但是想到是自己喜欢，自己热爱的东西，也没感觉什么。记得当时看BangDream，看到女主她们站到武道馆上时，唱着</p><p><strong>同じ景色見つめながら</strong></p><p><strong>走り続けるトレイン</strong></p><p><strong>わたしたちを連れていく</strong></p><p>感觉自己就是那样，大多数时间都是重复着做着同样的事，每天背单词，背语法，反馈不是很明显。但是自己知道自己正在慢慢进步，总会有「夢を打ち抜く」的时候。那时应该就是考完试的时候吧，当时会想</p><p><strong>夢を撃ち抜く瞬間に、君は何を思うの</strong></p><p>想知道自己考过N2时的心情，是一种怎么样的心情来见证自己的追求的喜欢的事成功的一刻的。</p><p>现在到了这个时候，却又挺平淡的。是一种努力过，然后顺其自然的感觉，知道自己肯定能过N2，所以当知道自己过了的时候就没有什么激动的感觉了。这可能就是</p><p><strong>到得还来别无事，庐山烟雨浙江潮</strong></p><p>倒也不是什么大事，只是努力过程中的一个阶段的结束的标记，路还很长，路上的风光更加令人感动</p><p><img src="https://i.loli.net/2021/08/27/iAZFWjvz96D5BXm.png"></p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习linux</title>
    <link href="/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0linux/"/>
    <url>/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0linux/</url>
    
    <content type="html"><![CDATA[<h2 id="服务器后台训练"><a href="#服务器后台训练" class="headerlink" title="服务器后台训练"></a>服务器后台训练</h2><h3 id="nohup-amp"><a href="#nohup-amp" class="headerlink" title="nohup + &amp;"></a>nohup + &amp;</h3><p>用nohup和&amp;一起，即可使得进程在服务器的后台运行</p><p>如果只用nohup，则ctrl+c会使得进程结束</p><p>如果只用&amp;，则关闭终端会使得进程结束</p><p>一般最简单的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup [Command] &amp;<br></code></pre></td></tr></table></figure><p>可以让程序在后台运行而不会因终端关闭或者断网而被终止，进程的输出会被默认重定向到nohup.out文件中</p><p>比如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup python trainer.py &amp;<br></code></pre></td></tr></table></figure><p>运行trainer文件的输出会被重定向到当前目录下面的nohup.out文件中</p><p>而如果希望输出重定向到人为设定的文件中，则用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup [Command] &gt; [output file] 2&gt;&amp;1 &amp; <br></code></pre></td></tr></table></figure><p>在上面的例子中，0 – stdin (standard input)，1 – stdout (standard output)，2 – stderr (standard error) ；</p><p>2&gt;&amp;1是将标准错误（2）重定向到标准输出（&amp;1），标准输出（&amp;1）再被重定向输入到myout.file文件中。</p><p>比如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup python trainer.py &gt; myoutput.txt 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>如果不想启用缓存，也就是直接把输出放到log中的话，那就加上参数u</p><p><strong>注意一定要加参数u，本人因为没加参数u直接白跑两次</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup python -u trainer.py &gt; myoutput.txt 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><h3 id="查看进程-显卡"><a href="#查看进程-显卡" class="headerlink" title="查看进程/显卡"></a>查看进程/显卡</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nvidia-smi<br></code></pre></td></tr></table></figure><p>如果没有进程在运行，那么输入完上面的命令会大概出现下面的样子</p><p><img src="https://i.loli.net/2021/08/25/lTtvkeRiQmU32Z8.png"></p><p>如果有进程在显卡上面运行，留意Processes就会看到进程</p><p><img src="https://i.loli.net/2021/08/25/rcufgobOGQVR1NC.png"></p><p>除了用查看显卡的方法查看进程，还可以使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">jobs -l<br></code></pre></td></tr></table></figure><p>但是jobs命令只能查看在自己终端上面运行的进程，不能看到服务器上其他的进程</p><p>如果想看到服务器上的其他运行的进程，可以用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ps -aux<br></code></pre></td></tr></table></figure><p>1）-a 显示现行终端机下的所有程序，包括其他用户的程序</p><p>2）-u 用户为主的格式来显示程序状况</p><p>3）-x 显示所有程序，不以终端机来区分</p><h3 id="终止进程"><a href="#终止进程" class="headerlink" title="终止进程"></a>终止进程</h3><p>如果使用了nohup+&amp;，就不能通过ctrl+c或者关闭终端来终止进程了，那么我们希望结束一个进程的时候，就需要使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kill -9 [PID]<br></code></pre></td></tr></table></figure><p>比如：如果我知道我要结束的进程的进程号是32580，则只需要像下面这样</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kill -9 32580<br></code></pre></td></tr></table></figure><h2 id="显卡设置"><a href="#显卡设置" class="headerlink" title="显卡设置"></a>显卡设置</h2><p>用服务器运行代码的目的一是本地运行的内存太小，基本上大一点的机器学习任务中途都会被linux操作系统kill掉，二是服务器上能用显卡运行程序，显卡深度学习的速度比CPU的速度要快，因此我们通常会使用服务器来运行深度学习训练</p><p>通常有两种方法可以设置在哪个显卡上面运行</p><h3 id="py文件"><a href="#py文件" class="headerlink" title="py文件"></a>py文件</h3><p>在要运行的py文件的前面几行，加入下面代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;0,1&quot;</span><br></code></pre></td></tr></table></figure><p>其中CUDA是加速深度学习的框架，需要自行到网上下载安装到linux中，一般的深度学习框架都可以用CUDA加速，像是Tensorflow和Pytorch</p><p>而0，1则是选择要运行的显卡，可以用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nvidia-smi<br></code></pre></td></tr></table></figure><p>查看空闲的显卡，然后就用空闲的显卡运行程序</p><h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><p>还可以在终端设置运行的显卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">CUDA_VISIBLE_DEVICES=3,4,1 python train.py<br></code></pre></td></tr></table></figure><p>但是这样就无法后台运行程序了，因为如果我们加上nohup</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup CUDA_VISIBLE_DEVICES=3,4,1 python train.py &amp;<br></code></pre></td></tr></table></figure><p>就会报错指令不对</p><p>因此最好的方法还是在py文件中添加os.environ设置显卡</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Skip-Gram</title>
    <link href="/2021/08/20/Skip-Gram/"/>
    <url>/2021/08/20/Skip-Gram/</url>
    
    <content type="html"><![CDATA[<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul><li><p>给定一个中心单词centerWord，通过该算法可以预测出最有可能在该中心单词附近的单词context Words</p></li><li><p>当训练集较小的时候，也能有很高的准确率，对比较少见的单词或短语的效果预测效果较好</p></li></ul><hr><ul><li><p>需要注意的是，<strong>Skip-Gram算法的最终目标并不是用于预测context Words，而是用于给出一个比较好的单词的向量表示</strong>，这也是为什么Word2Vec经常会和Skip-Gram一起出现的原因</p></li><li><p>用Skip-Gram算法得到的单词向量Word Vectors可以用于寻找related words，还可以用于 Analogy tasks</p></li></ul><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>Skip-Gram可以理解为一个<strong>神经网络</strong>的模型</p><p><img src="https://i.loli.net/2021/08/20/toOmJdU9CMyKbrF.png"></p><h3 id="输入-input"><a href="#输入-input" class="headerlink" title="输入 input"></a>输入 input</h3><p>Skip-Gram的输入为单词的one-hot编码的向量</p><h3 id="输入到隐层-W-input"><a href="#输入到隐层-W-input" class="headerlink" title="输入到隐层  $W_{input}$"></a>输入到隐层  $W_{input}$</h3><p>当隐层的维数为N时，$W_{imput}$为$V\times N$的矩阵，该矩阵不是普通的矩阵，可以理解为每个单词的单词向量，如图中，矩阵的每一行为一个3维的向量，因此在这里，单词空间为3维空间，每个单词的Word Vector为一个3维的向量。<strong>这是因为输入为one-hot编码的向量，因此在这里每一行就可以看作一个单词向量</strong></p><h3 id="隐层-hidden"><a href="#隐层-hidden" class="headerlink" title="隐层    $hidden$"></a>隐层    $hidden$</h3><p>隐层的N维向量就可以看作是单词的Word Vector,其实也就是$W_{input}$对应的一行</p><h3 id="隐层到输出层-W-output"><a href="#隐层到输出层-W-output" class="headerlink" title="隐层到输出层    $W_{output}$"></a>隐层到输出层    $W_{output}$</h3><p>实际上$W_{output}$​矩阵也是一个单词向量的编码，每一列是一个N维的单词向量Word Vector。但是这里的单词向量编码和前面的单词向量的编码是不同的，也就是说每个单词都有两种Word Vector表示：</p><ul><li><strong>当单词是center Word时，该单词的向量表示是$W_{input}$​​中的一行</strong></li><li><strong>当单词是context Word时，该单词的向量表示是$W_{output}$​中的一列</strong></li></ul><p>centerWord Vectors是输入到隐层的矩阵</p><p>outsideWord Vectors是隐层到输出层的矩阵（context Words的矩阵）</p><p>对于这里的理解，要<strong>结合softmax函数预测的概率来理解</strong></p><p><img src="https://i.loli.net/2021/08/20/VCHSsgFDORTzAXq.png"></p><p>虽然是矩阵表示$W_{output}$，但是其原理跟BP神经网络并不一样，也就是该矩阵不是单单跟隐藏层的向量相乘得到输出层的输入，而是用上面的softmax函数，用该矩阵与隐藏层一起作为softmax函数的输入来进行预测</p><h3 id="输出层-y-pred"><a href="#输出层-y-pred" class="headerlink" title="输出层 $y_{pred}$"></a>输出层 $y_{pred}$</h3><p>输出层就如上面所述，使用softmax函数作为激活函数，输出一个$V$维的向量</p><p>训练时用于与真实的one-hot向量作比较，得到误差</p><p>测试时该$V$维向量的最大的一个维度就是对应的所预测的向量</p><h2 id="训练-train"><a href="#训练-train" class="headerlink" title="训练 train"></a>训练 train</h2><p>我们需要确定机器学习的目标函数，也就是loss。机器学习的目的就是在测试集上达到loss的最小化，而训练的方向就是在训练集上达到loss的最小化，认为在训练集上达到loss最小化的模型在测试集上面也能达到一个比较好的效果。</p><p>首先，我们需要训练的参数就是$\theta=(W_{input},W_{output})$​​,分别表示中心单词的矩阵和文本单词的矩阵<br>$$<br>loss = J(\theta)=-\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>其中$T$​为整个语料库的大小，$w$​为设置的窗口的大小</p><p>显然当context Word出现在center Word附近的时候，我们需要让它的条件概率最大化，当上面loss最小化的时候，就达到了条件概率的最大化</p><p>整个过程就是遍历整个语料库，然后对每一个语料库中遍历到的单词$W_t$，计算<br>$$<br>J(\theta,W_t) = \sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>而对于每一个给定的context word$W_{t+j}$,显然所需要计算的就是<br>$$<br>\log(P(W_{t+j}|W_t;\theta))<br>$$<br>对于概率$P(W_{t+j}|W_t;\theta)$的计算，我们暂时有两种计算方法</p><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>把$W_{t+j}$也就是context word记为$u_o$​, 就是$\theta$的$W_{output}$中的一列单词向量</p><p>把$W_t$也就是center word记为$v_c$，就是$\theta$的$W_{input}$中的一行单词向量​</p><p>用softmax的公式<br>$$<br>P(W_{t+j}|W_t)=\frac{exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)}<br>$$<br>不难看出，这种方法概率的最后的和为1，也就是$\sum\limits_{i=1}^TP(Wi|W_t)=1$​</p><p>根据该损失函数，用梯度下降法，计算$\frac{\partial J}{\partial \theta}$<br>$$<br>\frac{\partial }{\partial v_c}(\frac{exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)})=W_{output}(y_{pred}-y_{true})<br>$$<br>对于$W_{input}$矩阵，我们只需要更新当前的中心单词的向量，因为其他单词向量的中心单词编码形式并没有出现在当前的损失函数中，因此并不会有其他单词向量的梯度更新</p><p>这里的$y_{true},y_{pred}$​都是V维的向量，</p><ul><li><p>$y_{pred}$​的每一个维度表示<strong>每一个单词的是当前中心单词的文本单词的概率</strong></p></li><li><p>$y_{true}$​​​​是语料库单词的one-hot编码</p></li></ul><p>$$<br>\frac{\partial}{\partial W_{output}}(\frac{\exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)})=v_c(y_{pred}-y_{true})^T<br>$$</p><p>$$<br>y_{pred}=softmax(W_{output}\cdot v_c )<br>$$</p><p>对于$W_{output}$矩阵，我们就需要对整个矩阵进行更新了，因为在softmax函数中，可以看到语料库中的所有单词都是出现在了该函数的分母中的，因此需要对整个矩阵求梯度并更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">naiveSoftmaxLossAndGradient</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">    centerWordVec,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideWordIdx,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideVectors,</span></span><br><span class="hljs-params"><span class="hljs-function">    dataset</span></span><br><span class="hljs-params"><span class="hljs-function"></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot; Naive Softmax loss &amp; gradient function for word2vec models</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the naive softmax loss and gradients between a center word&#x27;s </span><br><span class="hljs-string">    embedding and an outside word&#x27;s embedding. This will be the building block</span><br><span class="hljs-string">    for our word2vec models. For those unfamiliar with numpy notation, note </span><br><span class="hljs-string">    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which</span><br><span class="hljs-string">    you can effectively treat as a vector with length x.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    centerWordVec -- numpy ndarray, center word&#x27;s embedding</span><br><span class="hljs-string">                    in shape (word vector length, )</span><br><span class="hljs-string">                    (v_c in the pdf handout)</span><br><span class="hljs-string">    outsideWordIdx -- integer, the index of the outside word</span><br><span class="hljs-string">                    (o of u_o in the pdf handout)</span><br><span class="hljs-string">    outsideVectors -- outside vectors is</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    for all words in vocab (tranpose of U in the pdf handout)</span><br><span class="hljs-string">    dataset -- needed for negative sampling, unused here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">    loss -- naive softmax loss</span><br><span class="hljs-string">    gradCenterVec -- the gradient with respect to the center word vector</span><br><span class="hljs-string">                     in shape (word vector length, )</span><br><span class="hljs-string">                     (dJ / dv_c in the pdf handout)</span><br><span class="hljs-string">    gradOutsideVecs -- the gradient with respect to all the outside word vectors</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    (dJ / dU)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">### YOUR CODE HERE (~6-8 Lines)</span><br><br>    <span class="hljs-comment">### Please use the provided softmax function (imported earlier in this file)</span><br>    <span class="hljs-comment">### This numerically stable implementation helps you avoid issues pertaining</span><br>    <span class="hljs-comment">### to integer overflow. </span><br>    y_pred = softmax(outsideVectors @ centerWordVec)<br>    y = np.zeros(<span class="hljs-built_in">len</span>(outsideVectors))<br>    y[outsideWordIdx] = <span class="hljs-number">1</span><br>    gradCenterVec = outsideVectors.T @ (y_pred - y)<br>    gradOutsideVecs = np.multiply(centerWordVec, np.expand_dims(y_pred - y, axis = <span class="hljs-number">1</span>))<br>    loss = -(outsideVectors[outsideWordIdx].T @ centerWordVec) + np.log(np.<span class="hljs-built_in">sum</span>(np.exp(outsideVectors @ centerWordVec)))<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> loss, gradCenterVec, gradOutsideVecs<br></code></pre></td></tr></table></figure><p>显然，这种方法的分母的计算开销太大，我们需要计算整个语料库的所有单词向量与中心向量相乘,因此在实际应用中，通常会采用negative sample的方法来减少计算开销</p><h3 id="negative-sample"><a href="#negative-sample" class="headerlink" title="negative sample"></a>negative sample</h3><p>回顾一下上面的损失函数的计算<br>$$<br>loss = J(\theta)=-\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>可以看到$-\frac{1}{T}\sum\limits_{t=1}^T$，为了消去该项，我们采用负采样的方法</p><p>与此同时，在负采样中，概率的计算也和上面的softmax函数是不同的，不会有遍历整个语料库的分母计算</p><p>对于每一个单个的中心单词，我们的目标函数为：<br>$$<br>\arg\max\limits_{\theta}p(D=1|w,c_{pos;\theta})\prod\limits_{c_{neg}\in W_{neg}}p(D=0|w,c_{neg;\theta})<br>$$<br>$W_{neg}$为所有负采样得到的样本，负采样就是那些噪声样本，这些样本没有出现在中心单词的附近</p><p>其中$p(D=1|w,c_{pos;\theta})$表示在语料库中出现了$(w,c_{pos})$​​​这一单词对在窗口中的概率</p><p>$p(D=0|w,c_{neg};\theta)$表示在语料库中没有出现$(w,c_{neg})$这一单词对在窗口中的概率</p><p>$c_{pos}$就是正样例，表示确实在语料库中在中心单词w的窗口内出现过$c_{pos}$这一单词</p><p>$c_{neg}$就是负样例，表示语料库中中心单词w的窗口内没有出现过$c_{neg}$这一单词</p><p>把(14)式进一步化简<br>$$<br>\arg\max\limits_{\theta}\log p(D=1|w,c_{pos};\theta)+\sum\limits_{c_{neg}\in W_{neg}}(1-\log p(D=1|w,c_{neg};\theta))<br>$$<br>对于该概率$p(D=1|w,c;\theta)$​，我们不采用softmax的计算方法，而是用sigmoid函数来计算<br>$$<br>p(D=1|w,c;\theta)=\frac{1}{1+exp(-c_{output}\cdot w)}<br>$$<br>$c_{output}$就是该单词在$W_{output}$中的对应的单词向量</p><p>因此(10)式可以改为<br>$$<br>\arg\max\limits_{\theta}\log \frac{1}{1+exp(-c_{pos}\cdot w)}+\sum\limits_{c_{neg}\in W_{neg}}(1-\log \frac{1}{1+exp(-c_{neg}\cdot w)})<br>$$</p><p>$$<br>\arg\max\limits_{\theta}\log \frac{1}{1+exp(-c_{pos}\cdot w)}+\sum\limits_{c_{neg}\in W_{neg}}\log \frac{1}{1+exp(c_{neg}\cdot w)}<br>$$</p><p>设$\sigma(x)=\frac{1}{1+exp(-x)}$​<br>$$<br>\arg\max\limits_{\theta}\log\sigma(-c_{pos}\cdot w)+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w)<br>$$<br>从上式可以看出，对每一个中心单词进行损失计算的话，不需要遍历整个语料库进行计算，而只需要计算负样本和正样本就可以了。因此计算梯度并更新参数的时候，我们只需要更新$w,c_{pos},W_{neg}$</p><p>正常来讲，损失函数应该是<br>$$<br>J(\theta)=-\frac{1}{T}\sum\limits_{i=1}^T\sum\limits_{-c&lt;j&lt;c}(\log\sigma(-c_{pos}\cdot w)+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w))<br>$$<br>这种方法在遍历完一次整个语料库后对参数进行更新，叫做<strong>batch gradient descent</strong></p><p>但是因为它的高计算开销，我们基本不会在实际应用中使用到</p><p>一般实际应用中，采用的是随机梯度下降法 <strong>stochastic gradient descent</strong></p><p>我们对每一对正训练对$(w,c_{pos})$进行一次梯度的计算并对其相对应的单词向量进行更新<br>$$<br>J(\theta;w,c_{pos})=-\log\sigma(c_{pos}\cdot w)-\sum\limits_{c_{neg}\in W_{neg}} \log\sigma(-c_{neg}\cdot w)<br>$$<br><strong>在更新的时候，我们仅仅更新当前的中心单词的$W_{input}$里的单词编码，以及$c_{pos},c_{neg}\in W_{neg}$在$W_{output}$里面的单词编码。</strong></p><p>显然这种做法大大减小了计算开销，训练速度较快。</p><p>对于batch gradient descent和stochastic gradient descent，可以参考标准BP算法和累计BP算法的区别。标准BP算法每一更新只针对一个样例，参数的更新非常频繁，而且对不同的样例更新的时候可能会出现“抵消”的现象，同时为了达到<strong>累计误差</strong>极小值点，标准BP算法需要进行更多次数的迭代。累计BP算法直接针对<strong>累计误差</strong>进行最小化，它在读取整个训练集一次之后才更新参数，参数更新的频率要低得多。但在很多任务中，累计误差在下降到一定程度后，进一步的下降就会非常的慢，这时标准BP算法会获得更好的解，尤其是在训练集较大的时候更为明显</p><p>对于每一个正训练对$(w,c_{pos})$​​,梯度计算如下：$w$是$W_{input}$的编码，$c_{pos}$是$W_{output}$的编码<br>$$<br>\frac{\partial J}{\partial c_{pos}}=(\sigma(c_{pos}\cdot w)-1)\cdot w<br>$$</p><p>$$<br>\frac{\partial J}{\partial c_{neg}}=\sigma(c_{neg}\cdot w)\cdot w<br>$$</p><p>$$<br>\frac{\partial J}{\partial w}=(\sigma(c_{pos}\cdot w)-1)\cdot c_{pos}+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w)\cdot c_{neg}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">negSamplingLossAndGradient</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">    centerWordVec,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideWordIdx,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideVectors,</span></span><br><span class="hljs-params"><span class="hljs-function">    dataset,</span></span><br><span class="hljs-params"><span class="hljs-function">    K=<span class="hljs-number">10</span></span></span><br><span class="hljs-params"><span class="hljs-function"></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot; Negative sampling loss function for word2vec models</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the negative sampling loss and gradients for a centerWordVec</span><br><span class="hljs-string">    and a outsideWordIdx word vector as a building block for word2vec</span><br><span class="hljs-string">    models. K is the number of negative samples to take.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Note: The same word may be negatively sampled multiple times. For</span><br><span class="hljs-string">    example if an outside word is sampled twice, you shall have to</span><br><span class="hljs-string">    double count the gradient with respect to this word. Thrice if</span><br><span class="hljs-string">    it was sampled three times, and so forth.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># Negative sampling of words is done for you. Do not modify this if you</span><br>    <span class="hljs-comment"># wish to match the autograder and receive points!</span><br>    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)<br>    indices = [outsideWordIdx] + negSampleWordIndices<br>    <span class="hljs-comment">### YOUR CODE HERE (~10 Lines)</span><br><br>    <span class="hljs-comment">### Please use your implementation of sigmoid in here.</span><br>    loss = -np.log(sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec)) - np.<span class="hljs-built_in">sum</span>(np.log(sigmoid(-outsideVectors[negSampleWordIndices] @ centerWordVec)))<br>    posGrad = (sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec) - <span class="hljs-number">1</span>) * centerWordVec<br>    negGrad = np.expand_dims(sigmoid(outsideVectors[negSampleWordIndices] @ centerWordVec),axis =<span class="hljs-number">1</span>) @ np.expand_dims(centerWordVec,axis = <span class="hljs-number">0</span>)<br>    gradOutsideVecs = np.zeros(outsideVectors.shape)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>        gradOutsideVecs[negSampleWordIndices[i]] += negGrad[i]<br>    gradOutsideVecs[outsideWordIdx] += posGrad<br>    gradCenterVec = (sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec) - <span class="hljs-number">1</span>) * outsideVectors[outsideWordIdx] +\<br>                    np.<span class="hljs-built_in">sum</span>(np.expand_dims(sigmoid(outsideVectors[negSampleWordIndices] @ centerWordVec),axis=<span class="hljs-number">1</span>) * outsideVectors[negSampleWordIndices],axis = <span class="hljs-number">0</span>)<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> loss, gradCenterVec, gradOutsideVecs<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Intuitive Explanation of Field Aware Factorization Machines</title>
    <link href="/2021/08/14/An%20Intuitive%20Explanation%20of%20Field%20Aware%20Factorization%20Machines/"/>
    <url>/2021/08/14/An%20Intuitive%20Explanation%20of%20Field%20Aware%20Factorization%20Machines/</url>
    
    <content type="html"><![CDATA[<p>From LM to Poly2 to MF to FM to FFM</p><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>线性回归是最简单无脑的拟合一个因变量和一个或多个自变量之间函数关系的模型<br>$$<br>f(x_1,x_2)=w_1x_1+w_2x_2<br>$$<br>一般来说，线性回归模型不会表现得很好，因为它试图对每个自变量的行为分别拟合，而没有考虑到自变量之间有可能会有相互作用影响，也就是有可能$x_1$可能会依赖于$x_2$​</p><p><img src="https://i.loli.net/2021/08/14/NUX8tiWzfRonDVF.png"></p><h2 id="Poly2"><a href="#Poly2" class="headerlink" title="Poly2"></a>Poly2</h2><p><strong>为了理解Poly2和FM,FFM的内容，我们需要先搞清楚它们的自变量的域是什么</strong></p><p>对于原本的离散的域，我们会用one-hot编码生成新的域，如上面的图会转变成下面这样</p><table><thead><tr><th>Gender: Male</th><th>Gender: Female</th><th>Genre: Action</th><th>Genre: Romance</th><th>Genre: Thriller</th><th>Genre: Sci-Fi</th><th>Rating</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>5</td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>3</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>4</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>4</td></tr></tbody></table><p>为了改进线性模型，Poly2就加入了一个相互作用项<br>$$<br>f(x_1,x_2)=w_1x_1+w_2x_2+w_{1,2}x_1x_2<br>$$<br>对所有的特征都两两组合<br>$$<br>\phi_{Poly2}(w,x)=\sum\limits_{j_1=1}^n\sum\limits_{j_2=1}^nw_{h(j_1,j_2)}x_{j1}x_{j2}<br>$$<br>但是Poly2模型也有对应的几个缺点</p><p>一. 如果<strong>两个特征很少交互</strong>，那么预测是不可靠的</p><p>比如训练集中有1万个训练样本，但是只有两个样本是<strong>男性</strong>观看<strong>恐怖电影</strong>的，那么我们未来预测男性观看恐怖电影的打分的时候，就是以这两个样本作为基准预测的（也就是这两个特征组合的权重是依赖着两个样本点的）。显然这是不可靠的</p><p>二.对于一些<strong>零相互作用</strong>的特征组合，则预测也是不准的</p><p>假设如果没有女性观看科幻电影的打分的样本，则根据上面的模型的打分是没有意义的</p><p>三.由于用极暴力的手段对特征进行了两两组合，造成了<strong>数据稀疏性</strong>的问题</p><p>其实这一点和上面的两点是类似的，就是没有该特征组合的情况会很多，这时候该特征组合的权重难以收敛</p><table><thead><tr><th>A</th><th>B</th><th>C</th><th>D</th><th>F</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>4</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>6</td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>8</td></tr></tbody></table><p>如图，A，B，C，D都是自变量，其取值为(0,1)，F是连续的因变量</p><p>在这个样本集中，想要训练到$x_Bx_D$的权重是不可能的，因为$x_B,x_D$没有同时为1的情况，所以$w_{B,D}$无法训练得出。这就是样本稀疏性问题。</p><p>因为相互作用项一共有$n^2$​个，在上面的自变量中，想要训练到所有的相互作用项权重，至少需要16个样本，现实中很难使得样本的数量可以满足所有相互作用项权重的训练。</p><p>对于样本稀疏性的问题，通常可以使用<strong>降维</strong>的手段，使得样本矩阵变得稠密</p><p>容易看出，训练互相作用项的复杂度是$O(n^2)$</p><h2 id="MF"><a href="#MF" class="headerlink" title="MF"></a>MF</h2><p>MF全称Matrix Factorization，意思是矩阵分解</p><p>在该方法中，我们使用不同的方法表示样本，用推荐系统中常见的user-item形式</p><p><img src="https://i.loli.net/2021/08/14/vGdNMtQlJErPH7i.png"></p><p>每一行代表用户（user），每一列代表电影（item），矩阵的每一个元素表示用户给该电影的打分</p><p>我们要做的就是生成两个子空间，用户空间和商品空间，每一个用户对应用户空间里面的一个向量，同理每一部电影对应商品空间的一个向量</p><p>相对应的，就是把上面的矩阵分解<br>$$<br>\hat{R}=P\cdot Q^T<br>$$<br>$R$是$M\times N$​的矩阵（有M个用户，N部电影），$P$是$M\times k$的矩阵，每个用户对应P矩阵里面的一行向量，也就是每个用户用一个k维的向量表示；$Q$是$N\times k$的矩阵，每部电影用一个k维的向量表示。</p><p>分解的过程一般使用<strong>梯度下降法</strong>或者<strong>最小二乘法</strong>去逼近原矩阵，把损失函数的值降为最小</p><p><img src="https://i.loli.net/2021/08/14/Lmb23SFkW6AzwCt.png"></p><p><strong>MF对线性回归和Poly2的改进</strong></p><p>从上面的图可以看出，因为隐向量的存在，即使我们没有第一个用户对memento的打分情况，我们仍然可以训练出来他对memento的打分的情况。对数据稀疏性的问题有了较好的解决。</p><p><strong>MF的缺点</strong></p><p>它只是把用户项目矩阵分解的一个简单想法，没有考虑电影类型，语言等辅助的判断信息</p><p>因此这时候引进FM方法</p><h2 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h2><p>FM全称Factorization Machines（FM）<br>$$<br>y=w_0+\sum\limits_{i=1}^nw_ix_i+\sum\limits_{i=1}^n\sum\limits_{j=i+1}^n&lt;v_i,v_j&gt;x_ix_j<br>$$<br>对每一个特征，都分配一个向量$v$来表示，所有的向量$v_i\quad i\in n$组成一个矩阵，该矩阵就是机器学习需要学习的参数之一。</p><p>若令该潜在的向量空间的维数为$k$维，也就是每个特征对应一个$k$维的向量，则学习相互作用项的复杂度为$O(nk)$。可以看出比Poly2的复杂度要低（一般令$k&lt;n$)</p><p>且FM解决了数据稀疏性的问题，因为当我们需要$(A_1,B_1)$对应的相互作用权重的时候，我们可以通过$(A_1,B_2)$训练$A_1$对应的隐向量的参数，同样$(A_2,B_1)$也可以训练$B_1$对应的隐向量的参数。即使我们在训练集中没有$(A_1,B_1)$这个组合，我们也可以训练到对应的权重，因为一个相互交互项也会和其它交互项会有关系，也就是说得到一个样本，训练该样本的交互项的同时会潜在地训练与该交互项相关的其他的交互项，这样就使得即使我们没有在数据集中得到$(A_1,B_1)$交互项的信息，但是也可以训练到该交互项的权重</p><p><img src="https://i.loli.net/2021/08/14/8GbAMTvFJNgIoj1.png"></p><p>如图，我们把用户-项目交互项用one-hot来编码，每行对应一个用户和一个项目，同时还加入了一些辅助的信息（以改进MF的过于简单的矩阵分解算法），辅助信息可以是比如该用户对其他电影的打分，该用户观看的最后一步电影等等。</p><p>FM的相互交互项还有降低计算复杂度的公式：</p><p><img src="https://i.loli.net/2021/08/14/Qagf1UDtu5VYHn4.png"></p><h2 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h2><p>FFM全称Field Aware Factorization Machines</p><p>FFM是FM模型的改进算法，Field – 域</p><p><img src="https://i.loli.net/2021/08/14/oV3qGn6yiWAm5Ll.png"></p><table><thead><tr><th>Gender: Male</th><th>Gender: Female</th><th>Genre: Action</th><th>Genre: Romance</th><th>Genre: Thriller</th><th>Genre: Sci-Fi</th><th>Rating</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>5</td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>3</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>4</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>4</td></tr></tbody></table><p>FFM与FM最大的区别就是权重计算的不同</p><p>FM计算权重，以第一行为例，则为<br>$$<br>v_{male}\cdot v_{action}+v_{male}\cdot v_{northAmerica}+v_{action}\cdot v_{northAmerica}<br>$$<br>这样male和Genre域的特征隐向量相乘的隐向量和male和Region域的特征隐向量相乘的隐向量都是$v_{male}$</p><p>但是直观上来看，$&lt;v_{male},v_{action}&gt;$和$&lt;v_{male},v_{northAmerica}&gt;$是由区别的，也就是说male这个域和不同的域的相关性是不同的，我们应该更细致地划分male和不同域之间的权重</p><p>于是就有FFM的计算权重的方法：<br>$$<br>v_{male,genre}\cdot v_{action,gender}+v_{male,region}\cdot v_{NAmerica,gender}+v_{action,region}\cdot v_{NAmerica,genre}<br>$$<br>隐向量的个数变得更多</p><p>众所周知，越复杂的模型就越容易过拟合，当数据量足够大的时候，我们才应该考虑使用FFM，数据量小的时候用FM效果更好</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://i.loli.net/2021/08/14/PSr7JUjhoxX9Yb1.png"></p><p>英语参考原文，中间也加入了我自己的一些理解</p><p><a href="https://towardsdatascience.com/an-intuitive-explanation-of-field-aware-factorization-machines-a8fee92ce29f">https://towardsdatascience.com/an-intuitive-explanation-of-field-aware-factorization-machines-a8fee92ce29f</a></p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>日语杂学（1）</title>
    <link href="/2021/08/14/%E6%97%A5%E8%AF%AD%E6%9D%82%E5%AD%A6%EF%BC%881%EF%BC%89/"/>
    <url>/2021/08/14/%E6%97%A5%E8%AF%AD%E6%9D%82%E5%AD%A6%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>​    <img src="https://i.loli.net/2021/08/14/9VdoGY3iy7eI4X1.jpg"></p><p>​    最近了解到的冷知识，第一次看到感觉有点震惊。平时总是说日语的汉字都是抄中国的，我在学习的时候也总是觉得这个词不就是中文的那个词吗，印象中总是日本把中国的东西搬了过去。</p><p>​    像是“社会”“主义”，如果有人告诉我，这两个词是日本造的汉字，应该不太难接受，因为知道日本在明治维新后改革了国家体制，走上资本主义道路，思想也基本“脱亚入欧”，吸取了很多西方的先进的思想，中国在甲午海战被打败后才意识到自己的不足，后面孙中山那些人也基本都去了日本学习西方的思想。社会主义又是这么先进的词汇，因此大概也可以理解为是20世纪初孙中山他们从日本传回中国的词汇。</p><p>​    但是像是“历史”“表现”“场合”“取消”“手续”，这些词，居然也是从日语反向传播到中文里的，我觉得有点不可思议。毕竟像是“历史”，感觉应该中国古代就会出现的词语。查了一下，确实是中国古代就出现过的词语，在东晋时代，古书里就出现过</p><blockquote><p><a href="https://baike.baidu.com/item/%E8%A3%B4%E6%9D%BE%E4%B9%8B">裴松之</a> 注引《<a href="https://baike.baidu.com/item/%E5%90%B4%E4%B9%A6/36618">吴书</a>》：“﹝ 吴王 ﹞志存经略，虽有馀闲，博览书传<strong>历史</strong>，藉采奇异，不效诸生寻章摘句而已。”</p></blockquote><p>但是，在当时似乎没有流传开来该词。在古代一般都以“史”字表示历史，大概是古代汉语讲究语言的效率所以没有流传开来吧，毕竟“你吃了饭没有”，在古代只需要用“饭否”两字就可以表示。”历史“一词是日本学者在翻译英语的history的时候把其翻译为“历史”，最后流传到了中国，广泛使用。</p><p>​    还有“手续”那一个词，我一开始以为它是音读的词语，大概是读成“しゅうぞく”这个感觉，结果第一次查读音，发现是训读的”てつづき”,当时的我还觉得这日本照搬的词语都不会汉字发音，要自己搞个这么怪的发音哈哈，现在才知道小丑竟是我自己。原来是中国抄的日本的词语。或许可以给点启发，日语词中如果是训读的汉字，那大概是日本自创的汉字词，而不是中国传过去的汉字。同样的还有「場合」，也是训读「ばあい」</p><p>​    这样看来，语言真的是不断随着时间变化，流传。我们早已习以为常的东西，或许只是最近的100年才出现的，如果穿越回到200年前，我们说的很多东西大概清朝人都不会听明白吧</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Debugging Gradient Checking</title>
    <link href="/2021/08/13/Debugging-Gradient-Checking/"/>
    <url>/2021/08/13/Debugging-Gradient-Checking/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在很多机器学习的算法中，在训练模型的时候，我们都会经常使用梯度下降法来对模型中的需要学习的参数进行更新，而梯度下降法就必须要运用到导数的运算了，因此验证自己算出来的导数是否正确十分有必要，不然最后模型效果不理想却又不知道具体的原因就不好了</p><p><img src="https://i.loli.net/2021/08/13/l7Ttik9xOGbDRPH.png"></p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>回忆一下导数的数学推导, 假设$J:R\to R,\theta\in R$<br>$$<br>\frac{dJ{(\theta)}}{d\theta}=\lim\limits_{\epsilon\to0}\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}<br>$$<br>因此，给定一个点$\theta$，我们可以用上面的式子去接近导数的值</p><p>在实际应用中，通常可以取$\epsilon$为一个很小的值$10^{-4}$</p><p>然后还需要考虑的是$\theta$的取值。在实际应用中，一般$J:R^n\to R,\theta\in R^n$</p><p>通常可以对每一个维度都进行上面的操作<br>$$<br>g_i(\theta)=\frac{\partial J(\theta)}{\partial\theta_i}=\frac{J(\theta^{(i+)})-J(\theta^{(i-)})}{2\times EPSILON}<br>$$<br>$\theta^{(i+)}=\theta+ \epsilon e_i$</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradcheck_naive</span>(<span class="hljs-params">f, x, gradientText</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot; </span><br><span class="hljs-string">    Gradient check for a function f.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    f -- a function that takes a single argument and outputs the</span><br><span class="hljs-string">         loss and its gradients</span><br><span class="hljs-string">    x -- the point (numpy array) to check the gradient at</span><br><span class="hljs-string">    gradientText -- a string detailing some context about the gradient computation</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    rndstate = random.getstate()<br>    random.setstate(rndstate)<br>    fx, grad = f(x) <span class="hljs-comment"># Evaluate function value at original point</span><br>    h = <span class="hljs-number">1e-4</span>        <span class="hljs-comment"># Do not change this!</span><br><br>    <span class="hljs-comment"># Iterate over all indexes ix in x to check the gradient.</span><br>    it = np.nditer(x, flags=[<span class="hljs-string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>])<br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> it.finished:<br>        ix = it.multi_index<br><br>        x[ix] += h <span class="hljs-comment"># increment by h</span><br>        random.setstate(rndstate)<br>        fxh, _ = f(x) <span class="hljs-comment"># evalute f(x + h)</span><br>        x[ix] -= <span class="hljs-number">2</span> * h <span class="hljs-comment"># restore to previous value (very important!)</span><br>        random.setstate(rndstate)<br>        fxnh, _ = f(x)<br>        x[ix] += h<br>        numgrad = (fxh - fxnh) / <span class="hljs-number">2</span> / h<br><br>        <span class="hljs-comment"># Compare gradients</span><br>        reldiff = <span class="hljs-built_in">abs</span>(numgrad - grad[ix]) / <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">abs</span>(numgrad), <span class="hljs-built_in">abs</span>(grad[ix]))<br>        <span class="hljs-keyword">if</span> reldiff &gt; <span class="hljs-number">1e-5</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient check failed for %s.&quot;</span> % gradientText)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First gradient error found at index %s in the vector of gradients&quot;</span> % <span class="hljs-built_in">str</span>(ix))<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Your gradient: %f \t Numerical gradient: %f&quot;</span> % (<br>                grad[ix], numgrad))<br>            <span class="hljs-keyword">return</span><br><br>        it.iternext() <span class="hljs-comment"># Step to next dimension</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient check passed!.&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
