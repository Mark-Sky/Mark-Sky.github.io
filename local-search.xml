<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>机器翻译历史</title>
    <link href="/2021/10/29/%E5%BA%8F%E5%88%97%E5%8C%96-%E6%B3%A8%E6%84%8F%E6%9C%BA%E5%88%B6/"/>
    <url>/2021/10/29/%E5%BA%8F%E5%88%97%E5%8C%96-%E6%B3%A8%E6%84%8F%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="SMT"><a href="#SMT" class="headerlink" title="SMT"></a>SMT</h1><p>SMT全称Statistical Machine Translation，统计式机器翻译。在1990到2010的20年，这类方法是机器翻译的主流方法，人们投入了大量的时间去研究，做特征工程</p><h2 id="大概流程"><a href="#大概流程" class="headerlink" title="大概流程"></a>大概流程</h2><ol><li>把源句子分割（至于分割为短语还是字符还是单词取决于该语言的特性）</li><li>通过翻译模型（Translation model）来找到源句子的片段的对应的目标句子的片段</li><li>通过语言模型（Language Model）调整目标句子的语序使得句子流畅</li></ol><p><img src="https://i1.wp.com/kantanmtblog.com/wp-content/uploads/2019/04/smt-model.jpg?resize=572,367&ssl=1" alt="SMT Model"></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>SMT需要训练两个模型，分别是翻译模型（Translation model）和语言模型（Language Model）</p><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>给定中文句子x，我们需要找到最好的翻译的英语句子y（最有可能的翻译）<br>$$<br>\arg\max_{y} P(y|x)<br>$$<br>使用贝叶斯公式，可以把上面的优化目标分为两部分<br>$$<br>\arg\max_{y} P(x|y)\cdot P(y)<br>$$<br>$P(x|y)$就是翻译模型的优化目标，对应的是翻译的准确性，需要使用大量的平行数据，比如说要翻译中文为英文，需要大量的中英译本作为数据集</p><p>$P(y)$则为语言模型的优化目标，对应的是翻译的流畅性，只需要使用单语数据</p><p>从该模型的优化目标就可以看出来，SMT是不可能赶上优秀的人工翻译的。中学时学文言文的时候，老师就教过翻译的原则“信，达，雅”</p><blockquote><p>“译事三难，即信，达，雅。求其信，已大难也！顾信也，不达，虽译，犹不译也，则达尚焉。”</p></blockquote><p>“信”（faithfulness）是指忠实准确地传达原文的内容；“达”（expressiveness）指译文通顺流畅；“雅”（elegance）可解为译文有文才，文字典雅。</p><p>即使得到的模型能最大化$P(x|y),P(y)$，也仅仅只是做到了信和达，但是是不可能做到“雅”的。</p><h3 id="翻译模型"><a href="#翻译模型" class="headerlink" title="翻译模型"></a>翻译模型</h3><p>翻译模型需要最大化$P(x|y)$，但是实际上还需要引入一个隐变量$a$，代表着两种语言的单词，短语之间的对应关系，英文是alignment.。实际上最大化的是$P(x,a|y)$</p><p>语言单词之间的对应关系是一个非常复杂的东西，</p><p>有可能中文的一个单词会对应英语的多个单词 “男人” $\to$ “the man”</p><p>也会有英语的一个单词对应中文的多个单词，“spam”$\to$ “垃圾 邮件”</p><p>综上，对应关系可以是一对一，也可以是一对多，多对一，</p><p>学习$P(x,a|y)$的时候，需要更加具体地表示$P(x,a|y)$，它在语义上可以包含：</p><ul><li>特定单词对齐的概率</li><li>单词可能对应的译文单词的数目的概率</li><li>…..</li></ul><p>（翻译的有点怪hh）总之就是可以具象地表示$P(x,a|y)$这个抽象的概念</p><p>对应关系是一个隐变量，因此并不是在数据集中能直接观察得到的特征，需要使用到一些特殊的算法，比如说EM算法来学习隐变量的分布</p><h3 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h3><p>因为不可能把所有可能的目标句子$y$遍历一遍的，所以需要使用一些算法，比如所动态规划算法（Viterbi algorithm），这是概率图算法（好难）</p><h1 id="NMT"><a href="#NMT" class="headerlink" title="NMT"></a>NMT</h1><p>因为SMT的复杂，且需要投入大量的人力来做特征工程，到2010年后，NMT出现后，基本上就被替代了</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211030095953345.png" alt="image-20211030095953345"></p><p>这么多年来NMT也是逐渐的在进步，不断改进，才取得了现在的效果</p><h2 id="传统编码-译码模型"><a href="#传统编码-译码模型" class="headerlink" title="传统编码-译码模型"></a>传统编码-译码模型</h2><h3 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h3><p>给定一个源句子$x=(x_1,x_2,…,x_{T_x})$，编码的任务就是把$x$转化为一个固定长度的向量$c$</p><p>一般的做法就是，先得到RNN的隐层<br>$$<br>h_t=f(x_t, h_{t-1})\quad t=1,2,…,T_{x}<br>$$<br>然后对所有时间的隐层输出作整合得到编码向量c<br>$$<br>c=q({h_1,h_2,…,h_{T_x}})<br>$$<br>$f,q$一般都是非线性函数，可以用LSTM作为$f$</p><div class="note note-primary">            <p>值得注意的是，一般这种单向的序列化处理模型会让句子倒序输入，以减少长依赖关系，增加短依赖关系</p>          </div><h3 id="译码"><a href="#译码" class="headerlink" title="译码"></a>译码</h3><p>为了得到目标句子$y = (y_1,y_2,…,y_{T_y})$，我们训练模型，用于计算下面的概率<br>$$<br>p(y)=\prod\limits_{t=1}^{T_y}p(y_t|{y_1,y_2,…,y_{t-1}},c)<br>$$<br>实际上就是简单的RNN模型，每一步给定已经生成的$t’-1$个单词{$y_1,y_2,…,y_{t’-1}$}，要求生成第$t’$个单词$y_{t’}$<br>$$<br>p(y_t|y_1,y_2,..,y_{t-1},c)=g(y_{t-1},s_{t},c)<br>$$<br>其中$s_t$表示$t$时刻译码RNN隐层的输出</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>很明显，这种简单的模型的最大缺陷就是它尝试把整个句子编码为一个固定的向量，简单暴力，效果也没多好，难以处理长依赖关系。而且由于抛弃了SMT中的Align，模型的可解释性不强</p><blockquote><p>One of the motivations behind the proposed approach was the use of a fixed-length context vector in the basic encoder-decoder approach. We conjectured that this limitation may make the basic encoder-decoder approach to underperform with long sentences</p></blockquote><h2 id="RNN-Soft-Align"><a href="#RNN-Soft-Align" class="headerlink" title="RNN + Soft Align"></a>RNN + Soft Align</h2><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211104224635370.png" alt="image-20211104224635370" style="zoom:67%;" /><h3 id="编码-1"><a href="#编码-1" class="headerlink" title="编码"></a>编码</h3><div class="note note-primary">            <p>在论文中，<strong>annotation of the word</strong>经常出现，直接翻译就是单词的注释。这个单词感觉用得挺好的，因为单词注释就表示在编码的时候RNN的隐层的输出，该向量就好像是这个单词的解释一样</p>          </div><p>编码器使用LSTM，且使用双向的，这样可以更好的利用<strong>上下文</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = self.model_embeddings.source(source_padded)<br>X = pack_padded_sequence(X, [<span class="hljs-built_in">len</span>(sentence[sentence != <span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> source_padded.T])<br>enc_hiddens, (last_hidden, last_cell) = self.encoder(X)<br>seq_unpacked, lens_unpacked = pad_packed_sequence(enc_hiddens)<br></code></pre></td></tr></table></figure><p>在具体的实现中，需要对填充过的句子先删除掉没有意义的填充值0，然后再进行编码，编码之后，为了格式的统一再填充上0值</p><p>具体参考<a href="https://zhuanlan.zhihu.com/p/342685890">pack_padded_sequence 和 pad_packed_sequence - 知乎 (zhihu.com)</a></p><blockquote><p>当采用 RNN 训练序列样本数据时，会面临序列样本数据长短不一的情况。比如做 NLP 任务、语音处理任务时，每个句子或语音序列的长度经常是不相同。难道要一个序列一个序列的喂给网络进行训练吗？这显然是行不通的。</p><p>为了更高效的进行 batch 处理，就需要对样本序列进行填充，保证各个样本长度相同，在 PyTorch 里面使用函数 pad_sequence 对序列进行填充。填充之后的样本序列，虽然长度相同了，但是序列里面可能填充了很多无效值 0 ，将填充值 0 喂给 RNN 进行 forward 计算，不仅浪费计算资源，最后得到的值可能还会存在误差。因此在将序列送给 RNN 进行处理之前，需要采用 pack_padded_sequence 进行压缩，压缩掉无效的填充值。序列经过 RNN 处理之后的输出仍然是压紧的序列，需要采用 pad_packed_sequence 把压紧的序列再填充回来，便于进行后续的处理。</p></blockquote><ul><li>数学描述</li></ul><p>对于源句子，我们首先把每个单词都转换成为编码好的单词向量$x_1,x_2,..,x_m(x_i\in R^{e\times 1})$，m就是源句子的长度，e就是单词向量的编码长度</p><p>编码的Bi-LSTM中，有用的变量：</p><p>hidden states：$h_i^{enc}=[\overleftarrow{h_i^{enc}};\overrightarrow{h_i^{enc}}]$   where $h_i^{enc}\in R^{2h\times 1}$，$\overleftarrow{h_i^{enc}},\overrightarrow{h_i^{enc}}\in R^{h\times 1}$</p><p>cell states：$c_i^{enc}=[\overleftarrow{c_i^{enc}};\overrightarrow{c_i^{enc}}]$   where $h_i^{enc}\in R^{2h\times 1}$，$\overleftarrow{c_i^{enc}},\overrightarrow{c_i^{enc}}\in R^{h\times 1}$</p><div class="note note-info">            <p>关于hidden states和cell states，个人的直观理解是：hidden states更关注该时刻的最近的信息，而cell states可以存储更久远之前的信息</p>          </div><ul><li>代码</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X = self.model_embeddings.source(source_padded)<br>X = pack_padded_sequence(X, [<span class="hljs-built_in">len</span>(sentence[sentence != <span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> source_padded.T])<br>enc_hiddens, (last_hidden, last_cell) = self.encoder(X)<br>seq_unpacked, lens_unpacked = pad_packed_sequence(enc_hiddens)<br>enc_hiddens = seq_unpacked.transpose(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h3 id="译码-1"><a href="#译码-1" class="headerlink" title="译码"></a>译码</h3><p>每一步的概率：<br>$$<br>p(y_i|y_1,…,y_{i-1},x)=g(y_{i-1},s_i,c_i)<br>$$<br>可以回顾一下上面写的传统的模型的概率<br>$$<br>p(y_t|y_1,y_2,..,y_{t-1},c)=g(y_{t-1},s_{t},c)<br>$$<br>两者最大的不同就是用$c_i$代替了$c$，$c_i$是用对编码层的单词的注释的不同权重相加得到的，这里就有Attention的意思了，表示当前译码的输出可以回看整个编码的过程，并且有侧重的选择某些单词作为翻译的重点<br>$$<br>c_i=\sum\limits_{i=1}^{T_x}\alpha_{ij}h_j<br>$$<br>而注释的权重由下式得到：<br>$$<br>\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}<br>$$</p><p>$$<br>e_{ij}=a(s_{i-1},h_j)<br>$$</p><p>$a(s_{i-1},h_j)$就是一个align模型，它表示输入的第j个单词（的注释）对第i个位置的输出的影响的大小。和SMT不同的是，这里只需要把$a$设置为一个前向传输的神经网络来训练，它是和模型的其他参数一起显式的训练的，而不是一个隐变量。</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211105154727607.png" alt="image-20211105154727607"></p><div class="note note-warning">            <p>感觉这个是神经网络的一个很大的进步，因为神经网络本身的可解释性是很差的，但是该显式的soft align的引入使得机器翻译模型更符合人类的直观感觉-翻译的时候是每个单词看着对应的源句子里的单词来翻译的，而不是读完一个源句子变成一个向量然后让机器翻译这个向量</p>          </div><h3 id="阶段总结"><a href="#阶段总结" class="headerlink" title="阶段总结"></a>阶段总结</h3><p>引入了Soft Alignment的RNN模型对比起传统的SMT的优点就是把原来的hard Alignment（隐变量，计算算法复杂）抛弃，而采用soft Alignment。</p><p>而对比起NMT的传统的编码-译码模型，该模型的优点还是引入了alignment的机制，使得模型的可解释性变强</p><hr><p>未完待续</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN</title>
    <link href="/2021/10/15/LSTM/"/>
    <url>/2021/10/15/LSTM/</url>
    
    <content type="html"><![CDATA[<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>RNN全称Recurrent Neural Networks，循环神经网络，用于处理序列化的输入</p><p>LSTM是作为RNN的改进版而存在的，因此为了更好的了解LSTM，需要先对RNN的结构和它的不足之处有一个大概的认识</p><h2 id="RNN的应用"><a href="#RNN的应用" class="headerlink" title="RNN的应用"></a>RNN的应用</h2><p>RNN主要用于处理序列化的输入数据，当数据是有先后顺序的时候（也就是当前的输入和之前的输入是有关系的时候），比如说音频数据，视频数据。在对音频数据进行分析的时候， 比如说需要判断当前时间点属于歌曲的副歌部分还是主歌部分的时候，可以通过前面的和弦进行和当前的和弦一起进行判断。</p><p>当然，序列化的数据还有文本数据，我们可以利用Pytorch简单建立一个RNN的字符级的模型</p><p><a href="https://github.com/Mark-Sky/RNN">https://github.com/Mark-Sky/RNN</a></p><p>在该模型训练前期，输入一个“e”，得到</p><p> eohdo o ataI rtnvf a dcsF hit inirOedto L,ve  o tn u adminlArio ooancoletpwenid s te temaomt hn  e tt L  o ocNr ebrycaoms<br>igtc . nEta msrag  l hfat ee si i qH_oYho   aoza  ieeservenlhc mtpta afos ts</p><p>属于是完全狗屁不通</p><p>在该模型训练后期，输入一个“r”得到</p><p>rest ofps yo we che to goulate gour ytingeid evae no thans enc lat ireed, ond fime Pfeer billy comlatis tore ofirsiacometh bere anserat wreeslyFing oarse do efove and ufher cout aninntiguliy to Oeeu b </p><p>虽然还是狗屁不通，但是我们可以稍微看出来单词的样子，（因博主电脑资源有限，懒得等模型拟合了</p><p>前一段时间网上很火的大学生水课神器–狗屁不通文章生成器实际上就可以用RNN实现</p><h2 id="RNN的结构"><a href="#RNN的结构" class="headerlink" title="RNN的结构"></a>RNN的结构</h2><p>简单的来说，RNN其实就是利用了上一次的隐层的输出的一个神经网络，和时序逻辑电路的原理很像</p><p>下面以一个简单的一层隐藏层的RNN作为例子说明</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211015180642.png"></p><p>因为加入了上一次隐层的输出作为输入，所以神经网络的参数新增了$W_{hh}$</p><p>所有的权重参数：$W_{xh},W_{hh},W_{hy}$</p><p>前向传播：(用tanh作为激活函数)<br>$$<br>h_t = tanh(W_{xh}x+W_{hh}h_{t-1}+b_h)<br>$$</p><p>$$<br>y_t = W_{hy}h_t + b_y<br>$$</p><p>$$<br>p = softmax(y_t)<br>$$</p><p>但是我们经常会看到RNN的图如下：</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211015164703.png"></p><p>但是其实这只是把不同时间点的隐藏层画了出来，实际上还是和上面的简单的RNN图示的结构是一样的</p><h2 id="RNN的训练"><a href="#RNN的训练" class="headerlink" title="RNN的训练"></a>RNN的训练</h2><p>在对RNN训练的时候，一般使用批处理的训练，给n个输入，n个对应的目标输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = X[p:p + self.seq_length]<br>targets = X_id[p + <span class="hljs-number">1</span>:p + self.seq_length + <span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>比如说，给出一个长度为6的输入：見上げてごら</p><p>对应的目标输出就是：上げてごらん</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211022195952.png"></p><p>大致跟普通的神经网络的反向传播差不多，但是有一点需要注意的，就是我们正向传播的时候是按照时间顺序传播的<br>$$<br>h_t = tanh(W_{xh}x+W_{hh}h_{t-1}+b_h)<br>$$<br>但是我们在反向传播的时候，按照逆时间顺序来计算，注意$h_{t}$直接参与了两个式子<br>$$<br>h_{t+1} = tanh(W_{xh}x+W_{hh}h_{t}+b_h)<br>$$</p><p>$$<br>y_t = W_{hy}h_t+b_y<br>$$</p><p>而在计算$\frac{\partial loss}{\partial W_{xh}}$，$\frac{\partial loss}{\partial W_{hh}}$，$\frac{\partial loss}{\partial W_{bh}}$的时候，都需要经过<br>$$<br>\frac{\partial loss}{\partial h_t}=\frac{\partial loss}{\partial h_{t+1}}\cdot\frac{\partial h_{t+1}}{\partial h_t}+\frac{\partial loss}{\partial y_t}\cdot\frac{\partial y_t}{\partial h_t}<br>$$<br>因此，<strong>我们需要先计算$\frac{\partial loss}{\partial h_{t+1}}$，然后记下该结果，用于前一个时刻的梯度计算</strong>，因此反向传播是逆序的</p><p>而为了计算所有时刻的$\frac{\partial loss}{\partial h_{t+1}}$，我们需要在前传时保存下每个时刻下的隐层的状态，不能覆盖隐层状态$h = f(h)$这样</p><p>还有需要注意的就是<strong>梯度爆炸</strong>的问题，由上面的式子，我们很容易推导出<br>$$<br>\frac{\partial loss}{\partial h_{t-1}}=(\frac{\partial loss}{\partial h_{t+1}}\cdot\frac{\partial h_{t+1}}{\partial h_t}+\frac{\partial loss}{\partial y_t}\cdot\frac{\partial y_t}{\partial h_t})\frac{\partial h_t}{\partial h_{t-1}}+\frac{\partial loss}{\partial y_{t-1}}\cdot \frac{\partial y_{t-1}}{\partial h_{t-1}}<br>$$<br>对隐层的偏导是不断叠加的，最后一个时刻的隐层的偏导甚至会影响到第一个时刻的隐层的偏导，（最后一个时刻的RNN可以理解为一个深层的神经网络，该神经网络有$t$层隐层）因此很容易出现梯度爆炸的问题，因此需要clip来限制偏导的范围</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">np.clip(dparam, -<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, out=dparam)<br></code></pre></td></tr></table></figure><p>实际上一些RNN的变体就不会存在梯度爆炸的问题，如LSTM等具有门结构的神经网络</p><p>对梯度爆炸问题，可以参考下面的文章</p><p><a href="https://machinelearningmastery.com/exploding-gradients-in-neural-networks/">https://machinelearningmastery.com/exploding-gradients-in-neural-networks/</a></p><p>在每批训练结束后，需要清空隐层的状态，把上一批训练的最后一个时刻的隐层状态设置为下一批训练的第$-1$个隐层状态。</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211022205252.png"></p><p>还有一个对模型收敛很重要的点：使用Adam优化而不是SGD。使用交叉熵作为损失函数，对比两种梯度更新方法</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211022211916.png"></p><p>其中蓝色线代表Adam，黄色线代表SGD</p><p>总结一下，需要注意的点：</p><div class="note note-primary">            <ol><li>按照时间的逆序计算反向传播的梯度</li><li>避免梯度爆炸的情况</li><li>使用Adam优化而不是随机梯度下降</li></ol>          </div><h2 id="RNN的发展"><a href="#RNN的发展" class="headerlink" title="RNN的发展"></a>RNN的发展</h2><p>RNN是用于处理序列化输入的，最简单的模型就是本文所述的模型，该模型存在的问题是“尽管可以利用前文的信息，但是很难获取太久之前的信息，因此对长依赖的关系效果很差“</p><p>比如在文本预测中，我们需要根据前文判断中间空出来的词语：</p><p>小野さんは大学のころに中国語を学んだことがある。それから上海にきた、色んな人と出会た、いろんな知識を学んだ、仕事…二年前は日本に帰って来たんだけと、今でも＿＿語をぺらぺら喋られる</p><p>上面的一段显然那个__中应该填的是「中国」，但是简单的RNN无法利用太久远的信息「中国語」，虽然从后面的「語を喋られる」可以推测出是哪一个国家的语言，但是却很难推出是中文</p><p>可能百度翻译用的就是RNN？哈哈</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211022231747.png"></p><p>所以为了解决这种长依赖关系，LSTM就出现了，LSTM（Long Short Term Memory）的出厂设置就是可以利用很久以前的信息。但是由于它仍然是一个序列化的模型，后面还出现了更好的模型，<strong>Attention</strong>，该模型能更好的处理上面的情况</p><p>可以参考经典文章</p><p><a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211022224817.png"></p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>依存句法解析</title>
    <link href="/2021/10/07/parsing/"/>
    <url>/2021/10/07/parsing/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这次花了两天时间做了第三次作业，实际上感觉给的提示太多了，如果认真做的话应该很快就能做完。最后发现这个依存句法解析的学习是一个<strong>监督学习</strong>，也就是需要人工标签才能学习，还是有点失望。对比起论文</p><p><a href="https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf">emnlp2014-depparser.pdf (stanford.edu)</a></p><p>在作业中构建的模型实际上是一个简化了很多的模型</p><p>具体的实现可以看我的Github上的代码</p><p><a href="https://github.com/Mark-Sky/cs224n-assignment/tree/main/assignment3">https://github.com/Mark-Sky/cs224n-assignment/tree/main/assignment3</a></p><h1 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h1><h2 id="Adam-Optimization"><a href="#Adam-Optimization" class="headerlink" title="Adam Optimization"></a>Adam Optimization</h2><p>该优化方法是对随机梯度下降方法（SGD）的改进</p><p>令在一轮的训练中，参数$\theta$的导数为$\nabla \theta$，学习率为$lr$</p><p>传统梯度下降方法下，参数的更新公式为<br>$$<br>\theta \gets \theta-lr\times\nabla\theta<br>$$<br>在Adam方法中，超参数除了学习率$lr$外，还有$\beta_1,\beta_2,\alpha$<br>$$<br>m\gets\beta_1m+(1-\beta_1)\nabla\theta<br>$$</p><p>$$<br>v\gets \beta_2v+(1-\beta_2)(\nabla\theta\odot\nabla\theta)<br>$$</p><p>$$<br>\theta\gets\theta-\alpha m /\sqrt{v}<br>$$</p><p>$\odot$表示向量逐元素相乘的意思，而这里的除号也是逐元素相除的意思。</p><p>m和v分别表示一阶矩估计（平均值）和二阶矩估计（方差）</p><p>之前比赛的时候用的BERT模型里面也用到了这种更新参数的方法，但是我不太懂这个方法的原理（数学推导太复杂了懒得琢磨hh）。就知道它对深度神经网络模型有更好的收敛速度</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout是一种正则化的方法，用于降低过拟合的风险。</p><p>具体就是把<strong>隐层的神经元输出随机置为0</strong>，相当于在加了一层dropout层，但是不是只是把抽中的神经元输出置为0，还需要把没被抽中的神经元的输出乘上一个系数，使得隐层神经元的输出的期望值和没有dropout时一样<br>$$<br>h_{drop}= \gamma d\odot h<br>$$<br>其中$d$是一个和隐层神经元个数一致的向量，其值为0或1，有$p$的概率为0，有$1-p$的概率为1</p><p>$\gamma$是与$p$相关的量<br>$$<br>E_{p_{drop}}[h_{drop}]_i=h_i<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden_layer = activate_f(torch.matmul(x, self.embed_to_hidden_weight) + self.embed_to_hidden_bias)<br>hidden_layer = self.dropout(hidden_layer)<br></code></pre></td></tr></table></figure><p>需要注意的是，<strong>这个做法只在训练的时候用</strong>，在预测的阶段是不会用dropout的</p><blockquote><p> Note: In PyTorch we can signify train versus test and automatically have the Dropout Layer applied and removed, accordingly, by specifying whether we are training, <code>model.train()</code>, or evaluating, <code>model.eval()</code></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">parser.model.train() <span class="hljs-comment"># Places model in &quot;train&quot; mode, i.e. apply dropout layer</span><br></code></pre></td></tr></table></figure><h1 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h1><h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>依存句法分析器分析的是句子的语法结构，建立核心词和修饰该核心词的单词之间的关系</p><p>举个简单的例子：</p><blockquote><p>るるのことが好き</p></blockquote><p>若「好き」作为核心词，则存在两个与「好き」相关的依存关系</p><p>好き→が        </p><p>好き→こと</p><p>一般来说，有三种类型的依存句法分析器</p><ul><li>基于转换的分析器</li><li>基于图的分析器</li><li>基于特征的分析器</li></ul><p>作业里面用到的是第一种，基于转换的分析器</p><p>分析器需要三个存储列表<strong>Stack，Buffer，New_dependency</strong></p><p>而转换一共有三大类转换（简化）</p><ul><li>SHIFT:removes the first word from the buffer and pushes it onto the stack.</li><li>LEFT-ARC:marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack, adding a $first\quad word$  $\rightarrow$  $second\quad word$  dependency to the dependency list.</li><li>RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack, adding a $second\quad word$ $\to $$first \quad word $  dependency to the dependency list.</li></ul><p>我们用一个简单的例子来看看运行过程</p><blockquote><p>I parsed this sentence correctly</p></blockquote><p>对上面的句子进行语法结构分析，最终应该得到的语法结构如下</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211007152526.png"></p><p>而对应的部分中间过程如下</p><table><thead><tr><th>Stack</th><th>Buffer</th><th>New Dependency</th><th>Transition</th></tr></thead><tbody><tr><td>[ROOT]</td><td>[I,parsed,this,sentence,correctly]</td><td></td><td>Initial Configuration</td></tr><tr><td>[ROOT,I]</td><td>[parsed,this,sentence,correctly]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,I,parsed]</td><td>[this,sentence,correctly]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,parsed]</td><td>[this,sentence,correctly]</td><td>parsed$\to$I</td><td>LEFT-ARC</td></tr></tbody></table><p>最后完成句子解析的时候，Stack的大小为1，只剩下ROOT一个元素，Buffer为空，我们所需要的就是得到的单词和单词之间的依存关系，也就是New Dependency里的元素，该列表的元素为元组形式，如$(parsed,I)$</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211007185353.png"></p><p>输入：句子的集合（比如说一篇文章），依存句法解析模型</p><p>初始化：对于输入的每一个句子，都初始化一个对应的解析模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">partial_parses = [PartialParse(sentences[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sentences))]<br></code></pre></td></tr></table></figure><p>​                再创建一个该解析器集合的浅复制（也就是两个列表中的引用的对象实际上是一致的）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">unfinished_parses = partial_parses[:]<br></code></pre></td></tr></table></figure><p>while unfinished_parses is not empty do</p><p>​    从unfinished_parses中取出batch_size个句法解析对象作为minibatch(简单起见直接取前batch_size个)</p><div class="note note-success">            <p>使用批量处理可以更高效使用CPU或GPU</p>          </div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">minibatch = unfinished_parses[:batch_size]<br></code></pre></td></tr></table></figure><p>对于minibatch中的每个句子，用模型预测下一个转换应该是哪一步,也就是从SHIFT,LEFT-ARC,RIGHT-ARC三种转换中选择一种，然后让解析器执行步骤</p><p>model显然传回来的是一个执行步骤列表，如[SHIFT,LEFT-ARC,RIGHT-ARC,LEFT-ARC,SHIFT…..]，对于每一个句子都预测当前的下一步应该执行哪种转换</p><p>当有一个解析器里对应的buffer为空，且stack的大小为1时，就把该解析器从unfinished_parses列表中移除</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> unfinished_parses:<br>     minibatch = unfinished_parses[:batch_size]<br>     transitions = model.predict(minibatch)<br>     <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(transitions)):<br>         minibatch[i].parse_step(transitions[i])<br>         <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> minibatch[i].buffer <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(minibatch[i].stack) == <span class="hljs-number">1</span>:<br>            unfinished_parses.remove(unfinished_parses[i])<br>            <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><p>最后得到所有输入的句子对应的依存关系的列表作为输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dependencies = [i.dependencies <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> partial_parses]<br></code></pre></td></tr></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>从上面的算法可以看到，整个算法最重要的就是对于当前解析器中的状态（Stack，Buffer，New Dependency），我们需要预测下一个步骤要做什么，是SHIFT，LEFT-ARC，还是RIGHT-ARC</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>首先需要搞清楚输入的x是什么</p><blockquote><p>First, the model extracts a feature vector representing the current state.</p></blockquote><p>也就是说，模型会自动抓取当前解析器的状态，转换为一个向量，大小为<strong>n_features</strong></p><blockquote><p>This feature vector consists of a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.).</p></blockquote><p>该向量的每一个元素对应一个单词在词典中的索引，而该词典里面每行对应一个单词，以及已经训练好的单词向量（单词向量用于表示该单词的语义），向量大小为<strong>embed_size</strong>，例如</p><p>‘’big’’    (-2.33056, 0.833901, -0.819715, 0.614493, -0.731628, 1.97627, -0.575884, -0.134451…..)</p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>这里学到了一个新的技巧，在初始化模型时用nn.Parameter()函数来绑定模型里面的所需要更新的参数，在后面需要反向传播时，会自动更新模型里面定义的参数</p><blockquote><p>nn.Parameter()</p></blockquote><p>在模型中，有下面参数需要更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">self.embeddings = nn.Parameter(torch.tensor(embeddings))<br>self.embed_to_hidden_weight = nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.embed_size * self.n_features, self.hidden_size)))<br>self.embed_to_hidden_bias = nn.Parameter(nn.init.uniform_(torch.empty(<span class="hljs-number">1</span>)))<br>self.dropout = nn.Dropout(p=dropout_prob)<br>self.hidden_to_logits_weight = nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.hidden_size, self.n_classes)))<br>self.hidden_to_logits_bias = nn.Parameter(nn.init.uniform_(torch.empty(<span class="hljs-number">1</span>)))<br></code></pre></td></tr></table></figure><p>分别是，单词向量的编码，编码层到隐层的权重、阈值，隐层到输出层的权重、阈值</p><div class="note note-success">            <p>虽然单词向量的编码一开始是使用已经训练好的单词向量，但是在训练时还是会对其进行调整更新，以使得该单词向量更符合该任务</p>          </div><h3 id="前向"><a href="#前向" class="headerlink" title="前向"></a>前向</h3><p>$$<br>x = [E_{w_1},…,E_{w_m}]\in R^{n-features}<br>$$</p><p>其中$E$表示单词的词典，$E_{w_1}$则表示词典中的第$w_1$行，是当前状态的其中一个特征（也就是一个单词）<br>$$<br>h=RELU(xW+b_1)<br>$$</p><p>$$<br>l=hU+b_2<br>$$</p><p>$$<br>\hat{y}=softmax(l)<br>$$</p><p>损失函数定义为<br>$$<br>J(\theta)=CE(y,\hat{y})=-\sum\limits_{i=1}^3 y_i\log \hat{y_i}<br>$$<br>输出则为<br>$$<br>(\hat{y_1},\hat{y_2},\hat{y_3})<br>$$<br>每个y对应着对应转换动作的概率（因为是用softmax函数）</p><blockquote><p>Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss</p></blockquote><p>我们在forward函数中，最后只返回logits，而没有用softmax函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, w</span>):</span><br>x = self.embedding_lookup(w) <span class="hljs-comment">#(batch_size, n_features * embed_size)</span><br>    activate_f = nn.ReLU()<br>    hidden_layer = activate_f(torch.matmul(x, self.embed_to_hidden_weight) + self.embed_to_hidden_bias)<br>    hidden_layer = self.dropout(hidden_layer)<br>    logits = torch.matmul(hidden_layer, self.hidden_to_logits_weight) + self.hidden_to_logits_bias<br>    <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><h3 id="后向"><a href="#后向" class="headerlink" title="后向"></a>后向</h3><p>只需要很简单的几行代码就可以自动根据损失函数来更新对应的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">logits = parser.model.forward(train_x)<br>loss = loss_func(logits, train_y)<br>loss.backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure><p>当然，这里使用的参数更新方法是上面所说的Adam Optimization，损失函数是交叉熵损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.Adam(parser.model.parameters(), lr=lr)<br>loss_func = torch.nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><h1 id="后话"><a href="#后话" class="headerlink" title="后话"></a>后话</h1><p>作业中有很多地方对论文的模型做了简化，其实挺可惜的，没体现出那篇论文的精髓</p><p>比如说在神经网络的输入，作业中只选择了当前解析器状态的Stack和Buffer中的单词，但是没利用单词的属性</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211007212445.png"></p><p>除了单词向量外，论文中还使用到了POS tags（词性标签），和单词之间的关系属性arc labels</p><div class="note note-primary">            <p>POS tags  ={NN,NNP,NNS,DT,JJ,….}</p><p>arc labels = {amod, tmod, nsubj, csubj, dobj}</p>          </div><p>而且隐层的激活函数使用的是3次函数，它的直观的想法就是因为存在三类的输入（words, POS tags, arc labels），所以用三次函数可以得到三类输入的各种相互的关系</p><blockquote><p>Intuitively, every hidden unit is computed by a (non-linear) mapping on a weighted sum of input units plus a bias. Using $g(x) = x^3$ can model the product terms of $x_ix_jx_k$ for any three different elements at the input layer directly:</p></blockquote><p>$$<br>g(w_1x_1+…+w_mx_m+b)=\sum\limits_{i,j,k}(w_iw_jw_k)x_ix_jx_k+\sum\limits_{i,j}b(w_iw_j)x_ix_j+…<br>$$</p><h2 id="错误类型"><a href="#错误类型" class="headerlink" title="错误类型"></a>错误类型</h2><p>算法最后所得到的语法结构依存关系的错误类型可以分为4类</p><ul><li>介词短语附加错误</li><li>动词短语附加错误</li><li>修饰词附加错误</li><li>协调词附加错误</li></ul><p>介词短语附加错误：</p><p>例子：Moscow sent troops into Afghanistan.</p><p>Afghanistan是表示sent这个动作的地点的（介词）</p><p>而错误的则为比如说：Afghanistan与troops有关系</p><p>动词短语附加错误：</p><p>例子：Leaving the store unattended, I went outside to watch the parade</p><p>Leaving应该是修饰went的时间的，链接到其他单词上则为错误</p><p>修饰词附加错误：</p><p>例子：I am extremely short</p><p>extremely是修饰short的，链接到其他单词上则为错误</p><p>协调词附加错误：</p><p>其实我不知道正确的翻译是什么（Coordination Attachment Error），但是意思还是很明确的，就是and，or，but等连词的两边的单词之间的关系</p><p>例子：Would you like brown rice or garlic naan?</p><p>naan和rice是有同位的关系的，链接到其他单词上则为错误</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北岭山游记</title>
    <link href="/2021/10/02/%E5%8C%97%E5%B2%AD%E5%B1%B1%E6%B8%B8%E8%AE%B0/"/>
    <url>/2021/10/02/%E5%8C%97%E5%B2%AD%E5%B1%B1%E6%B8%B8%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>在中秋的第二天，突然兴起想去爬一次山，主要还是想在山顶上试试新买的露营椅子，体验一下大自然的感觉。于是叫上户外爱好者泰航出来向山进发！</p><p>那天的天气很好，早上的时候就是蔚蓝澄澈的天空，太阳还不是那么猛，路上的木漏日很惬意。</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2031.JPG"></p><p>整段路分为两段，前面的是平缓的，基本上跟鼎湖山的感觉差不多，比较适合散步吧，那时的我还觉得就这就这？也一直在和泰航聊天，完全没感觉有什么压力</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2032.JPG"></p><p>走了一个小时的时候，看地图知道已经走了一半的路程了，感觉胜利在望，但是遇到了一个阿伯，当他跟我们说从这里到山顶至少还要一个半小时的时候，我们是不信的哈哈，（我们的体力跟老年人怎么能比？？）</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2041.JPG"></p><p>但是到了一个休息的平台之后，就开始陡峭的部分了，开始痛苦面具模式，基本上上一步楼梯就要休息一下，跟着前面的人是看着前面的人的脚前进的。山上没什么树遮太阳，身上的衣服都湿透了，只有不时吹过的凉风，没其他声音，所以自己的心跳声变得很明显，每上一步都感觉自己的心跳跳的很快，实在是想不懂为什么走的这么慢但是会有这么高的心率。</p><p>中间还见到了很别致的座椅，很小的一块地方，坐在椅子上就是面对山下的风景，坐着就能看到很远的地方，但是那时又累又热，完全没有那个心情坐了</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2044.JPG"></p><p>每上一小段就会有休息的地方，我们坐了几次吧，两个人的汗直接印出两个印，哈哈让人想起了打完篮球坐在地上的时候的感觉</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2045.JPG"></p><p>山顶的风比较多，凉风吹过被汗浸湿的贴在身上衣服很让人难受，于是我和泰航直接赤膊（反正不是美少女哈哈</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2047.JPG"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2054.JPG"></p><p>到山顶上时居然有一堆登山少女，我都爬的这么痛苦，真的很意外会有这么多女生会来登山。</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2061.JPG"></p><p>不管怎么样，到山顶时心情还是很舒畅的，特别是坐在露营椅上面的时候，真是不愧我这么辛苦背个椅子上山顶了。就坐在山顶上发呆，看着蔚蓝的天空，还有感觉就在同一个水平上面的白云，感受不时吹过的凉风，真的是别有一番感觉</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2059.JPG"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2081(20210928-105037).JPG"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2061.JPG"></p><p>可惜的是到山顶时已经11点了，还要下山去吃饭，于是只坐了20多分钟就下山了，感觉下次可以在山顶吃了，坐久一点，也不用赶下山。下山走了条车路，又是一波痛苦面具</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2074.JPG"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2075.JPG"></p><p>下到山脚下，看到了个凉亭，就坐在那里等车，躺在那里，看着好像还是很矮的丰满的白云，很累，但是也很惬意。</p><p>中午到了大润发，喝了久违的贡茶，也吃了久违的和家寿司，每个假期回来都会去一次啊哈哈，真的是陪伴我从初中走到大学了</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2077.JPG"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2078.JPG"></p><p>（おわり）</p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Climbing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动漫台词抄录</title>
    <link href="/2021/10/02/%E5%8A%A8%E6%BC%AB%E5%8F%B0%E8%AF%8D%E6%8A%84%E5%BD%95/"/>
    <url>/2021/10/02/%E5%8A%A8%E6%BC%AB%E5%8F%B0%E8%AF%8D%E6%8A%84%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>感觉到了大三开始变得闲了下来，总要找点事干干。这个将会是一个持续更新的系列，排名不分先后，想到哪个就会更新，算是记录笔者的二次元轨迹吧。当然是用神番赛马娘东海帝皇（トウカイテイオー）镇楼</p><p>ねえねえねえ、どうしてみんな見てくれないの、こんなに真面目なのに</p><p><img src="https://i.loli.net/2021/09/21/yxD46jrE3klfRgS.png"></p><h1 id="恋爱小行星"><a href="#恋爱小行星" class="headerlink" title="恋爱小行星"></a>恋爱小行星</h1><p>这部番是我第一步追着更新看的番，真的是老兵回忆录了，看到完结撒花的时候真的很舍不得</p><p>从那时候完全不懂她们在说什么到现在能听译下来</p><div class="note note-success">            <p>みんな　好きなものや得意なもの　その人の世界を持ってる</p><p>ひとりでいたら　世界はひとつだけと</p><p>それが繋がったら　たくさんの可能性がどんどん広がって</p><p>大きくて　未知数で</p><p>宇宙みたい</p>          </div><div class="note note-primary">            <p>大家热爱的事物，擅长的事物都不同，都拥有自己的世界</p><p>一个人的话只有一个世界</p><p>这些互相连接的话</p><p>就会展开许多的可能性</p><p>广大 未知</p><p>就像是宇宙一样</p>          </div><p><img src="https://i.loli.net/2021/09/18/clP5r6pnWGstjDL.png"></p><p><img src="https://i.loli.net/2021/09/18/hOKpePdcbrQV1xF.png"></p><p><img src="https://i.loli.net/2021/09/18/KXYB69nemGhw5Vs.png">     <img src="https://i.loli.net/2021/09/18/LwTG9x4ioCHXNqW.png"></p><p>这段话配上背景的气氛和配乐真的很有感觉啊，番里面出现了几个只出现了很短时间的配角，但是每个配角都没有强行插入的感觉，只是日常很普通的相遇，然后交流，到分开，每个人都给这故事的日常的感觉增色许多，每个人都会有自己的世界，和主角团她们相遇，让故事里的世界更加丰富多彩。因为是一部日常番，所以并没有必要插入配角来推动情节发展，加入的这些配角或许只是让结尾的这段话更有含义吧。每个人相遇在一起，所能创造的世界就和那星空一样，壮美，深邃。还有米拉的眼睛，画的也很像宇宙一样漂亮</p><hr><h1 id="紫罗兰永恒花园"><a href="#紫罗兰永恒花园" class="headerlink" title="紫罗兰永恒花园"></a>紫罗兰永恒花园</h1><p>京紫前几集看的其实是有点血压上升，隔了很久才重新拾起，但是到了4集以后每集都是爆好看啊，我觉得这个可以更很多个场景台词有没有</p><h2 id="在某处的星空下"><a href="#在某处的星空下" class="headerlink" title="在某处的星空下"></a>在某处的星空下</h2><p><img src="https://i.loli.net/2021/09/21/92ZKNQAtS4uai5G.png"></p><div class="note note-success">            <p>旅先で再び彼女と会える可能性がどのくらいあるんだろうか</p><p>もう一度あの彗星を見上げる確率だろうか</p><p>それでも俺はもう躊躇うことはないだろう</p><p>閉じられていた扉の向こうに</p><p>歩き出す勇気を</p><p>彼女がくれたんだから</p><p>いつかきっと</p><p>どこかの星空の下で</p>          </div><p><img src="https://i.loli.net/2021/09/21/qw7M6XhIkiUtrpP.png"></p><div class="note note-primary">            <p>旅途中能再次遇到她的可能性会有多大呢</p><p>是否就像再次目击那颗彗星一样微乎其微</p><p>尽管如此我也不再会犹豫</p><p>因为她已经赐予了我</p><p>走出这扇门的勇气</p><p>有朝一日</p><p>在某处的星空下</p>          </div><p><img src="https://i.loli.net/2021/09/21/TwrNlH9LfkdGvZ4.png"></p><p>实际上这一集的剧情没有其他的催泪，不过它在最后展现的男主的期待的感觉还是挺有感染性的，由原来的自闭封锁，变成对未知的未来充满希望的感觉，配上清晨的蓝天白云的背景，让人感受到了少年心中的希望和激情因为薇妹终于回来了。最后的「どこかの星空の下で」跟中国古代的以意境为结尾的手法很像</p><blockquote><p>孤帆远影碧空尽，唯见长江天际流。</p></blockquote><p>以意境结尾，很有日本人的那种委婉的感觉</p><p>实际上这句话肯定是</p><p>「いつかきっとどこかの星空の下で（彼女と一緒に星空を見上げる）」</p><p>但是把全部说出来倒变得很没意思了，留有一点想象空间，以意境结尾重点就放在了那ほしぞら上面而不是人物的动作上面了，更有一种美的感觉，和「今晩の月は綺麗です」大概是一个道理吧</p><p><img src="https://i.loli.net/2021/09/21/oWOsVZ5mSjdR4Fc.png"></p><p>最后还要再吹一句，每次ED都是神插入啊，真就「星空」啊</p><hr><h1 id="赛马娘二"><a href="#赛马娘二" class="headerlink" title="赛马娘二"></a>赛马娘二</h1><p>这真的是神番啊，有些场面看一次就泪目一次，本来想开一个新的文章来谈谈的，但是反正都是只有自己看，就在这里把想写的写下来吧</p><h2 id="秒表"><a href="#秒表" class="headerlink" title="秒表"></a>秒表</h2><p>这集巨虐。整篇都是笼罩着阴郁的气氛，人们不经意的话语，梦中的无力感…</p><p>最后的一段帮麦昆计时是气氛渲染的最好的一段</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211002215205.png"></p><div class="note note-primary">            <p>あ早いな</p><p>まるで飛んでるみたい</p><p>なんできれいなんだ</p><p>そか、そうなんだ</p><p>ぼくはもうあんな風に走れないんだ</p>          </div><p>夕阳黄昏的背景，配上凄美的弦乐。帝皇看着麦昆的飞快的身影，一开始不是哭了，而是笑了，那笑容才是最虐的啊，导演太懂了，那是对自己已经无法站在赛场上面的释怀啊，不再会感到不甘的帝皇才是最虐的，因为那是绝望，那个永不放弃的帝皇在这里真正的抛弃了自己当初执着追求的梦想。笑着笑着就哭了，那是对自己的命运的不甘，对伙伴们的内疚</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211002215411.png"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211002215600.png"></p><h2 id="绝对，一定"><a href="#绝对，一定" class="headerlink" title="绝对，一定"></a>绝对，一定</h2><div class="note note-primary">            <p>追いつけない背中を</p><p>認めるのは怖かった</p><p>本気の夢だったから</p>          </div><div class="note note-success">            <p>承认自己追不上的背影很让我害怕</p><p>只因那是我真心的梦想</p>          </div><p>これがあきらめないってことだ！　トウカイテイオー</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211002211224.png"></p><p>这里真的是感动的泪目！在前面第五集帝皇跟连败的双涡轮说的</p><blockquote><p>あきらめないってことは大事だからね</p></blockquote><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211002212344.png"></p><p>到第十集双涡轮跟骨折无法再跑的帝皇说这句话。真的是意味深长，跟别人说过的永不放弃，最后自己却放弃了，那是被迫无奈的放弃，职业体育的赛场就是能让一个不会放弃的人也只能放弃的残酷的东西啊</p><p>双涡轮领跑时观众的呼声，解说员的激动的解说，还有若隐若现的背景音乐，双涡轮的嘶喊和喘气声，帝皇在台上愣住的样子，所有的这些加起来，把气氛推向高潮。这里的剧情把前面的抑郁的气氛一扫而空，让人重新看到了希望，帝皇还有这么多支持她的人，这么多希望她复出的伙伴，还有教会她永不放弃的双涡轮！尽管帝皇自己原来已经觉得不可能再跑了，但是她还是选择回应那些支持她的人，也是那些人再一次给了帝皇勇气，希望去再一次奔跑在赛场上，追逐她的梦想。那个梦想似乎没有原来想象中那么遥远了。</p><h1 id="异世界食堂"><a href="#异世界食堂" class="headerlink" title="异世界食堂"></a>异世界食堂</h1><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211107192537596.png"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211107193858513.png" alt="image-20211107193858513"></p><div class="note note-primary">            <p>「店主」それじゃあまだ来週よろしくな</p><p>「アレッタ」はい、もちろんです</p><p>「アレッタ」黒さんも帰り気をつけてくださいね</p><p>「黒」（そんな心配は不要だ、私が住む空の果てには私以外の生物はおらず）</p><p>（まだ、私を害せる存在などあの世界には数える程度しかない、けど）</p><p>「黒」わかった、アレッタも気を付けて</p>          </div><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211107193714689.png"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211107193641577.png"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Picturesimage-20211107193606309.png"></p><p>这是小黑的第一次开口说话吧，晚上结束工作后的时间真的太美好太治愈了~，感觉这部番让我回到了二次元的初心</p><h1 id="阴晴不定大哥哥"><a href="#阴晴不定大哥哥" class="headerlink" title="阴晴不定大哥哥"></a>阴晴不定大哥哥</h1><div class="note note-primary">            <p><strong>もし将来の夢が叶わなかったら　どうすればいいの</strong></p><p>夢ってね　抱えたまま　時が経ってば経つほど</p><p>どんどん重くなっていくんだよ</p><p>夢を持つ続くことで　自分の居場所を見失わずにいられる人がいれば</p><p>手放しほうが楽に歩ける人もいて</p><p>だから良い子のみんながいま持ってる夢は</p><p>抱え続けるのも　どこかおいでいくのもいいことだと　お兄さんは思うよ</p>          </div><p>梦想这东西，抱着越久，就会越发沉重，有一直坚持自己梦想，找到自己的归属的人，也有放弃梦想，觉得这样更能轻松地前行的人，不管怎么样，都是人生，谁也不知道哪样的人生才是好的，那种人生是不好的。尽管可能梦想无法实现，但是也不一定是什么坏事</p><p>里道大哥哥真的是参透了好多的人生哲理啊，太有教育职业者的素养了</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211111224050.png"></p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211111224122.png"></p><p>（未完待续，持续更新中…）</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《日本语》摘录</title>
    <link href="/2021/09/28/%E6%97%A5%E6%9C%AC%E8%AF%AD%E6%91%98%E5%BD%95/"/>
    <url>/2021/09/28/%E6%97%A5%E6%9C%AC%E8%AF%AD%E6%91%98%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="のだ"><a href="#のだ" class="headerlink" title="のだ"></a>のだ</h1><p><strong>日本人在进行言语表达时，喜欢只讲述缘由而让听话人领悟到结论，所有才经常使用”のだ““のです”这一表达形式。</strong>上一节中的“あの子は寂しいんだ”，实际上想表达的可能是“あの子は寂しいので口をきかない”的意思，而“僕は知らなかったのです”中可能隐含着“僕は知らなかったので、先生にお辞儀をしませんでした”的含义</p><p>另外，“のだ”中还包含着“それが事実（真実）だ”的语义，进而派生出很多的用法</p><ul><li>（决心）何と言っても、俺はやり抜くのだ</li><li>（催促）おい、どうした、起きるのだ</li></ul><h1 id="部分否定"><a href="#部分否定" class="headerlink" title="部分否定"></a>部分否定</h1><p><strong>日语中能够变为否定形的只有句末的动词，因此句末出现否定形时，并不能清楚地看出否定的是前面地哪个词</strong>，例如：</p><blockquote><p>車は急にとまれない</p></blockquote><p>其中的「ない」的否定对象既可以是「急に」，也可以是「とまれる」</p><p>为了解决这个问题，前辈们想了很多办法。例如在需要否定的部分插入「は」「しも」「と」等助词，如：</p><blockquote><p>伯楽は常には有らず</p></blockquote><p>其中，「ず」否定的是 「常に」，意思是：</p><blockquote><p>常にいるとは限らない、いないこともある</p></blockquote><p>这与「常に有らず」=「いつもいない」不同</p><p>这里的表达的意思和中文中的“一直不是”和“不是一直”的区别差不多，就是小学学的部分否定</p><p>英语中就是’not all’ and ‘all not’</p><h1 id="人称代词"><a href="#人称代词" class="headerlink" title="人称代词"></a>人称代词</h1><h2 id="使用频率"><a href="#使用频率" class="headerlink" title="使用频率"></a>使用频率</h2><p>英语人称代词的使用频率，占全体词汇的十分之一，其中“I”和“you”用得很多。日语中频繁地说「私が」「私が」<strong>好像是过分主张自己，会给别人带来不快地感觉</strong>。不必使用时不用，通过上下文让对方区去揣摩，这种方式更受欢迎。</p><p>英语的“I” “you”只要不是后续关系代词构成复句，一般不能附加修饰它的词语，但日语中，落语“粗忽者”的台词可以这样说：</p><blockquote><p>ここに死んでいる奴はたしかに俺だが、<strong>それを担いでいるおれ</strong>は一体誰だろう</p></blockquote><p>川端康成应邀到斯德哥尔摩去领取诺贝尔文学奖时，曾做过“美しい日本の私”的演讲，这个演讲题目应该很难译成欧洲语吧。赛登斯迪克在英语里艰难地把它译成了“美しい日本と私”</p><h2 id="词汇之多"><a href="#词汇之多" class="headerlink" title="词汇之多"></a>词汇之多</h2><p>日语中的第一人称代词就有</p><p>わたくし、わたし、わし、あたし、ぼく、おれ、わがはい</p><p>在电话中被问到“どなたですか”时，还有人回答：</p><blockquote><p>あたしよ、あたし</p></blockquote><p>这样回答，除了期待能让对方通过声音特点知道是谁之外，还似乎有这样的心理在起作用：<strong>对于那个人，称呼自己为「あたし」的人应该很少</strong></p><p>なんだかアニメのキャラが出できますが、どうにも思い出せない</p><p>格林童话“狼和七只小山羊”中有这样一段，狼扮母山羊的样子，在门外向小山羊们喊道：</p><blockquote><p>Eure Mutter ist da（お前たちのおかあさんだよ）</p></blockquote><p>笔者小时候读过的日语版的这则故事，喊的是：“<strong>わたしだよ</strong>“那才是日本式的读法</p><h1 id="「は」と「が」"><a href="#「は」と「が」" class="headerlink" title="「は」と「が」"></a>「は」と「が」</h1><p>日语中的“述定句”和“述定+传达句”这样的句子可以分成</p><ul><li>定品句</li><li>物语句</li></ul><p>按我个人的理解，定品句就是「は」的语句，物语句则为「が」，两种句子的中心是不同的</p><p>定品句中的中心是は前面的提示的主题</p><blockquote><p>あれは富士山だ</p></blockquote><p>这个句子的中心就是あれ</p><p>但是在物语句中，句子是谓语的单极结构</p><div class="note note-success">            <p>注意：和中文英文的主谓宾结构不同，日语中物语句的主语实际上是“<strong>主格补语</strong>”，它的地位和“目的格补语”以及其他附加上所谓格助词的“名词”相同，它们都从属于句子中的谓语</p>          </div><div class="note note-primary">            <p>甲が</p><p>乙に　　$\to$      紹介する</p><p>丙を</p>          </div><p>也就是说「が」前面的主语的地位并没有汉语和英语那么高</p><h1 id="日语结尾"><a href="#日语结尾" class="headerlink" title="日语结尾"></a>日语结尾</h1><p><strong>日语句尾非常明确</strong>这点，时常招致文学家的不满。用日语书写过去的事情时，每个句子都以“…た”结尾。佐藤春夫曾称之为“似是喇叭调的和声，又像是口吃一般”。如果在乎这点的话，那确实会感觉不舒服。但是，句子结尾明确，本来是件感觉很舒服的事情。至少，旨在正确传达作者意图的文章中，句尾明确是一大优点。从不同的角度去看，甚或从美学观点来看，句子结束处用一定的音节或文字，也可以看做一种形式美吧</p><div class="note note-primary">            <p>从上面的这一点，可以推出日语歌的一些特点以及好听的原因。</p>          </div><p>在前面的从语音看日语里写过，元音之中好像也有好听的和不好听的音。声乐家四家文子生前说过，唱歌的时候，要把声音拉高，这时候后面跟的是什么音，就决定了这首歌好唱还是不好唱。据她说，好唱的歌后面跟的是a和o，不好唱的歌后面跟的是i，而e和u居中。</p><p><strong>5个元音ア、イ、ウ、エ、オ之中，ア、オ才是最好听的音，イ是最难听的音。</strong>或者可以说，因为イ是元音中最接近辅音的，所以是最不好听的音。</p><p>拿YOASOBI的新歌「ラブレター」举例</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20211004192110.png"></p><div class="note note-success">            <p>あぁ　受け取ってどうか私の想い<strong>を</strong></p><p>さぁ笑って泣いてそんな毎日<strong>を</strong></p><p>歩いていくんだいつもいつまで<strong>も</strong></p><p>辛い暗い痛いこともたくさんあるけ<strong>ど</strong></p><p>この世界はいつでもどこでも音楽で溢れてる</p><p>目の前のことも将来のこと<strong>も</strong></p><p>不安になってどうしたらいい<strong>の</strong></p><p>分かんなくて迷うこともあるけ<strong>ど</strong></p><p>そんな時もきっとあなたがいてくれれ<strong>ば</strong></p><p>前を向けるん<strong>だ</strong></p><p>こんな気持ちになるの<strong>は</strong></p><p>こんな想いができるの<strong>は</strong></p><p>きっと音楽だけなん<strong>だ</strong></p><p>どうか千年先もどうか鳴り止まないで</p><p>あぁ　いつも本当にありが<strong>とう</strong></p>          </div><p>一共有8次以o结尾，5次以a结尾，u和e各一次。日语句法多以だ，た结尾，本身就符合上述的好听的元音结尾的说法，而且还有那些表示作者主观意图的词语一般出现在句子的结尾，如「ね」「よ」「な」「わ」都是a和o元音。</p><p>或许上面说的这些就是日语歌比较好听的部分原因吧</p><h1 id="清音和浊音"><a href="#清音和浊音" class="headerlink" title="清音和浊音"></a>清音和浊音</h1><p>关于日语的辅音有一个重要的问题，就是比之カ行，サ行…..的差异，清音与浊音的差异更具表述效果。<strong>清音有细小，清纯，迅速的感觉</strong>，比如说，「コロコロ」，形容的就是水珠在荷叶上滚动的那种情形，而如果说「ゴロゴロ」，则是大的，粗的，迟缓的感觉，就好比相扑力士在比赛的土台上滚动的感觉。说「キラキラ」那是宝石的光芒，而「ギラギラ」则是形容好比是蝮蛇的眼睛放光的那种情景。</p><p>从一般的名词，形容词等来看，最显而易见的是清浊音与美丑的关系，<strong>和语词汇中浊音打头的单词很多都是带有肮脏的感觉的</strong>，如</p><div class="note note-warning">            <p>ドブー溝　下水道</p><p>ビリ　最后，倒数第一</p><p>ドロー泥</p><p>ゴミ　垃圾</p><p>ゲター下駄　木屐</p>          </div><p>正因如此，女孩子的名字以浊音开头的极少，比如「バラ」，虽说是很美的花，但我从未听到有哪个女孩子叫「バラ子」的</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>近端梯度下降</title>
    <link href="/2021/09/23/%E8%BF%91%E7%AB%AF%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <url>/2021/09/23/%E8%BF%91%E7%AB%AF%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>为了防止过拟合的现象，我们通常会对目标函数引入正则项，由于我们的重点是近端梯度下降，所以我们以最简单的线性回归模型为例，以平方误差为损失函数<br>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2<br>$$<br>若希望参数$w$的分量均衡，也即非零分量个数尽量稠密，则用$L_2$范数<br>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2+\lambda ||w||_2^2<br>$$</p><p>若希望非零分量个数尽量少，则使用$L_1$范数<br>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2+\lambda ||w||_1<br>$$<br>上面的这些都是西瓜书里面直接列出的陈述，我们再用画图的方法来观察一下为什么使用$L_1$范数的时候，得到的参数会比较稀疏</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20210923191125.png"></p><p>假设x仅有两个特征属性，那么$w$就只有两个分量$w_1, w_2$。</p><p>图中的平方误差项等值线也就对应着经验误差<br>$$<br>\sum\limits_{i=1}^m (y_i-w^Tx_i)^2<br>$$<br>该项看作二次函数，则可以画出对应的椭圆，实际上相当于标准椭圆移位<br>$$<br>\frac{w_1^2}{a^2}+\frac{w_2^2}{b^2}=1<br>$$<br>于是就得到了平方误差等值线，越靠近中心，则说明经验误差越小</p><p>同样的，若是$L_1$范数，等值线则为下，k越大说明离中心越远，结构误差项越大，而且显然该方程对应图中的正方形<br>$$<br>|w_1|+|w_2|=k<br>$$<br>若是$L_2$范数，等值线为下，k越大说明离中心越远，结构误差项越大，该方程对应图中的同心圆<br>$$<br>w_1^2+w_2^2=k<br>$$<br>我们引入正则项，在图中表现为，本来我们只需要取到一个较小的椭圆上面的点，但现在需要<strong>既使得该点靠近经验误差对应的椭圆的中心，又靠近结构误差对应的正方形的中心</strong></p><p>而在图中可以直观看到若选择$L_1$范数作为正则项，交点一般出现在正方形的顶点处，而这些顶点就是其中的某一个分量为0的点，因此可以使得参数的分量尽量的稀疏</p><h1 id="L1正则化问题"><a href="#L1正则化问题" class="headerlink" title="L1正则化问题"></a>L1正则化问题</h1><p>$$<br>\min\limits_w \sum\limits_{i=1}^m (y_i-w^Tx_i)^2+\lambda ||w||_1<br>$$</p><p>$L_1$范数正则化的问题和$L_2$范数正则化的问题求解是不同的，因为$L_2$范数是连续可导的，因此可以使用随机梯度下降法（SGD）- 通过对当前所处点求导数然后沿着导数的方向下降，若是凸优化问题，则最后可以得到一个最优解。</p><p>但是$L_1$ <strong>范数不是一直可导的</strong>，因为绝对值的存在，它在那些0点都是不可导的，从上面的图就可以很清楚直观看出来。因此不能直接求导然后更新参数</p><h1 id="近端梯度下降（PGD）"><a href="#近端梯度下降（PGD）" class="headerlink" title="近端梯度下降（PGD）"></a>近端梯度下降（PGD）</h1><blockquote><p>与经典的梯度下降法和随机梯度下降法相比，近端梯度下降法的适用范围相对狭窄。对于凸优化问题，当其目标函数存在不可微部分（例如目标函数中有 L1-范数或迹范数）时，近端梯度下降法才会派上用场。</p></blockquote><p>近端梯度下降所适用的函数一般如下：<br>$$<br>h(x)=f(x)+g(x)<br>$$<br>其中$f(x)$为可微的凸函数，$g(x)$为不可微或局部不可微的凸函数</p><p>这里换一个记号，把例子中的样本数据x看作常数，把w看作函数的参数<br>$$<br>f(w)=\sum\limits_{i=1}^m (y_i-w^Tx_i)^2<br>$$</p><p>$$<br>g(w)=\lambda ||w||_1<br>$$</p><ul><li><p>写出$f(w)$的二阶泰勒展式<br>$$<br>f(w)=f(w_k)+&lt;\nabla f(w_k), w-w_k&gt;+\frac{1}{2}(w-w_k)^T\frac{\partial^2 f(w_k)}{\partial w_k^2}(w-w_k)<br>$$</p></li><li><p>假设$f(x)$满足L-Lipschitz条件，即存在常数$L&gt;0$，使得<br>$$<br>||\nabla f(x’)-\nabla f(x) ||\leq L|||x’-x||^2_2<br>$$</p></li><li><p>将上式代入到二阶泰勒展式，然后进行配方</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20210923231359.png"></p></li><li><p>代入到原问题中，可以得到（求和项的每个$f$函数都是对应样本不同的）<br>$$<br>\min\limits_w\sum\limits_{i=1}^m\frac{L}{2}||w-(w_k-\frac{1}{L}\nabla f(w_k))||^2+\lambda||w||_1<br>$$</p></li><li><p>每次在$w_k$的附近寻找最优点，不断迭代<br>$$<br>x_{k+1}=\min\limits_w\sum\limits_{i=1}^m\frac{L}{2}||w-(w_k-\frac{1}{L}\nabla f(w_k))||^2+\lambda||w||_1<br>$$</p></li><li><p>假设$z=w_k-1/L\nabla f(w_k)$，上式有闭式解（实际上就相当于对一个有多种情况的二次函数进行分类讨论，每种情况都会有其闭式解，只需要得到最小的就行）</p></li></ul><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20Pictures20210923231312.png"></p><p>从上面的式子画图，我们可以发现，正则项相当于把$x^i_{k+1}=z^i$这个线性函数向中间拉扯了</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesE77E9F23B1777EE772381A6E7E467C9A.png"></p><p>这也符合了$L_1$正则项能使得参数的分量尽量稀疏的说法</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>信息熵的解释</title>
    <link href="/2021/09/16/%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E8%A7%A3%E9%87%8A/"/>
    <url>/2021/09/16/%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E8%A7%A3%E9%87%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="数学定义"><a href="#数学定义" class="headerlink" title="数学定义"></a>数学定义</h1><p>给定数据集$D$，D中第$i$类样本所占比例为$p_i(i=1,2,…,|Y|)$</p><p>则该数据集的信息熵定义为<br>$$<br>Ent(D)= -\sum\limits_{i=1}^{|Y|}p_klog_2p_k<br>$$</p><h1 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h1><p>信息熵可以理解为**$\frac{1}{p_k}$​的二进制编码长度的数学期望<strong>，更进一步，可以理解为</strong>数据集$D$的各个类别的稀有度的数学期望**</p><h2 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h2><p>对于信息熵的数学定义，可以看成是如下数学期望<br>$$<br>E[log_2\frac{1}{p_k}]=\sum\limits_{i=1}^{|Y|}p_klog_2\frac{1}{p_k}<br>$$<br>$X_k = log_2\frac{1}{p_k}$表示随机变量X的分类为k时，其值$x_k$为$log_2\frac{1}{p_k}$<br>$$<br>E[X]=\sum\limits_{i=1}^{|Y|}p_kx_k<br>$$</p><h2 id="稀有度"><a href="#稀有度" class="headerlink" title="稀有度"></a>稀有度</h2><p>$1/p_k$ 就表示该类别的稀有度，该值越大，则表示稀有程度越高</p><p>而$log_2$则表示对稀有度进行二进制编码的长度，$log_21/p_k$​​越大，表示所需编码长度就越长。这个也不难理解，<strong>越稀有的事物，则描述难度就越高</strong>，而越常见的事物，描述就越简单。比如说，在二次元里，黄毛是很常见的，我们只需要用两个字就可以表示出来，但是最后输了的傲娇黄毛是比较稀缺的，我们需要用9个字才表示出来这个意思。在上面的自然语言例子里面，换一个角度看，我们只描述一个普遍的东西是比较简单的，只需要用一个对应的名词就可以表示出来，但是若在名词前面加上定语之类的，那么显然该事物的稀有程度就上升了</p><p><img src="https://i.loli.net/2021/09/16/Ulr2NmF4fiQwW6b.png"></p><p>因此$log_21/p_k$同样表示类别的稀有程度</p><h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>$Ent(D)$越大，表示该数据集的存在的不同的类别越多，而且稀有类别的类别数也越多。</p><p>极端地想，$D$数据集如下</p><p>${1,2,3,4,5}$</p><p>该数据集的每个类别的稀有程度都一样高，都只有$1/5$​，而且类别的数目也很多，则信息熵也会比较高</p><p>而换一种比较高的信息熵的数据集，还可以像是下面这样</p><p>${1,1,1,1,1,5}$</p><p>该数据集存在一个稀有类别5，该稀有类别所占的期望会很大，导致信息熵比较高</p><p>我们在决策树中做特征集划分的时候，就是要取出一个特征，使得用该特征可以把那些稀有类别都剔除出去，稀有类别为一个新的数据子集，还是用上面的例子，就是希望取出一个特征，能把上面的数据集划分成两个数据子集</p><p>${1,1,1,1}$和${5}$</p><p>如此，两个子集都不存在稀有类别了，在子集的范围内，1和5都是常见类别了，两个集合的信息熵都变为了0</p><h1 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h1><p>$$<br>Gain(A)=Ent(D)-\sum\limits_{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)<br>$$</p><p>极端地想，若我们划分出来的子集全部都不存在稀有类别，则$Ent(D^v)$​全都等于0，则该特征选取是最好的</p><p>稀有程度的期望值越小，则$Gain(A)$的值就越大，该特征选取就越好</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>降维与度量学习</title>
    <link href="/2021/09/16/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    <url>/2021/09/16/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这篇博客并不打算把下面的所有内容都推导整理一遍，而只是把上课时老师讲的比较有意思的记录下来，也结合一些自己的理解吧。毕竟如果把所有内容都推导一遍那就相当于抄一遍书了</p><h1 id="k近邻学习"><a href="#k近邻学习" class="headerlink" title="k近邻学习"></a>k近邻学习</h1><p>k近邻学习是懒惰学习的代表。指的是其在得到训练集的时候不学习，而是在得到了测试样本时再运行，实际上这个大概不属于传统意义上的机器学习，因为它没有模型参数之类的需要用训练集训练得到的，最简单的k近邻学习的唯一一个参数就是k，还是超参数</p><p>注意区分k近邻学习和k-means聚类学习，前者是监督学习算法，而后者是无监督学习</p><p>k近邻学习看起来好像和后面的内容没什么关系，实际上k近邻的一个重要部分就是距离计算</p><ul><li>选择不同的距离计算也就和后面的度量有关</li><li>如何简化距离的计算和降维有关</li></ul><h2 id="NCA"><a href="#NCA" class="headerlink" title="NCA"></a>NCA</h2><p>NCA是k近邻学习的进阶版，它不是懒惰学习，而是需要学习马氏距离中的半正定矩阵$M$，这个是后面的度量学习的内容，这里只分析NCA的意义</p><p>若NCA只关心k近邻学习的距离计算，那么其学习的度量矩阵和样本的维度$d$，相对应为$d\times d$​​。由于k近邻的距离计算就是普通的欧氏距离，所以有可能对于一些非线性空间的数据集的分类效果不是很好</p><p><img src="https://i.loli.net/2021/09/16/dwZgTptXSPGDsvH.png"></p><p>如上图，要学习的$d(x_i,x_j)=\sqrt{(x_i-x_j)^TM(x_i-x_j)}$，能使得两个点的距离并不是欧氏距离而是测地线距离，才能使得分类效果较好</p><p>若NCA还同时希望能使得距离计算的复杂度下降，那么需要如下转换一下$M$<br>$$<br>M = A^TA<br>$$<br>其中$A$为$d’\times d$​的矩阵，如此计算距离的式子可以变为如下<br>$$<br>d(x_i, x_j)=\sqrt{(Ax_i-Ax_j)^T(Ax_i-Ax_j)}<br>$$<br>就相当于把样本的维度降到了$d’$</p><p>这样就解决了k近邻的维数灾难问题</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>k近邻分类器在判别分类时通常用多数投票法，但是NCA为了使得目标函数方便优化，替换为概率投票法，这样容易写出目标函数，也容易对目标函数进行优化，因为这样目标函数就是连续函数，NCA的优化目标：<br>$$<br>\min\limits_P\quad1-\sum\limits_{i=1}^m\sum\limits_{j\in \Omega_i}\frac{exp(-||A^Tx_i-A^Tx_j||^2_2)}{\sum_lexp(-||A^Tx_i-A^Tx_l|^2_2|)}<br>$$<br>目标函数的里层实际上类似于一个softmax函数，计算的是一种概率，表示任意样本$x_j$对$x_i$分类结果影响的概率</p><h1 id="低维嵌入"><a href="#低维嵌入" class="headerlink" title="低维嵌入"></a>低维嵌入</h1><p><strong>低维嵌入和主成分分析一样，都是一种对数据降维的方法，但是其目标和PCA是不同的</strong></p><p>它的目标是寻找一个子空间，使得子空间的距离和样本在原来的空间的距离是尽量保持一致的</p><p>它可以转换为另一个问题：<strong>如何在低维子空间和高维空间之间保持样本之间的内积不变</strong>？</p><p>设样本维度为$d$，样本个数为$m$，样本之间的内积矩阵在低维和高维空间都是$B(m\times m)$​，可以对其进行特征值分解<br>$$<br>B= V\Lambda V^T<br>$$<br>$V$为$m\times d$矩阵，$\Lambda$为$d\times d$矩阵</p><p>然后只取$\Lambda$​中较大的$d’$个特征值，组成新的矩阵$\Lambda_1$，同时$V$也截断部分，变为$V_1$，$V_1$为$m\times d’$矩阵<br>$$<br>B=V_1\Lambda_1V^T_1<br>$$<br>这样操作后，内积矩阵$B$基本不会有太大变化，但是样本的维度由$d$降维$d’$</p><p>样本在低维空间中组成的矩阵为$Z$,<br>$$<br>Z =\Lambda_1^{1/2}V_1^T\in R^{d’\times m}<br>$$</p><p>$$<br>B=Z^TZ\in R^{m\times m}<br>$$</p><p>关键是要记得，<strong>低维嵌入MDS的目标是保持距离不变，转换为保持内积不变，其做法是对样本的内积矩阵进行特征值分解</strong></p><h1 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h1><p>关于PCA，主要就是想说，注意其和低维嵌入的区别</p><ul><li>它是对样本的协方差矩阵进行特征值分解，而不是内积矩阵</li><li>目标是找到超平面，使得满足<ul><li>最近重构性：样本点到这个超平面的距离都足够近</li><li>最大可分性：样本点在这个超平面上的投影能尽可能分开</li></ul></li></ul><h2 id="内积矩阵和协方差矩阵"><a href="#内积矩阵和协方差矩阵" class="headerlink" title="内积矩阵和协方差矩阵"></a>内积矩阵和协方差矩阵</h2><p>内积矩阵是描述两个向量相似度的东西，若样本个数为m个，样本维度为n维，则内积矩阵会是$m\times m$维</p><p>而协方差矩阵是从特征的角度研究向量，和上面正好是相反的，若样本个数为m个，样本维度为n维，则内积矩阵会是$n\times n$​维</p><p>内积矩阵的计算是<br>$$<br>P=XX^T<br>$$<br>协方差矩阵的计算是<br>$$<br>C=X^TX<br>$$</p><h1 id="流形学习"><a href="#流形学习" class="headerlink" title="流形学习"></a>流形学习</h1><p>关键：<strong>测地线距离（近似），保距</strong></p><p>流形学习用一种类似于树的最短路径搜索的方法来近似得到点与点之间的测地线距离</p><h1 id="度量学习"><a href="#度量学习" class="headerlink" title="度量学习"></a>度量学习</h1><p>降维的主要目的是希望找到一个“合适的”低维空间，而机器要学习的就是学出合适的距离的度量</p><p>而一般来说，就是要学习马氏距离的矩阵$M$</p><p>还要知道对M的学习的目标是什么，优化的是什么目标函数</p><ul><li><p>某种分类器的性能</p><ul><li>如以近邻分类器的性能为目标，得到NCA</li></ul></li><li><p>该距离希望结合领域知识</p><p><img src="https://i.loli.net/2021/09/16/KhTwcW467sZLDMk.png"></p></li></ul><p>这里有个有意思的地方，就是我们是最小化同类之间的距离，同时令异类之间的距离大于1，但是没有用最大化异类之间的距离，同时令同类之间的距离小于某个值，因为最大化是趋向无穷的，如果优化时目标为最大化，很容易导致M的结果有很多问题，而最小化只能到0，一般来说优化都是最小化而没有最大化的</p><p><img src="https://i.loli.net/2021/09/16/ZHKPqEAzyXrxv59.png"></p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>词法分析</title>
    <link href="/2021/09/07/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <url>/2021/09/07/%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="词法分析相关概念"><a href="#词法分析相关概念" class="headerlink" title="词法分析相关概念"></a>词法分析相关概念</h1><p><strong>注意区分词法单元的单元名和词素</strong>，词法单元是抽象层次更高的，词素则是具体的实例</p><p>词法单元包含两个部分，单元名和属性，其中属性值是一个结构化的数据，其中包含词素，类型，第一次出现的位置，….编译时能够识别undefined variable，就是靠属性值中记录的第一次出现的位置来判定的</p><p><img src="https://i.loli.net/2021/09/07/BLNioCHqZO5Qf82.png"></p><p>如上图，一般词素就是具体的定义，比如说人定义的变量a,b,c等等，都是词素，而它们都可以抽象成词法单元中的id</p><hr><p>识别词法单元时，源程序会有一个固定的Pattern，这个Pattern需要有精确的数学描述定义，而且计算机能够运行识别，我们使用正则表达式来作为该模式的数学描述</p><p>在正则表达式之前，需要了解一些相关概念</p><h2 id="字母表"><a href="#字母表" class="headerlink" title="字母表"></a>字母表</h2><p>一个<strong>有限</strong>的符号集合</p><p>比如英语的字母表就是${a,b,..,z,A,B,..,Z}$</p><h2 id="串"><a href="#串" class="headerlink" title="串"></a>串</h2><p>字母表中符号组成的一个有穷序列</p><p>比如英语字母表的串，就是一个英语单词</p><h2 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h2><p>给定字母表上一个任意的可数的串的集合</p><ul><li>语法正确的C程序的集合，英语，汉语，日语</li></ul><p>如英语就是由所有英语单词（串）组成的集合，这里的语言不包含语言的语法，仅仅是一个单词的集合的意思</p><h2 id="语言的运算"><a href="#语言的运算" class="headerlink" title="语言的运算"></a>语言的运算</h2><p><img src="https://i.loli.net/2021/09/07/PuWX9v1VIRThyoZ.png"></p><img src="https://i.loli.net/2021/09/07/FMonZEIW1Ubt6ya.png" style="zoom: 67%;" /><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p>正则表达式是一种形式的描述语言，它可以抽象地表示语言的模式类型，表示语言中的具有某种固定形式的子集。</p><p>可以先看一个正则表达式的例子，在C语言中的定义的标识符规则，可以用正则表达式表示<br>$$<br>letter_(letter_|digit)^*<br>$$<br>其中的**letter_**表示任一字母或者下划线，竖线相当于语言的并运算，表示可以是字母或者下划线或者数字，星号的定义和语言的闭包的定义一样，表示可以由任意长度的括号内的表达式连接起来。而letter_直接与后面的括号连接相当于语言的连接运算。该定义是与自然语言的表述规则是一样的：</p><ul><li>标识符的开头需要是字母或者下划线，不能是数字等其他东西</li><li>标识符中可以包含数字，字母，下划线</li><li>标识符的长度可以是大于1的任意长度</li></ul><p>从该例子可以体会到</p><ul><li>正则表达式自身是一种语言，它也有自己定义的运算</li><li>正则表达式的运算可以映射到其描述的语言的运算</li><li>正则表达式是一种简洁的，精确的描述<strong>语言</strong>的固有模式（或说规则）的<strong>语言</strong></li></ul><h2 id="正则表达式的形式定义及到语言上的映射"><a href="#正则表达式的形式定义及到语言上的映射" class="headerlink" title="正则表达式的形式定义及到语言上的映射"></a>正则表达式的形式定义及到语言上的映射</h2><ul><li><p>字母表$\Sigma$上的正则表达式的定义</p></li><li><p>基本部分</p><ul><li>$\varepsilon$ 是一个正则表达式，$L(\varepsilon)= {\varepsilon}$</li><li>如果a是$\Sigma$上的一个符号，那么a是正则表达式，$L(a)={a}$</li></ul></li><li><p>归纳步骤</p><ul><li>选择：$(r)|(s)$​，$L((r) | (s))=L(r) \cup L(s)$​；</li><li>连接：$(r)(s)，L((r)(s))=L(r)L(s) $；</li><li>闭包：$(r)<em>，L((r)^</em>)=(L(r))^*$​​​​；</li><li>括号：$(r)，L((r))=L(r)$</li></ul></li><li><p>运算的优先级：$* &gt;$ 连接符$&gt;$ |</p></li></ul><p>该节内容类似于<strong>数理逻辑</strong>中的内容，实际上正则表达式是一个抽象的东西，其自身由正则表达式（元素）及正则表达式的运算（运算）组成，但是可以定义映射$L$​​来把正则表达式的元素映射到语言上，也可以把正则表达式的运算映射到语言的运算（<strong>语言的运算实际上是集合的运算</strong>）中</p><h2 id="正则表达式的例子"><a href="#正则表达式的例子" class="headerlink" title="正则表达式的例子"></a>正则表达式的例子</h2><p><img src="https://i.loli.net/2021/09/14/etjpvGn4Xka5Vr8.png"></p><h2 id="正则表达式的性质"><a href="#正则表达式的性质" class="headerlink" title="正则表达式的性质"></a>正则表达式的性质</h2><ul><li><p>等价性：如果两个正则表达式$r$和$s$表示同样的语言,也就是$L(r)=L(s)$，则$r=s$</p></li><li><p>代数定律</p><p><img src="https://i.loli.net/2021/09/14/8PSHAJiIgMjWmEL.png"></p></li></ul><h2 id="正则定义"><a href="#正则定义" class="headerlink" title="正则定义"></a>正则定义</h2><p>通过正则定义，可以使得正则表达式更加贴近人类可读的语言，且更为简洁</p><p>例如：C语言的标识符集合</p><p>$letter_\rightarrow A|B|…|Z|a|b…|z|_$</p><p>$digit\to 0|1|…|9$</p><p>$id\to letter_(letter_|digit)^*$</p><p>通过定义letter和digit，使其更接近人类语言，且用这两个正则定义可以继续定义$id$​，这使得id的定义十分简洁。否则可以把id的定义拆开，会写的十分的冗长且不d可读 </p><h1 id="状态转换图"><a href="#状态转换图" class="headerlink" title="状态转换图"></a>状态转换图</h1><p>上面的正则表达式始终还是形式的抽象定义，而词法分析器需要的是计算机能处理的表达，因此我们需要一个更为具象的实现正则表达式的方式，那就是状态转换图。</p><p><strong>根据正则表达式，可以定义出状态转换图，而状态转换图可以由计算机处理，且返回识别出来的词法单元</strong></p><h2 id="词法单元模式"><a href="#词法单元模式" class="headerlink" title="词法单元模式"></a>词法单元模式</h2><p>下面的讨论都会围绕这里列出的词法单元进行</p><p><img src="https://i.loli.net/2021/09/14/VdLlinS3fxZ8qy2.png"></p><p>ws表示空白符</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>状态转换图就是一个图，有节点和边组成</p><ul><li>状态（节点）：表示在识别词素的过程中可能出现的情况<ul><li>状态看作是已处理部分的总结</li><li>一般的状态的节点就是一个圈</li><li>某些状态作为接受状态，表示已经找到词素，在图中用两个圈表示</li><li>加上*的接收状态表示最后读入的符号不在词素中</li><li>开始状态用start边表示</li></ul></li><li>边：从一个状态到另一个状态<ul><li>如果当前状态为s，下一个输入符号为a，就沿着从s离开，标号为a的边到达下一状态</li></ul></li></ul><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><img src="https://i.loli.net/2021/09/14/oSaJB5dPe2Zz76U.png" style="zoom: 80%;" /><p><img src="https://i.loli.net/2021/09/14/NPgjATwifarKLZ1.png"></p><p>图3-16可以表示无符号数字的识别，若状态到达19，20，21这三个的任意一个，则表示该数字识别成功，返回数字。该图可以解释$10.$不可以作为最终状态，也就是若程序中写数字时，写成$10.$的话，编译时就会报错</p><p>初始状态为12，然后读取1，进入13状态，再读取0，进入13状态，读取.，进入14状态，最后没有可读取的了，于是最终会停留在14状态，但是14状态不是接受状态，因此10.并不是一个正确的数字的写法</p><h2 id="构造词法分析器"><a href="#构造词法分析器" class="headerlink" title="构造词法分析器"></a>构造词法分析器</h2><p>用程序实现词法分析器有很多方法，当然可以在每个状态都用if实现，但是若语言支持switch的话，可以比较简洁地写出来</p><ul><li>用state记录当前状态</li><li>一个switch根据state地值转到相应的代码</li><li>进入到某个接受状态时，返回相应的词法单元</li></ul><img src="https://i.loli.net/2021/09/14/wi9sZnJKVDyxXWg.png" style="zoom:67%;" /><p>该段代码对应图3-13的状态转换图</p><h2 id="多个模式集成到词法分析器"><a href="#多个模式集成到词法分析器" class="headerlink" title="多个模式集成到词法分析器"></a>多个模式集成到词法分析器</h2><p>在解析源程序的时候，肯定是要有多种正则表达式匹配的，比如说，digits正则表达式匹配数字，id正则表达式匹配标识符，但是一个状态转换图只能对应一个正则表达式，因此我们识别词法单元时，需要用多个状态转换图来识别</p><ul><li>可以顺序匹配多个模式，若引发fail，则回退并启动下一个状态转换图</li><li>可以并行的运行各个状态转换图</li><li>所有的状态转换图合并为一个图</li></ul><h1 id="有穷自动机"><a href="#有穷自动机" class="headerlink" title="有穷自动机"></a>有穷自动机</h1><ul><li><p>有穷自动机在本质上等价于状态转换图</p></li><li><p><strong>区别在于</strong>，自动机是识别器，对每个输入串回答yes or no</p></li><li><p>分为两类</p><ul><li>不确定的有穷自动机（NFA）</li><li>确定的有穷自动机（DFA）</li></ul></li></ul><h2 id="不确定与确定"><a href="#不确定与确定" class="headerlink" title="不确定与确定"></a>不确定与确定</h2><p>不确定的意思就是对于一个状态，读取下一个符号后，在不同的情况下可能跳转到不同的状态。以debug作类比的话，就是同一个程序，同样编译两次，但是两次的出错的地方都不同的感觉，做PA时就深感痛苦，同一个程序，有时出错，有时没事，有时出错的地方和上次不同….</p><p>一共有两点不同</p><ul><li><strong>不确定的自动机的边可以是空串$\varepsilon$</strong></li><li><strong>不确定自动机在一个状态下的多条边可以是同一个标记</strong></li></ul><p>第二点很好理解为什么是不确定的</p><img src="https://i.loli.net/2021/09/14/bgrVflPF8WCIHBQ.png" style="zoom: 25%;" /><p>对于状态S1，读取到同一个符号a，但是有可能跳转到不同的状态，这就是不确定</p><p>对于第一点，也很好理解，下图中，S1状态读取了a后，同样也是可以跳转到S3或者S4，这是因为边的符号可以是空串$\varepsilon$导致的</p><img src="https://i.loli.net/2021/09/14/GhHa57lrfJkA6SL.png" style="zoom:33%;" /><h2 id="不确定的有穷自动机"><a href="#不确定的有穷自动机" class="headerlink" title="不确定的有穷自动机"></a>不确定的有穷自动机</h2><p><img src="https://i.loli.net/2021/09/14/JqLdTOmXfjrZwos.png"></p><p><img src="https://i.loli.net/2021/09/14/LD9hX61QmVksedf.png"></p><h2 id="自动机对输入字符串的接受"><a href="#自动机对输入字符串的接受" class="headerlink" title="自动机对输入字符串的接受"></a>自动机对输入字符串的接受</h2><p>这里有一点需要注意的，对于不确定的自动机，只要存在一条从开始状态到接受状态的路径，就认为符号串可以被NFA接受。如字符串aabb，然后转换图如下</p><p><img src="https://i.loli.net/2021/09/14/JtswoRlOIfzrDiB.png"></p><p>aabb字符串显然可以到达接受状态3，但是也可能一直在0状态那里。我们不能认为有一条路没走到接受状态就认为aabb是不能被接受的串</p><h2 id="自动机与语言"><a href="#自动机与语言" class="headerlink" title="自动机与语言"></a>自动机与语言</h2><p>由一个NFA <em>A</em>定义（接受）的语言是从开始状态到某个接受状态的所有路径上的符号串集合，称为*L(A)*。</p><p><img src="https://i.loli.net/2021/09/14/AVdy1rkQgaPOFhT.png"></p><h2 id="确定有穷自动机"><a href="#确定有穷自动机" class="headerlink" title="确定有穷自动机"></a>确定有穷自动机</h2><p>正则表达式容易被人工翻译成不确定有穷自动机，但是确定有穷自动机才容易被计算机实现，因为这是一个step by step的过程，实现起来高效简洁</p><p><strong>每个NFA都有一个等价的DFA，即它们接受同样的语言</strong></p><p><img src="https://i.loli.net/2021/09/14/L8uirxAokmeI7Jn.png"></p><p>上图中，如果是NFA，move就是返回一个状态的集合而不是一个状态，因此就可能需要计算机并行地计算下一步，实现起来会很麻烦，但是如果是DFA，move仅仅返回一个状态给s，运行简单，所以一般词法分析器的模式识别最终是用DFA的</p><h1 id="正则表达式到自动机"><a href="#正则表达式到自动机" class="headerlink" title="正则表达式到自动机"></a>正则表达式到自动机</h1><p><strong>正则表达式可以简洁、精确地描述词法单元的模式但是在进行模式匹配时需要模拟DFA的执行。</strong></p><p>将正则表达式转换为DFA需要两步：</p><ul><li>正则表达式到NFA</li><li>NFA到DFA</li></ul><h2 id="NFA到DFA-子集构造法"><a href="#NFA到DFA-子集构造法" class="headerlink" title="NFA到DFA-子集构造法"></a>NFA到DFA-子集构造法</h2><p><strong>基本思想：</strong></p><ul><li>NFA的状态子集合并为DFA的一个状态</li><li>用$\varepsilon$-closure方法把NFA中的空串边消除</li></ul><p>理论上，最坏情况下DFA的状态个数会是NFA状态个数的指数多个。但是对于大部分应用，NFA和相应的DFA的状态数量大致相同。</p><p>操作：</p><p><img src="https://i.loli.net/2021/09/14/7qGFVWP1YaAkI2J.png"></p><p><img src="C:\Users\11056\AppData\Roaming\Typora\typora-user-images\image-20210914211606289.png" alt="image-20210914211606289"></p><p>对于$\varepsilon$​-closure,一般采用图搜索的方法，就是一种深度优先搜索</p><p><img src="https://i.loli.net/2021/09/14/bvDKr1z25UPF7Mh.png"></p><p><img src="https://i.loli.net/2021/09/14/xz7Iivrhjs3OY9e.png"></p><p>可以看到，有很多个NFA的状态合成了一个DFA状态，而且一个NFA状态可以出现在不同的DFA状态里，像1状态同时在B状态和C，D状态里</p><hr><p>未完待续</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Compile</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Direct Link Networks</title>
    <link href="/2021/09/07/Direct%20Link%20Networks/"/>
    <url>/2021/09/07/Direct%20Link%20Networks/</url>
    
    <content type="html"><![CDATA[<h2 id="链路层服务和帧"><a href="#链路层服务和帧" class="headerlink" title="链路层服务和帧"></a>链路层服务和帧</h2><ul><li><p>帧(Framing)：在每个网络层数据报经过链路传送前，几乎所有的链路层协议都要用链路层帧封装起来。一个帧由一个数据字段和若干首部字段组成，网络层的数据报就插在数据字段中。帧的结构由链路层协议决定</p><p>为了实现链路层的功能，不同的链路层协议会用不同的方法，由此会有不同的帧结构，下面还会继续说明，但是在这里简要看一下，HDLC协议为了链路层的差错检测功能采用循环冗余纠错码，而PPP则采用了Check Sum的方法，因此两个协议的帧结构是不同的</p></li></ul><img src="https://i.loli.net/2021/09/06/wzPeg7VHbB9XKOa.png" style="zoom: 67%;" /><p><img src="https://i.loli.net/2021/09/06/xNrRIto5CAlF3dh.png"></p><hr><ul><li><strong>链路接入</strong>，媒体控制访问（<strong>MAC</strong>）协议规定了帧在链路上传输的规则</li></ul><p>因为链路上面可能会有其他帧在传输而引起冲突等问题，所以需要规定帧的传输规则</p><p>对于链路的一端仅有一个发送方，另一端只有一个接收方的点对点链路，则MAC协议很简单，只要规定谁接受谁发送帧就可以，无论何时链路空闲就可以发送帧</p><p>但是大多数时候需要的都是多个节点共享单个广播链路的情况，MAC用于协调多个节点的帧运输</p><hr><ul><li><strong>可靠交付</strong></li></ul><p>当链路层协议提供可靠交付服务时，它保证无差错地经链路层移动每个网络层数据报</p><p>该服务主要通过<strong>确认和重传</strong>实现</p><hr><ul><li><strong>差错检测和纠正</strong></li></ul><p>链路层的差错检测和纠正与数字电路设计里面的类似，基本都是靠硬件实现的，但实际上计算机网络应用的差错检测和纠错不多，主要都是在硬件设计里面用的</p><hr><ul><li><strong>流量控制</strong></li></ul><p>流量控制表示在接收方接受到并处理帧之前，发送方被限制只能发送多少帧</p><p>流量控制用于让发送方能以较高的速率发送帧，并保证接收方能及时接受和处理接收到的帧</p><p>更具体地，因为接收方接受是通过读取接收方的缓冲区的数据来接收到信息的，但是其缓冲区的大小并不是无限大的，所以需要限制数据报进入缓冲区的速率，避免缓冲区溢出</p><h2 id="链路层的实现"><a href="#链路层的实现" class="headerlink" title="链路层的实现"></a>链路层的实现</h2><ul><li>链路层的主体就是网络适配器，通俗点就是网卡，该芯片实现了很多的链路层服务，包括成帧，链路接入，还有差错检测等</li><li>链路层的大部分是在硬件中实现，但是部分的链路层是在运行在主机CPU上，的软件中实现的，链路层的软件组件实现了高层的链路层功能，如组装链路层寻址信息和激活控制器硬件，而在接收方，链路层软件响应控制器终端，处理差错条件并将数据报向上传递给网络层，所以<strong>链路层是软件和硬件的结合体</strong></li></ul><p><img src="https://i.loli.net/2021/09/06/X1tcrnm5uGSM6Wf.png"></p><h2 id="差错检测和可靠传输"><a href="#差错检测和可靠传输" class="headerlink" title="差错检测和可靠传输"></a>差错检测和可靠传输</h2><p><img src="https://i.loli.net/2021/09/06/vVlXq3KRpWtYyIu.png"></p><ul><li>D是数据报本身</li><li>EDC是为了进行差错检测，链路层中所添加的纠错码</li><li>越长的EDC域会有更好的检测和纠错效果</li></ul><p>主要的差错检测和纠正有三种方法</p><ul><li>奇偶校验</li><li>循环冗余检测</li><li>检验和</li></ul><h2 id="控制流速"><a href="#控制流速" class="headerlink" title="控制流速"></a>控制流速</h2><p>方法：</p><ul><li>停止等待</li><li>滑动窗口</li></ul><h3 id="停止等待"><a href="#停止等待" class="headerlink" title="停止等待"></a>停止等待</h3><ul><li>源节点发送一个帧</li><li>目的节点接受帧，并返回ACK信号</li><li>源节点等待返回的ACK信号</li><li>若源节点收到返回的ACK信号，则继续发送新的帧</li><li>目的节点可以通过不发送ACK信号来停止流</li></ul><p>可以很容易看出来，这种方法效率很低，只适用于每个帧都比较大的情形</p><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><ul><li>这种方法允许源节点每次发送多个帧</li><li>接收方的缓冲区大小设为Win</li><li>发送方可以最多一次发送Win个帧，不需要接受到ACK信号</li><li>每个帧都有自己的编号</li><li>ACK信号包括下一个接收方希望要的帧</li></ul><p><img src="https://i.loli.net/2021/09/07/tRrO6PjTYGZEqgw.png"></p><p>如上图，窗口大小为7个帧，A第一次发送3个帧，然后B接受到3个帧后窗口向右移动到3号开始的帧，然后发送ACK信号给A，RR3表示下一个帧希望是3号帧，于是A再从3号帧开始发送4个帧给B，但是这次B只接受到了3号帧，因此ACK要求下一个帧是4号帧，于是A重新发送4，5，6帧给B，B接收到了就变成右下角的形式</p><h4 id="传输出错时的方案"><a href="#传输出错时的方案" class="headerlink" title="传输出错时的方案"></a>传输出错时的方案</h4><ul><li>Go Back N</li><li>选择拒绝</li></ul><p>实际上，一般都会用Go Back N的方案，因为选择拒绝的方案所需要的成本高，技术比较复杂</p><p>实现上，假设3号帧出错，但是发送方一开始发送了4个帧，2，3，4，5号帧，若采用Go Back N的方案，则需要从3号帧全部重传，也就是需要发送方重传3，4，5号帧；若采用选择拒绝的方案，则需要重新单独发送3号帧。</p><p>但是，因为一般来说，在帧传输的过程中，会因为外部未知因素干扰而导致帧出错，而一般干扰会持续一段时间，也就是说3号帧出错，由于持续的干扰的影响，后面的帧也会出错，因此我们一般会在出错帧后面也选择重传</p><h2 id="以太网"><a href="#以太网" class="headerlink" title="以太网"></a>以太网</h2><h3 id="多路访问链路"><a href="#多路访问链路" class="headerlink" title="多路访问链路"></a>多路访问链路</h3><p>在这节开始之前，上面的都是讨论单个发送方和单个接收方在一个链路上面的问题，而从现在就是要考虑协调多个发送和接收节点对一个共享广播信道的访问</p><p>多个发送和接受节点都连接到相同的，单一的，共享的广播信道上，实际生活中有很多，如WIFI，卫星信号传播，电缆接入网</p><p><img src="https://i.loli.net/2021/09/07/89Zzewo7fQl2Anx.png"></p><p>在这种链路上面，我们主要需要解决的问题就是<strong>冲突</strong>问题</p><h3 id="链路组织的拓扑结构"><a href="#链路组织的拓扑结构" class="headerlink" title="链路组织的拓扑结构"></a>链路组织的拓扑结构</h3><p><img src="https://i.loli.net/2021/09/07/Kh4ZkwAQNfbxYce.png"></p><p>在上面的Bus结构中，显然会出现数据冲突的问题，Tree结构则稍微减少了冲突的现象但是显然还是需要协议来解决冲突问题，Ring结构可以规定数据流的流动方向从而减少冲突的现象，最后Star结构比较复杂，但是基本解决了冲突问题</p><h3 id="信道划分协议"><a href="#信道划分协议" class="headerlink" title="信道划分协议"></a>信道划分协议</h3><p>假设一个支持N个节点的信道且信道的传输速率为R bps</p><h4 id="时分多路复用"><a href="#时分多路复用" class="headerlink" title="时分多路复用"></a>时分多路复用</h4><p>该方法的想法很简单，实际上跟操作系统里面的分时提供服务差不多，就是把时间划分为时间帧(time frame)，并进一步把每个时间帧划分为N个时隙(slot)，帧的概念就相当于一个周期，在每个帧内，规定一个可以发送数据分组的发送节点，每个节点在一个帧内都有发送数据分组的机会。</p><p>但是这个方法的缺点就是，每个节点被限制于R/N bps 的平均速率，即使当它是唯一有分组要发送的节点</p><p><img src="https://i.loli.net/2021/09/07/7jTqVuKhYesMiJQ.png"></p><h4 id="频分多路复用"><a href="#频分多路复用" class="headerlink" title="频分多路复用"></a>频分多路复用</h4><p>FDM将R bps信道划分为不同的频段（每个频段具有R/N的带宽），并把每个频率分配给N个节点中的一个，因此FDM在单个较大的R bps信道中创建了N个较小的R/N bps信道，公平的划分了带宽，但是和时分多路复用有同样的缺点，就是每个节点只能使用R/N的带宽</p><p><img src="https://i.loli.net/2021/09/07/4r6uWX2gjM1hmAH.png"></p><h4 id="码分多址（CMDA）"><a href="#码分多址（CMDA）" class="headerlink" title="码分多址（CMDA）"></a>码分多址（CMDA）</h4><p>在课本里面，用鸡尾酒会作类比，一个CMDA协议类似于让聚会客人使用多种语言来谈论，人们善于锁定他们能听懂的语言的谈话，而过滤其余的对话</p><p>定义时隙等于要发送1个比特所需要的时间，比特为0或1，但是这里把0比特视为-1</p><p>每一个时隙都设计一个对应的CDMA编码的比特，设第i个比特时隙中的数据比特值为$d_i$，对于$d_i$比特传输时间的第$m$个微时隙，CDMA编码器的输出$Z_{i,m}$是$d_i$乘以分配的CDMA编码的第$m$比特$c_m$:<br>$$<br>Z_{i,m}=d_i \cdot c_m<br>$$<br><img src="https://i.loli.net/2021/09/07/CNgVHsSjEI3bv6m.png"></p><p>CMDA的工作中有一个假设，就是对干扰的传输比特信号是加性的，也就是在第i个比特时隙的第m个微时隙期间，接收方所受到的是在那个微时隙中所有N个发送方传输的比特的总和：<br>$$<br>Z_{i,m}^* = \sum\limits_{s=1}^NZ_{i,m}^s<br>$$<br>如果仔细的选择CMDA的编码，每个接收方通过下面式子使用发送方的编码，就能从聚合的信号中恢复一个给定的发送方发送的数据<br>$$<br>d_i=\frac{1}{M}\sum\limits_{m=1}^M Z_{i,m}^*\cdot c_m<br>$$</p><h3 id="随机接入协议"><a href="#随机接入协议" class="headerlink" title="随机接入协议"></a>随机接入协议</h3><h4 id="ALOHA"><a href="#ALOHA" class="headerlink" title="ALOHA"></a>ALOHA</h4><ul><li><p>发送方</p><ul><li>只要有要发送的帧，就立即发送</li><li>发送方在一段很短的时间内侦听回复</li><li>如果收到ACK，则没问题，如果没收到ACK，将以概率p立即重传该帧，否则该节点等待一个帧传输时间</li><li>如果重传了几次之后都没有ACK，则放弃</li></ul></li><li><p>接收方</p><ul><li>进行差错检测</li><li>如果帧没问题，则发送ACK给发送方</li></ul></li></ul><p>帧可能会被冲突破坏，该方法的线路最大利用率是$18%$，超过$18%$就会可能出现死锁的情况，因为一个冲突可能会导致后面的多个冲突，就如生态系统中的正反馈一样，冲突的问题有可能越来越严重最后导致整条线路都无法传输帧</p><img src="https://i.loli.net/2021/09/07/queX72zZBvAlSft.png" style="zoom: 50%;" /><p>如上图2站点发出的2帧的冲突会导致后面的冲突，可想而知，一个冲突可能不是一个冲突的问题，而是会引发连锁的冲突</p><h4 id="Slot-ALOHA"><a href="#Slot-ALOHA" class="headerlink" title="Slot ALOHA"></a>Slot ALOHA</h4><p>跟上面的简单的ALOHA不同的就是，每个站点需要发送帧的时候，需要等到每个时隙的开始才能发送</p><p><img src="https://i.loli.net/2021/09/07/GFnod5D2SvtCiH6.png"></p><p>在这种方案下，线路的利用率是$37%$,比简单的ALOHA高了一倍</p><h4 id="Nonpersistent-CSMA"><a href="#Nonpersistent-CSMA" class="headerlink" title="Nonpersistent CSMA"></a>Nonpersistent CSMA</h4><p>发送站点会保持监听线路是否空闲</p><ul><li>如果处于空闲状态，则传输帧，否则到下面的步骤</li><li>如果处于繁忙状态，则等待一个随机的时间，然后到上面的步骤</li></ul><p>这种方法是一种谦让的方案，但是因为谦让可能会导致即使线路处于空闲状态，但是也有几个站点在等待发送</p><h4 id="1-persistent-CSMA"><a href="#1-persistent-CSMA" class="headerlink" title="1-persistent CSMA"></a>1-persistent CSMA</h4><p>发送站点会保持监听线路是否空闲</p><ul><li>如果处于空闲状态，则传输，否则到下面的步骤</li><li>如果线路繁忙，则持续监听直到线路空闲，然后立即传输帧</li></ul><p>这种方案是一种自私的方案，如果同时有两个或以上的站点在等待发送，则会导致冲突</p><h4 id="p-persistent-CSMA"><a href="#p-persistent-CSMA" class="headerlink" title="p-persistent CSMA"></a>p-persistent CSMA</h4><p>该方案尝试折中上述的两种方案</p><ul><li>像NonPersistent那样避免冲突</li><li>同时像1-persistent那样减少线路空闲时间</li></ul><p>规则：</p><ul><li>(1) 如果线路空闲，则以$p$的概率传输帧，以$(1-p)$的概率推迟一个时间单元</li><li>一般时间单元会设置为最大传输延迟</li><li>(2) 如果线路繁忙，则监听直至线路空闲，然后跳到(1)步骤</li><li>当传输推迟了一个时间单元，则跳到步骤(1)</li></ul><p>有理论表明，$p$的最优值是$1/N$ （N是在等待发送帧的站点）</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编译原理入门</title>
    <link href="/2021/08/31/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8/"/>
    <url>/2021/08/31/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="编译器和解释器"><a href="#编译器和解释器" class="headerlink" title="编译器和解释器"></a>编译器和解释器</h1><h2 id="编译器"><a href="#编译器" class="headerlink" title="编译器"></a>编译器</h2><ul><li>效率高，一次编译，多次运行</li><li>通常目标程序是可执行的</li></ul><p>一个源代码文件，比如C文件，或者py文件，其内容并不能被机器识别运行，因此需要先编译为汇编语言，再转化为机器语言，在windows上的目标程序（可执行程序）就是exe文件，编译器只会编译一遍，生成一个可执行文件，以后用户只需要执行可执行文件，不需要再编译就可以运行，与源程序再也没有关系</p><p><img src="https://i.loli.net/2021/08/31/UQGviYcNXPy3Sel.png"></p><h2 id="解释器"><a href="#解释器" class="headerlink" title="解释器"></a>解释器</h2><ul><li>直接利用用户提供的输入，执行源程序中指定的操作。</li><li>不生成目标程序，而是根据源程序的语义直接运行。</li><li>边解释，边执行，错误诊断效果好。</li></ul><p><img src="https://i.loli.net/2021/08/31/7n9x4MAhIXFGkaQ.png"></p><p>其实仍然需要生成可执行的代码才能运行，但是是逐条转换，而且不会保留可执行的代码，因此下一次再有程序输入时仍然需要解释器再重新编译源程序</p><h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><p>如果举生活中的例子，编译器就相当于翻译一本英文书为中文书，只需要有一个人翻译出来，有了中文译本，用户就可以一直看中文译本而不需要再去看英文本了。解释器就相当于有一个人逐条翻译英文本给用户，用户可以一页页听着翻译来看完整本书，但是看完整本书后，不会留下译本，下次需要再看这本书的时候，还需要译者去逐条逐条地翻译才能看懂</p><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><p>Java的语言处理器结合了编译和解释的过程，一个Java源程序首先被编译成一个字节码(bytecode)的中间表示形式，然后由一个虚拟机对得到的字节码加以解释执行。这样会比每次都重新编译源程序要快，又能有解释的易进行错误诊断的效果</p><p>好处是再一台机器上编译得到的字节码可以再另一台机器上解释执行</p><p><img src="https://i.loli.net/2021/08/31/vQWGT6JYimoR4XA.png"></p><h1 id="从源程序到机器代码"><a href="#从源程序到机器代码" class="headerlink" title="从源程序到机器代码"></a>从源程序到机器代码</h1><p>这一部分是在计算机系统基础中的知识，在这里再复习下</p><ul><li>源程序$\to$ 预处理器$\to$​ 经过预处理的源程序（hello.c $\to$​ hello.i）</li></ul><p>预处理，比如宏定义的转换</p><ul><li>经过预处理的源程序$\to$ 编译器$\to$ 目标汇编程序 （helloc.i$\to$ hello.s）</li></ul><p>把高级语言的程序翻译为汇编语言程序</p><ul><li>目标汇编程序$\to$汇编器$\to$ 可重定位机器代码（hello.s $\to$ hello.o）</li></ul><p>生成可重定位的机器代码，也就是还没有经过链接的过程，变量的地址还不知道</p><ul><li>可重定位机器代码$\to$ 链接器/加载器$\to$ 目标机器代码（hello.o$\to$ hello.exe）</li></ul><p>链接库文件和可重定位对象文件，生成机器可以运行的机器代码，不同的操作系统的可执行文件不同</p><h1 id="编译器的结构"><a href="#编译器的结构" class="headerlink" title="编译器的结构"></a>编译器的结构</h1><h2 id="分析部分"><a href="#分析部分" class="headerlink" title="分析部分"></a>分析部分</h2><ul><li>把源程序分解为多个组成要素，并得到语法结构，用该语法结构创建源程序的一个中间表示</li><li>搜集源程序中的相关信息，放入符号表</li><li>分析，定位程序中可能存在的语法，语义错误</li><li>又称为编译器的前端，是与机器无关的部分</li></ul><h2 id="综合部分"><a href="#综合部分" class="headerlink" title="综合部分"></a>综合部分</h2><ul><li>根据符号表和中间表示构造目标程序</li><li>又称编译器的后端，是与机器相关的部分</li></ul><h1 id="编译器中的步骤"><a href="#编译器中的步骤" class="headerlink" title="编译器中的步骤"></a>编译器中的步骤</h1><p><img src="https://i.loli.net/2021/08/31/QbvJsqdoTrMU9uN.jpg"></p><p>除了上面标出来的，还有一个贯穿整个编译过程的东西，<strong>符号表</strong>，用于存放源程序的相关信息，可由各个步骤使用</p><p>下面的过程的例子将用一个简单的源程序语句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">position = initial + rate * <span class="hljs-number">60</span><br></code></pre></td></tr></table></figure><h2 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h2><p>编译器的第一个步骤是词法分析(lexical analysis)，它读入组成源程序的字符流，并将他们组织成有意义的词素的序列（词素是语言学中能表达意思的最小单位，在英语中是单词，在汉语中则是汉字）</p><p>对于每个词素，词法分析器产生如下的<strong>词法单元</strong>作为输出:</p><p>$&lt;token-name,attribute-value&gt;$​</p><p>其中token-name是一个抽象符号，相同类型的词素会使用相同的抽象符号</p><p>而attribute-value指向符号表中关于这个词法单元的条目，也就是相当于一个指针</p><p>该词法单元会传递给下一个步骤，也就是语法分析</p><ul><li>position，initial，rate，=，+，*，60 都是词素</li></ul><table><thead><tr><th>词素</th><th>词法单元</th></tr></thead><tbody><tr><td>position</td><td>&lt;id, 1&gt;</td></tr><tr><td>initial</td><td>&lt;id, 2&gt;</td></tr><tr><td>rate</td><td>&lt;id, 3&gt;</td></tr><tr><td>=</td><td>&lt;=&gt;</td></tr><tr><td>+</td><td>&lt;+&gt;</td></tr><tr><td>*</td><td>&lt;*&gt;</td></tr><tr><td>60</td><td>&lt;60&gt;</td></tr></tbody></table><p>字符流变为符号流<br>$$<br>&lt;id, 1&gt;&lt;=&gt;&lt;id,2&gt;&lt;+&gt;&lt;id,3&gt;&lt;*&gt;&lt;60&gt;<br>$$</p><h2 id="语法分析"><a href="#语法分析" class="headerlink" title="语法分析"></a>语法分析</h2><p><img src="https://i.loli.net/2021/08/31/D3zLGAaEYq4XQFu.png"></p><ul><li>生成语法树</li><li>指出了词法单元流的语法结构</li></ul><h2 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h2><p><img src="https://i.loli.net/2021/08/31/oNkTWFrRZ8pgeJq.png"></p><ul><li>得到语义，对于编译器来说比较难</li><li>语义分析<ul><li>使用语法树和符号表中的信息，检查源程序是否满足语言定义的语义约束（比如，在C语言中，当&lt;id,3&gt;与60相乘，会检查&lt;id, 3&gt;是否为定义的数字类型，否则报错）</li><li>同时收集类型信息，用于代码生成</li><li>类型检查，类型转换（如果int类型与float类型相乘，就需要先把int扩展为float类型）</li></ul></li></ul><h3 id="语义分析的例子"><a href="#语义分析的例子" class="headerlink" title="语义分析的例子"></a>语义分析的例子</h3><p>自然语言中代词的分析</p><ul><li>Jack said Jerry left his assignment at home.</li><li>Jack said Jack left his assignment at home?</li></ul><p>第一句中的his指代的his是Jerry，人很容易看出来，但是怎么能让机器看出来</p><p>第二句中两个Jack是不是同一个人，his又是指代的哪一个Jack</p><p>而在程序设计中，也会出现类似的情况</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C">&#123;<br>   <span class="hljs-keyword">int</span> Jack = <span class="hljs-number">3</span>;<br>   &#123;<br>      <span class="hljs-keyword">int</span> Jack = <span class="hljs-number">4</span>;<br>      <span class="hljs-built_in">cout</span> &lt;&lt; Jack;<br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>当中的Jack应该是哪一个值，机器需要怎么识别</p><h2 id="中间代码生成"><a href="#中间代码生成" class="headerlink" title="中间代码生成"></a>中间代码生成</h2><p><img src="https://i.loli.net/2021/08/31/Pnm3iFry4QZXJl5.png"></p><p>变成三地址代码：也就是每个指令最多包含3个运算分量</p><h2 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h2><p><img src="https://i.loli.net/2021/08/31/38eCmqhBGOVQUYL.png"></p><p>改进中间代码，使用更少行代码，运行的更快，占用更少内存</p><h2 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h2><p><img src="https://i.loli.net/2021/08/31/teQPw8fIJUOTNGd.png"></p><p>把中间表示形式映射到目标语言</p><ul><li>寄存器的分配</li><li>指令选择</li><li>内存分配</li></ul><h1 id="编译器的趟-Pass"><a href="#编译器的趟-Pass" class="headerlink" title="编译器的趟(Pass)"></a>编译器的趟(Pass)</h1><p>在一个特定的实现中，上面的多个步骤的活动可以被组合成一趟。每趟读入一个输入文件并产生一个输出文件</p><p>比如，前端步骤中的词法分析，语法分析，语义分析，以及中间代码生成可以被组合在一起成为一趟，然后可以有一个为特定目标机生成代码的后端趟</p><h1 id="人工智能编译器"><a href="#人工智能编译器" class="headerlink" title="人工智能编译器"></a>人工智能编译器</h1><p><img src="https://i.loli.net/2021/08/31/JZoOyiQK83mYatL.png"></p><p>现在有很多很火的人工智能深度学习框架，像是Pytorch和Keras，它们是不同的框架，但是最终都要运行在机器上，可以是CPU，也可以用GPU，TPU，如何让这些深度学习框架更快地运行在机器上面，有一部分就是编译器的工作</p><blockquote><p><strong>首先，由于前端接口和后端实现之间的差异，从一个人工智能框架切换到另一个AI框架是非常重要的</strong>。此外，算法开发人员可能会使用多个框架作为开发和交付流程的一部分。在AWS上，我们有客户希望在MXNet上部署他们的Caffe模型，以享受Amazon EC2上的高速性能。Joaquin Candela最近在博客中写道，用户可能会使用PyTorch进行快速开发，然后部署在Caffe2上。然而，在将模型从一个框架转换到另一个框架后，有人抱怨二者的调试结果存在差异。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Compile</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>表义文字的魅力</title>
    <link href="/2021/08/30/%E8%A1%A8%E4%B9%89%E6%96%87%E5%AD%97%E7%9A%84%E9%AD%85%E5%8A%9B/"/>
    <url>/2021/08/30/%E8%A1%A8%E4%B9%89%E6%96%87%E5%AD%97%E7%9A%84%E9%AD%85%E5%8A%9B/</url>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/08/30/ueUXYyxVpCNMrm7.png"></p><p>只是作为一个外行人，来谈谈自己对表义文字的看法</p><p>世界上的语言文字可以分为两种，表音文字与表义文字。而从它们的名字也可以立即知道两种文字的区别，前者的语言仅仅表示发音，世界上除了汉语外的文字全都是表音文字，如英语，德语，法语等等。而汉语，作为唯一的表义文字，现在仅仅在中国和日本，新加坡等地方使用，而只有中国以汉语作为国语全面使用的国家。我们在中国一直都在享受着汉语给我们带来的好处，但是我们自身却没有察觉，而且认为很多都是理所当然的。语言，是有表达效率的高低之分的。但是大多数人都仅仅限制在汉语和英语里面，没有接触过其他语言，所以很难知道汉语的表达力有多强</p><p>表义文字，顾名思义，就是每个字都有自己本身的意思。为了更好说明，可以与英语做对比，英语的文字是没有意义的。一个单个的字母不会有任何含义，而就算把一个单词拿出来，这个单词有它本身的意义，但是基本不会再用这个单词的本意来组合出新的单词而保留其原来的意思。比如说【色】这个字，在英文中，应该是【color】，而在表义文字中，我们而已用【色】这个字本身的意思来组合出新的单词【色素】而保留其原来的意思，而在英文中的色素则是【pigment】，一个和color完全没有关的单词。</p><p>从上面的例子就可以很容易看出来，汉语，只要学会了最基础的几千个字，就可以用这几千个字去组合出成千上万的新词，而这些新词尽管是第一次看到，也能通过文字的原意来大概知道意思，这就是作为表义文字的汉语的效率之高。这作用在我们的学习研究尤为明显。在初中开始，我们就需要学习很多的领域相关的词语了，像上面提到的色素。还有【叶绿体】，我们即便从来没有学到过这个知识，但却也能对其有一个大概的绿色的叶子的印象，而在英文中则是【chloroplast】，对于以英语为母语的人来说，刚接触到这个概念的时候应该是完全不知道这个是什么东西的吧。相比于英文，或许中文更适合于教学，只可惜中国的科学方面的水平不如外国，只能靠中国的科学家把外国的难以理解的词汇，都逐一找到对应的中文意思的汉字来翻译出来，让国人得以受益。</p><p>表义文字的效率还体现在可读性上面。在日语中，尽管他们有自己的文字，但是却仍然一直保留着汉字。原因有二，一是日语是不作间隔的，与英文不一样，日语的每个单词之间并没有空格隔开，所以如果没有汉字，日语就会相当于英语而且单词之间没有空格的情况，比如</p><p>IamJohnwhataboutyou</p><p>わたしはたなかですがおなまえは</p><p>可读性是很差的，因此日语的很多单词都会把汉字放在单词的开头，然后以他们的固有字结尾，比如「素晴らしい」「美しい」</p><p>これは|本当に|綺麗な|花|ですね</p><p>通过汉字，很容易就可以把句子断开，可读性大大提高。二则是汉字的表达能力强，只需要看到文章中的汉字，人们就大概能知道整篇文章说的是什么，这是表音文字不可能做到的。 </p><p>因为表义文字的存在，也让我们发展出来了自己的起名文化。在外国，因为只有表音文字，所以对于人名，一般都是用圣经里面的圣人的名字，像是什么John，James，实际上都是没有实际含义的，然后到Johnson，Robinson，实际上是用刚才提到的圣人的名字加上son，也就是儿子的意思，表示约翰的儿子…。人如其名这个成语也应该只能存在于汉语存在的地方吧。中国的名字，一般都能看出来其父母对儿女日后作为或性格的期许，像是【伟】【立名】【怡宁】之类，都是可以看出来的。而在一些已经弃用汉字的国家（韩国），都会在身份证件上面使用汉字来书写自己的名字，虽然流传的说法是韩国的同音异义词太多，如果用韩文的话会太多重名，所以会使用汉字来写，但或许他们也感受到了汉字的魅力才会继续沿用下去吧。</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Chinese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>N2之路</title>
    <link href="/2021/08/27/N2%E4%B9%8B%E8%B7%AF/"/>
    <url>/2021/08/27/N2%E4%B9%8B%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<p>在大二的暑假准备结束时，闲来无事，突然想写写自己的日语学习之路，不只是备考方面的经历，也想写一下中途的心路历程</p><p>现在想想学日语的初心，当时就是想看番能自己看懂不用字幕。大概这也是很多人的初心吧，隔着语言的墙壁始终觉得看番不够感觉，于是就在大一的寒假的时候开始了日语学习之路。想来那是多简单纯粹的初心，当时确实是单纯的热爱吧。对二次元世界的热爱让我有了学习日语的动力，这个动力也持续了很久，让我坚持学习了很久。</p><p>刚开始学习日语的时候，基本只认得汉字，看到片假名和平假名都觉得像看外星语一样，完全是看不懂的，感觉很隔阂，特别是片假名。我还记得第一个学的片假名单词是テレビ，现在看到这单词一看就知道意思和读音了，但是当时记这个单词的时候并没有那么容易，按着五十音的发音，逐个假名这样查，逐个记住，然后终于记住了第一个片假名单词。</p><p>慢慢地，寒假也学完了几篇课文，学了一些最基础的单词。那时候是反馈比较明显的时候，因为单词比较接近生活，也有很多明显日本特色的单词，像是什么本当に、けれど、せっかく、でも、しかし、あります、なんでもない之类的，很多都在番里面或者歌曲里面出现过，于是第一次“看到”，而不是“听到”，就会感觉很新鲜，觉得终于知道平时经常听到的单词或者短句是什么意思，会比较有成就感吧。那时候发现像是通常是连词，句尾语气词，动词之类的比较容易记住，很多都是在看番的时候看字幕就知道了这个发音对应的中文意思，因为通常这些词都比较独立，能表达独立的意思，翻译成中文也能很明显的分隔地看出来，但是像是名词通常是出现在句子的中间，看番即使一直有字幕，听过几遍，也很难直接把这个读音和意思联系起来，因为在这种情况，我们即使听到了这个发音，因为没有学过，大脑无法把这单词与单词附着的短语或者是其他语气词之类的分割开来，就会自动过滤掉，也就是说听到也反应不过来自己听到了，于是就相当于没听到过这个单词了，所以这种单词如果没“看到”过的话，是基本不可能学会的。而在那个时候，会学到这种单词，然后惊觉这个单词在哪个地方出现过。我印象比较深刻的就是「入り口　いりぐち」了，当我在课文中看到，并记住了这个单词的发音的时候，当我有一次不自觉唱起【相遇天使】，才发现这个单词在这首歌里面出现过，但是以前一直都是单听到什么就唱出来，完全不知道这个句子或者单词的意思</p><p><strong>明日の入り口に</strong></p><p><strong>置いてかなくちゃいけないのかな</strong></p><p>这种感觉很奇妙，会让学的时候更有动力，不断地学习，然后得到反馈，然后继续学习</p><p>逐渐的，很多歌的意思都能看懂，听懂了。有些以前只是觉得旋律好听的歌，细看一下歌词，更加惊叹于这首歌，这种感觉就像是高中的时候回看小学时候喜欢听的周杰伦和许嵩的歌，以前只是喜欢旋律，但是有了自己的经历，看过很多书，能明白很多艺术手法之后，才发现他们的歌不仅是旋律厉害，这歌词也是一流的。</p><p>背单词，学课文，过完大一的第二学期，标日初级也基本学完了，当时候就开始想更进一步地开发学习的乐趣，于是进军二次元轻小说了，在kindle上面看完了几本动漫化的轻小说，也学会了很多单词，也在轻小说里面巩固了自己的句节的分解的能力，说到底还是语法基础，但是那时候开始，学习日语的反馈没有一开始那么明显了，学来学去都是很多单词看不懂，开始觉得有点无聊，热情也开始消减。到大二时，简单的文章基本都能看懂，但是复杂的文章看不懂，也不想硬啃，因为啃完所有单词，单词也大多都不会出现在日常生活中，所以就算背完一次，没有后续的巩固，也很快会忘掉，和一开始学的简单的日常生活的单词不同，学完那些单词也很难在番剧或者歌曲里面得到反馈，很多都是低频词。感觉进入了瓶颈期，但是还是尽力啃完了标日中级两册。那时候单看课文也越来越枯燥。于是开始上网看youtube，twitter，看到真实的日本，发现真实的日本并没有多么美好，对我们中国的情感也是很奇怪的一致的厌恶。开始慢慢不知道为什么一开始要学日语，难道学习日语就是为了看这些吗。但是我过了一会儿还是不信邪，我还是觉得哪里都有脑残，还是没有放弃学日语。虽然看到了很多不好的东西，但是也看到了很多美好的东西，只要不涉及政治，很多时候看日本还是很美好的，有很多美的音乐，有很多精彩的动漫，有好看的樱花，有精美的和服，不得不说，他们的文化输出实在是太厉害了。</p><p>在那段时间，我看到了很多负面的东西，但同时也会看到美好的东西。越来越深刻地感受到一些日语单词的含义，像是「しあわせ」「辿り着いた」「あたたかい」「ひかり」「うれしい」「ほしぞら」「はなび」「いのり」。虽然这些单词都有对应的汉字，但是尽管只是写成表音文字的形式，我也能体会到这单词背后的意思，同时在我的脑海中这发音也与单词的美好本意紧紧地联系在了一起。小小的一个反馈，但也能支持我继续学习下去</p><p><img src="https://i.loli.net/2021/08/27/sWkiVBHw6lRtTY8.png"></p><p>尽管在接触日本文化后，我就大概知道日本的社会的压抑感，但是我还是想去看落樱，想去感受京都的唐朝留下来的文化风貌，想去看烟火大会，想去祭り</p><p><img src="https://i.loli.net/2021/08/27/oFGnQtBe6uXmClP.png"></p><p>一路下来，也坚持了不久了。于是想去日本留学。当时候看到学校有交流项目，如果我想大三去日本交流的话，就要在今年的7月份把N2考了，于是21年的寒假就开始复习备考了，整个寒假都是在学日语（最后在练车），每天6，7个小时，感觉算是高考之后少有的高强度学习了，每天红宝书，蓝宝书，一个单元单词，一个单元语法，WPS，OneNote，各种软件都用上。</p><p><img src="https://i.loli.net/2021/08/27/OqfaWgpQTivUYVR.png">现在想想自己的毅力还是有点强的，中间也会觉得累，但是想到是自己喜欢，自己热爱的东西，也没感觉什么。记得当时看BangDream，看到女主她们站到武道馆上时，唱着</p><p><strong>同じ景色見つめながら</strong></p><p><strong>走り続けるトレイン</strong></p><p><strong>わたしたちを連れていく</strong></p><p>感觉自己就是那样，大多数时间都是重复着做着同样的事，每天背单词，背语法，反馈不是很明显。但是自己知道自己正在慢慢进步，总会有「夢を打ち抜く」的时候。那时应该就是考完试的时候吧，当时会想</p><p><strong>夢を撃ち抜く瞬間に、君は何を思うの</strong></p><p>想知道自己考过N2时的心情，是一种怎么样的心情来见证自己的追求的喜欢的事成功的一刻的。</p><p>现在到了这个时候，却又挺平淡的。是一种努力过，然后顺其自然的感觉，知道自己肯定能过N2，所以当知道自己过了的时候就没有什么激动的感觉了。这可能就是</p><p><strong>到得还来别无事，庐山烟雨浙江潮</strong></p><p>倒也不是什么大事，只是努力过程中的一个阶段的结束的标记，路还很长，路上的风光更加令人感动</p><p><img src="https://i.loli.net/2021/08/27/iAZFWjvz96D5BXm.png"></p><p>补上考完三个月之后才到的证书</p><p><img src="https://raw.githubusercontent.com/Mark-Sky/picGo/main/C%3A%5CUsers%5Czqf%5COneDrive%5C%E5%9B%BE%E7%89%87%5CSaved%20PicturesIMG_2199.JPG"></p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习linux</title>
    <link href="/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0linux/"/>
    <url>/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0linux/</url>
    
    <content type="html"><![CDATA[<h2 id="服务器后台训练"><a href="#服务器后台训练" class="headerlink" title="服务器后台训练"></a>服务器后台训练</h2><h3 id="nohup-amp"><a href="#nohup-amp" class="headerlink" title="nohup + &amp;"></a>nohup + &amp;</h3><p>用nohup和&amp;一起，即可使得进程在服务器的后台运行</p><p>如果只用nohup，则ctrl+c会使得进程结束</p><p>如果只用&amp;，则关闭终端会使得进程结束</p><p>一般最简单的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup [Command] &amp;<br></code></pre></td></tr></table></figure><p>可以让程序在后台运行而不会因终端关闭或者断网而被终止，进程的输出会被默认重定向到nohup.out文件中</p><p>比如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup python trainer.py &amp;<br></code></pre></td></tr></table></figure><p>运行trainer文件的输出会被重定向到当前目录下面的nohup.out文件中</p><p>而如果希望输出重定向到人为设定的文件中，则用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup [Command] &gt; [output file] 2&gt;&amp;1 &amp; <br></code></pre></td></tr></table></figure><p>在上面的例子中，0 – stdin (standard input)，1 – stdout (standard output)，2 – stderr (standard error) ；</p><p>2&gt;&amp;1是将标准错误（2）重定向到标准输出（&amp;1），标准输出（&amp;1）再被重定向输入到myout.file文件中。</p><p>比如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup python trainer.py &gt; myoutput.txt 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>如果不想启用缓存，也就是直接把输出放到log中的话，那就加上参数u</p><p><strong>注意一定要加参数u，本人因为没加参数u直接白跑两次</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup python -u trainer.py &gt; myoutput.txt 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><h3 id="查看进程-显卡"><a href="#查看进程-显卡" class="headerlink" title="查看进程/显卡"></a>查看进程/显卡</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nvidia-smi<br></code></pre></td></tr></table></figure><p>如果没有进程在运行，那么输入完上面的命令会大概出现下面的样子</p><p><img src="https://i.loli.net/2021/08/25/lTtvkeRiQmU32Z8.png"></p><p>如果有进程在显卡上面运行，留意Processes就会看到进程</p><p><img src="https://i.loli.net/2021/08/25/rcufgobOGQVR1NC.png"></p><p>除了用查看显卡的方法查看进程，还可以使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">jobs -l<br></code></pre></td></tr></table></figure><p>但是jobs命令只能查看在自己终端上面运行的进程，不能看到服务器上其他的进程</p><p>如果想看到服务器上的其他运行的进程，可以用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ps -aux<br></code></pre></td></tr></table></figure><p>1）-a 显示现行终端机下的所有程序，包括其他用户的程序</p><p>2）-u 用户为主的格式来显示程序状况</p><p>3）-x 显示所有程序，不以终端机来区分</p><h3 id="终止进程"><a href="#终止进程" class="headerlink" title="终止进程"></a>终止进程</h3><p>如果使用了nohup+&amp;，就不能通过ctrl+c或者关闭终端来终止进程了，那么我们希望结束一个进程的时候，就需要使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kill -9 [PID]<br></code></pre></td></tr></table></figure><p>比如：如果我知道我要结束的进程的进程号是32580，则只需要像下面这样</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kill -9 32580<br></code></pre></td></tr></table></figure><h2 id="显卡设置"><a href="#显卡设置" class="headerlink" title="显卡设置"></a>显卡设置</h2><p>用服务器运行代码的目的一是本地运行的内存太小，基本上大一点的机器学习任务中途都会被linux操作系统kill掉，二是服务器上能用显卡运行程序，显卡深度学习的速度比CPU的速度要快，因此我们通常会使用服务器来运行深度学习训练</p><p>通常有两种方法可以设置在哪个显卡上面运行</p><h3 id="py文件"><a href="#py文件" class="headerlink" title="py文件"></a>py文件</h3><p>在要运行的py文件的前面几行，加入下面代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;0,1&quot;</span><br></code></pre></td></tr></table></figure><p>其中CUDA是加速深度学习的框架，需要自行到网上下载安装到linux中，一般的深度学习框架都可以用CUDA加速，像是Tensorflow和Pytorch</p><p>而0，1则是选择要运行的显卡，可以用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nvidia-smi<br></code></pre></td></tr></table></figure><p>查看空闲的显卡，然后就用空闲的显卡运行程序</p><h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><p>还可以在终端设置运行的显卡</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">CUDA_VISIBLE_DEVICES=3,4,1 python train.py<br></code></pre></td></tr></table></figure><p>但是这样就无法后台运行程序了，因为如果我们加上nohup</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">nohup CUDA_VISIBLE_DEVICES=3,4,1 python train.py &amp;<br></code></pre></td></tr></table></figure><p>就会报错指令不对</p><p>因此最好的方法还是在py文件中添加os.environ设置显卡</p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Skip-Gram</title>
    <link href="/2021/08/20/Skip-Gram/"/>
    <url>/2021/08/20/Skip-Gram/</url>
    
    <content type="html"><![CDATA[<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul><li><p>给定一个中心单词centerWord，通过该算法可以预测出最有可能在该中心单词附近的单词context Words</p></li><li><p>当训练集较小的时候，也能有很高的准确率，对比较少见的单词或短语的效果预测效果较好</p></li></ul><hr><ul><li><p>需要注意的是，<strong>Skip-Gram算法的最终目标并不是用于预测context Words，而是用于给出一个比较好的单词的向量表示</strong>，这也是为什么Word2Vec经常会和Skip-Gram一起出现的原因</p></li><li><p>用Skip-Gram算法得到的单词向量Word Vectors可以用于寻找related words，还可以用于 Analogy tasks</p></li></ul><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>Skip-Gram可以理解为一个<strong>神经网络</strong>的模型</p><p><img src="https://i.loli.net/2021/08/20/toOmJdU9CMyKbrF.png"></p><h3 id="输入-input"><a href="#输入-input" class="headerlink" title="输入 input"></a>输入 input</h3><p>Skip-Gram的输入为单词的one-hot编码的向量</p><h3 id="输入到隐层-W-input"><a href="#输入到隐层-W-input" class="headerlink" title="输入到隐层  $W_{input}$"></a>输入到隐层  $W_{input}$</h3><p>当隐层的维数为N时，$W_{imput}$为$V\times N$的矩阵，该矩阵不是普通的矩阵，可以理解为每个单词的单词向量，如图中，矩阵的每一行为一个3维的向量，因此在这里，单词空间为3维空间，每个单词的Word Vector为一个3维的向量。<strong>这是因为输入为one-hot编码的向量，因此在这里每一行就可以看作一个单词向量</strong></p><h3 id="隐层-hidden"><a href="#隐层-hidden" class="headerlink" title="隐层    $hidden$"></a>隐层    $hidden$</h3><p>隐层的N维向量就可以看作是单词的Word Vector,其实也就是$W_{input}$对应的一行</p><h3 id="隐层到输出层-W-output"><a href="#隐层到输出层-W-output" class="headerlink" title="隐层到输出层    $W_{output}$"></a>隐层到输出层    $W_{output}$</h3><p>实际上$W_{output}$​矩阵也是一个单词向量的编码，每一列是一个N维的单词向量Word Vector。但是这里的单词向量编码和前面的单词向量的编码是不同的，也就是说每个单词都有两种Word Vector表示：</p><ul><li><strong>当单词是center Word时，该单词的向量表示是$W_{input}$​​中的一行</strong></li><li><strong>当单词是context Word时，该单词的向量表示是$W_{output}$​中的一列</strong></li></ul><p>centerWord Vectors是输入到隐层的矩阵</p><p>outsideWord Vectors是隐层到输出层的矩阵（context Words的矩阵）</p><p>对于这里的理解，要<strong>结合softmax函数预测的概率来理解</strong></p><p><img src="https://i.loli.net/2021/08/20/VCHSsgFDORTzAXq.png"></p><p>虽然是矩阵表示$W_{output}$，但是其原理跟BP神经网络并不一样，也就是该矩阵不是单单跟隐藏层的向量相乘得到输出层的输入，而是用上面的softmax函数，用该矩阵与隐藏层一起作为softmax函数的输入来进行预测</p><h3 id="输出层-y-pred"><a href="#输出层-y-pred" class="headerlink" title="输出层 $y_{pred}$"></a>输出层 $y_{pred}$</h3><p>输出层就如上面所述，使用softmax函数作为激活函数，输出一个$V$维的向量</p><p>训练时用于与真实的one-hot向量作比较，得到误差</p><p>测试时该$V$维向量的最大的一个维度就是对应的所预测的向量</p><h2 id="训练-train"><a href="#训练-train" class="headerlink" title="训练 train"></a>训练 train</h2><p>我们需要确定机器学习的目标函数，也就是loss。机器学习的目的就是在测试集上达到loss的最小化，而训练的方向就是在训练集上达到loss的最小化，认为在训练集上达到loss最小化的模型在测试集上面也能达到一个比较好的效果。</p><p>首先，我们需要训练的参数就是$\theta=(W_{input},W_{output})$​​,分别表示中心单词的矩阵和文本单词的矩阵<br>$$<br>loss = J(\theta)=-\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>其中$T$​为整个语料库的大小，$w$​为设置的窗口的大小</p><p>显然当context Word出现在center Word附近的时候，我们需要让它的条件概率最大化，当上面loss最小化的时候，就达到了条件概率的最大化</p><p>整个过程就是遍历整个语料库，然后对每一个语料库中遍历到的单词$W_t$，计算<br>$$<br>J(\theta,W_t) = \sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>而对于每一个给定的context word$W_{t+j}$,显然所需要计算的就是<br>$$<br>\log(P(W_{t+j}|W_t;\theta))<br>$$<br>对于概率$P(W_{t+j}|W_t;\theta)$的计算，我们暂时有两种计算方法</p><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>把$W_{t+j}$也就是context word记为$u_o$​, 就是$\theta$的$W_{output}$中的一列单词向量</p><p>把$W_t$也就是center word记为$v_c$，就是$\theta$的$W_{input}$中的一行单词向量​</p><p>用softmax的公式<br>$$<br>P(W_{t+j}|W_t)=\frac{exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)}<br>$$<br>不难看出，这种方法概率的最后的和为1，也就是$\sum\limits_{i=1}^TP(Wi|W_t)=1$​</p><p>根据该损失函数，用梯度下降法，计算$\frac{\partial J}{\partial \theta}$<br>$$<br>\frac{\partial }{\partial v_c}(\frac{exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)})=W_{output}(y_{pred}-y_{true})<br>$$<br>对于$W_{input}$矩阵，我们只需要更新当前的中心单词的向量，因为其他单词向量的中心单词编码形式并没有出现在当前的损失函数中，因此并不会有其他单词向量的梯度更新</p><p>这里的$y_{true},y_{pred}$​都是V维的向量，</p><ul><li><p>$y_{pred}$​的每一个维度表示<strong>每一个单词的是当前中心单词的文本单词的概率</strong></p></li><li><p>$y_{true}$​​​​是语料库单词的one-hot编码</p></li></ul><p>$$<br>\frac{\partial}{\partial W_{output}}(\frac{\exp(u_o^Tv_c)}{\sum\limits_{i=1}^Texp(u_i^Tv_c)})=v_c(y_{pred}-y_{true})^T<br>$$</p><p>$$<br>y_{pred}=softmax(W_{output}\cdot v_c )<br>$$</p><p>对于$W_{output}$矩阵，我们就需要对整个矩阵进行更新了，因为在softmax函数中，可以看到语料库中的所有单词都是出现在了该函数的分母中的，因此需要对整个矩阵求梯度并更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">naiveSoftmaxLossAndGradient</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">    centerWordVec,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideWordIdx,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideVectors,</span></span><br><span class="hljs-params"><span class="hljs-function">    dataset</span></span><br><span class="hljs-params"><span class="hljs-function"></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot; Naive Softmax loss &amp; gradient function for word2vec models</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the naive softmax loss and gradients between a center word&#x27;s </span><br><span class="hljs-string">    embedding and an outside word&#x27;s embedding. This will be the building block</span><br><span class="hljs-string">    for our word2vec models. For those unfamiliar with numpy notation, note </span><br><span class="hljs-string">    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which</span><br><span class="hljs-string">    you can effectively treat as a vector with length x.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    centerWordVec -- numpy ndarray, center word&#x27;s embedding</span><br><span class="hljs-string">                    in shape (word vector length, )</span><br><span class="hljs-string">                    (v_c in the pdf handout)</span><br><span class="hljs-string">    outsideWordIdx -- integer, the index of the outside word</span><br><span class="hljs-string">                    (o of u_o in the pdf handout)</span><br><span class="hljs-string">    outsideVectors -- outside vectors is</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    for all words in vocab (tranpose of U in the pdf handout)</span><br><span class="hljs-string">    dataset -- needed for negative sampling, unused here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">    loss -- naive softmax loss</span><br><span class="hljs-string">    gradCenterVec -- the gradient with respect to the center word vector</span><br><span class="hljs-string">                     in shape (word vector length, )</span><br><span class="hljs-string">                     (dJ / dv_c in the pdf handout)</span><br><span class="hljs-string">    gradOutsideVecs -- the gradient with respect to all the outside word vectors</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    (dJ / dU)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">### YOUR CODE HERE (~6-8 Lines)</span><br><br>    <span class="hljs-comment">### Please use the provided softmax function (imported earlier in this file)</span><br>    <span class="hljs-comment">### This numerically stable implementation helps you avoid issues pertaining</span><br>    <span class="hljs-comment">### to integer overflow. </span><br>    y_pred = softmax(outsideVectors @ centerWordVec)<br>    y = np.zeros(<span class="hljs-built_in">len</span>(outsideVectors))<br>    y[outsideWordIdx] = <span class="hljs-number">1</span><br>    gradCenterVec = outsideVectors.T @ (y_pred - y)<br>    gradOutsideVecs = np.multiply(centerWordVec, np.expand_dims(y_pred - y, axis = <span class="hljs-number">1</span>))<br>    loss = -(outsideVectors[outsideWordIdx].T @ centerWordVec) + np.log(np.<span class="hljs-built_in">sum</span>(np.exp(outsideVectors @ centerWordVec)))<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> loss, gradCenterVec, gradOutsideVecs<br></code></pre></td></tr></table></figure><p>显然，这种方法的分母的计算开销太大，我们需要计算整个语料库的所有单词向量与中心向量相乘,因此在实际应用中，通常会采用negative sample的方法来减少计算开销</p><h3 id="negative-sample"><a href="#negative-sample" class="headerlink" title="negative sample"></a>negative sample</h3><p>回顾一下上面的损失函数的计算<br>$$<br>loss = J(\theta)=-\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{-w&lt;j&lt;w}\log(P(W_{t+j}|W_t;\theta))<br>$$<br>可以看到$-\frac{1}{T}\sum\limits_{t=1}^T$，为了消去该项，我们采用负采样的方法</p><p>与此同时，在负采样中，概率的计算也和上面的softmax函数是不同的，不会有遍历整个语料库的分母计算</p><p>对于每一个单个的中心单词，我们的目标函数为：<br>$$<br>\arg\max\limits_{\theta}p(D=1|w,c_{pos;\theta})\prod\limits_{c_{neg}\in W_{neg}}p(D=0|w,c_{neg;\theta})<br>$$<br>$W_{neg}$为所有负采样得到的样本，负采样就是那些噪声样本，这些样本没有出现在中心单词的附近</p><p>其中$p(D=1|w,c_{pos;\theta})$表示在语料库中出现了$(w,c_{pos})$​​​这一单词对在窗口中的概率</p><p>$p(D=0|w,c_{neg};\theta)$表示在语料库中没有出现$(w,c_{neg})$这一单词对在窗口中的概率</p><p>$c_{pos}$就是正样例，表示确实在语料库中在中心单词w的窗口内出现过$c_{pos}$这一单词</p><p>$c_{neg}$就是负样例，表示语料库中中心单词w的窗口内没有出现过$c_{neg}$这一单词</p><p>把(14)式进一步化简<br>$$<br>\arg\max\limits_{\theta}\log p(D=1|w,c_{pos};\theta)+\sum\limits_{c_{neg}\in W_{neg}}(1-\log p(D=1|w,c_{neg};\theta))<br>$$<br>对于该概率$p(D=1|w,c;\theta)$​，我们不采用softmax的计算方法，而是用sigmoid函数来计算<br>$$<br>p(D=1|w,c;\theta)=\frac{1}{1+exp(-c_{output}\cdot w)}<br>$$<br>$c_{output}$就是该单词在$W_{output}$中的对应的单词向量</p><p>因此(10)式可以改为<br>$$<br>\arg\max\limits_{\theta}\log \frac{1}{1+exp(-c_{pos}\cdot w)}+\sum\limits_{c_{neg}\in W_{neg}}(1-\log \frac{1}{1+exp(-c_{neg}\cdot w)})<br>$$</p><p>$$<br>\arg\max\limits_{\theta}\log \frac{1}{1+exp(-c_{pos}\cdot w)}+\sum\limits_{c_{neg}\in W_{neg}}\log \frac{1}{1+exp(c_{neg}\cdot w)}<br>$$</p><p>设$\sigma(x)=\frac{1}{1+exp(-x)}$​<br>$$<br>\arg\max\limits_{\theta}\log\sigma(-c_{pos}\cdot w)+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w)<br>$$<br>从上式可以看出，对每一个中心单词进行损失计算的话，不需要遍历整个语料库进行计算，而只需要计算负样本和正样本就可以了。因此计算梯度并更新参数的时候，我们只需要更新$w,c_{pos},W_{neg}$</p><p>正常来讲，损失函数应该是<br>$$<br>J(\theta)=-\frac{1}{T}\sum\limits_{i=1}^T\sum\limits_{-c&lt;j&lt;c}(\log\sigma(-c_{pos}\cdot w)+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w))<br>$$<br>这种方法在遍历完一次整个语料库后对参数进行更新，叫做<strong>batch gradient descent</strong></p><p>但是因为它的高计算开销，我们基本不会在实际应用中使用到</p><p>一般实际应用中，采用的是随机梯度下降法 <strong>stochastic gradient descent</strong></p><p>我们对每一对正训练对$(w,c_{pos})$进行一次梯度的计算并对其相对应的单词向量进行更新<br>$$<br>J(\theta;w,c_{pos})=-\log\sigma(c_{pos}\cdot w)-\sum\limits_{c_{neg}\in W_{neg}} \log\sigma(-c_{neg}\cdot w)<br>$$<br><strong>在更新的时候，我们仅仅更新当前的中心单词的$W_{input}$里的单词编码，以及$c_{pos},c_{neg}\in W_{neg}$在$W_{output}$里面的单词编码。</strong></p><p>显然这种做法大大减小了计算开销，训练速度较快。</p><p>对于batch gradient descent和stochastic gradient descent，可以参考标准BP算法和累计BP算法的区别。标准BP算法每一更新只针对一个样例，参数的更新非常频繁，而且对不同的样例更新的时候可能会出现“抵消”的现象，同时为了达到<strong>累计误差</strong>极小值点，标准BP算法需要进行更多次数的迭代。累计BP算法直接针对<strong>累计误差</strong>进行最小化，它在读取整个训练集一次之后才更新参数，参数更新的频率要低得多。但在很多任务中，累计误差在下降到一定程度后，进一步的下降就会非常的慢，这时标准BP算法会获得更好的解，尤其是在训练集较大的时候更为明显</p><p>对于每一个正训练对$(w,c_{pos})$​​,梯度计算如下：$w$是$W_{input}$的编码，$c_{pos}$是$W_{output}$的编码<br>$$<br>\frac{\partial J}{\partial c_{pos}}=(\sigma(c_{pos}\cdot w)-1)\cdot w<br>$$</p><p>$$<br>\frac{\partial J}{\partial c_{neg}}=\sigma(c_{neg}\cdot w)\cdot w<br>$$</p><p>$$<br>\frac{\partial J}{\partial w}=(\sigma(c_{pos}\cdot w)-1)\cdot c_{pos}+\sum\limits_{c_{neg}\in W_{neg}}\sigma(c_{neg}\cdot w)\cdot c_{neg}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">negSamplingLossAndGradient</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">    centerWordVec,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideWordIdx,</span></span><br><span class="hljs-params"><span class="hljs-function">    outsideVectors,</span></span><br><span class="hljs-params"><span class="hljs-function">    dataset,</span></span><br><span class="hljs-params"><span class="hljs-function">    K=<span class="hljs-number">10</span></span></span><br><span class="hljs-params"><span class="hljs-function"></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot; Negative sampling loss function for word2vec models</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the negative sampling loss and gradients for a centerWordVec</span><br><span class="hljs-string">    and a outsideWordIdx word vector as a building block for word2vec</span><br><span class="hljs-string">    models. K is the number of negative samples to take.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Note: The same word may be negatively sampled multiple times. For</span><br><span class="hljs-string">    example if an outside word is sampled twice, you shall have to</span><br><span class="hljs-string">    double count the gradient with respect to this word. Thrice if</span><br><span class="hljs-string">    it was sampled three times, and so forth.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># Negative sampling of words is done for you. Do not modify this if you</span><br>    <span class="hljs-comment"># wish to match the autograder and receive points!</span><br>    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)<br>    indices = [outsideWordIdx] + negSampleWordIndices<br>    <span class="hljs-comment">### YOUR CODE HERE (~10 Lines)</span><br><br>    <span class="hljs-comment">### Please use your implementation of sigmoid in here.</span><br>    loss = -np.log(sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec)) - np.<span class="hljs-built_in">sum</span>(np.log(sigmoid(-outsideVectors[negSampleWordIndices] @ centerWordVec)))<br>    posGrad = (sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec) - <span class="hljs-number">1</span>) * centerWordVec<br>    negGrad = np.expand_dims(sigmoid(outsideVectors[negSampleWordIndices] @ centerWordVec),axis =<span class="hljs-number">1</span>) @ np.expand_dims(centerWordVec,axis = <span class="hljs-number">0</span>)<br>    gradOutsideVecs = np.zeros(outsideVectors.shape)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(K):<br>        gradOutsideVecs[negSampleWordIndices[i]] += negGrad[i]<br>    gradOutsideVecs[outsideWordIdx] += posGrad<br>    gradCenterVec = (sigmoid(outsideVectors[outsideWordIdx].T @ centerWordVec) - <span class="hljs-number">1</span>) * outsideVectors[outsideWordIdx] +\<br>                    np.<span class="hljs-built_in">sum</span>(np.expand_dims(sigmoid(outsideVectors[negSampleWordIndices] @ centerWordVec),axis=<span class="hljs-number">1</span>) * outsideVectors[negSampleWordIndices],axis = <span class="hljs-number">0</span>)<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> loss, gradCenterVec, gradOutsideVecs<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Intuitive Explanation of Field Aware Factorization Machines</title>
    <link href="/2021/08/14/An%20Intuitive%20Explanation%20of%20Field%20Aware%20Factorization%20Machines/"/>
    <url>/2021/08/14/An%20Intuitive%20Explanation%20of%20Field%20Aware%20Factorization%20Machines/</url>
    
    <content type="html"><![CDATA[<p>From LM to Poly2 to MF to FM to FFM</p><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>线性回归是最简单无脑的拟合一个因变量和一个或多个自变量之间函数关系的模型<br>$$<br>f(x_1,x_2)=w_1x_1+w_2x_2<br>$$<br>一般来说，线性回归模型不会表现得很好，因为它试图对每个自变量的行为分别拟合，而没有考虑到自变量之间有可能会有相互作用影响，也就是有可能$x_1$可能会依赖于$x_2$​</p><p><img src="https://i.loli.net/2021/08/14/NUX8tiWzfRonDVF.png"></p><h2 id="Poly2"><a href="#Poly2" class="headerlink" title="Poly2"></a>Poly2</h2><p><strong>为了理解Poly2和FM,FFM的内容，我们需要先搞清楚它们的自变量的域是什么</strong></p><p>对于原本的离散的域，我们会用one-hot编码生成新的域，如上面的图会转变成下面这样</p><table><thead><tr><th>Gender: Male</th><th>Gender: Female</th><th>Genre: Action</th><th>Genre: Romance</th><th>Genre: Thriller</th><th>Genre: Sci-Fi</th><th>Rating</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>5</td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>3</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>4</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>4</td></tr></tbody></table><p>为了改进线性模型，Poly2就加入了一个相互作用项<br>$$<br>f(x_1,x_2)=w_1x_1+w_2x_2+w_{1,2}x_1x_2<br>$$<br>对所有的特征都两两组合<br>$$<br>\phi_{Poly2}(w,x)=\sum\limits_{j_1=1}^n\sum\limits_{j_2=1}^nw_{h(j_1,j_2)}x_{j1}x_{j2}<br>$$<br>但是Poly2模型也有对应的几个缺点</p><p>一. 如果<strong>两个特征很少交互</strong>，那么预测是不可靠的</p><p>比如训练集中有1万个训练样本，但是只有两个样本是<strong>男性</strong>观看<strong>恐怖电影</strong>的，那么我们未来预测男性观看恐怖电影的打分的时候，就是以这两个样本作为基准预测的（也就是这两个特征组合的权重是依赖着两个样本点的）。显然这是不可靠的</p><p>二.对于一些<strong>零相互作用</strong>的特征组合，则预测也是不准的</p><p>假设如果没有女性观看科幻电影的打分的样本，则根据上面的模型的打分是没有意义的</p><p>三.由于用极暴力的手段对特征进行了两两组合，造成了<strong>数据稀疏性</strong>的问题</p><p>其实这一点和上面的两点是类似的，就是没有该特征组合的情况会很多，这时候该特征组合的权重难以收敛</p><table><thead><tr><th>A</th><th>B</th><th>C</th><th>D</th><th>F</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>1</td><td>4</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>6</td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>8</td></tr></tbody></table><p>如图，A，B，C，D都是自变量，其取值为(0,1)，F是连续的因变量</p><p>在这个样本集中，想要训练到$x_Bx_D$的权重是不可能的，因为$x_B,x_D$没有同时为1的情况，所以$w_{B,D}$无法训练得出。这就是样本稀疏性问题。</p><p>因为相互作用项一共有$n^2$​个，在上面的自变量中，想要训练到所有的相互作用项权重，至少需要16个样本，现实中很难使得样本的数量可以满足所有相互作用项权重的训练。</p><p>对于样本稀疏性的问题，通常可以使用<strong>降维</strong>的手段，使得样本矩阵变得稠密</p><p>容易看出，训练互相作用项的复杂度是$O(n^2)$</p><h2 id="MF"><a href="#MF" class="headerlink" title="MF"></a>MF</h2><p>MF全称Matrix Factorization，意思是矩阵分解</p><p>在该方法中，我们使用不同的方法表示样本，用推荐系统中常见的user-item形式</p><p><img src="https://i.loli.net/2021/08/14/vGdNMtQlJErPH7i.png"></p><p>每一行代表用户（user），每一列代表电影（item），矩阵的每一个元素表示用户给该电影的打分</p><p>我们要做的就是生成两个子空间，用户空间和商品空间，每一个用户对应用户空间里面的一个向量，同理每一部电影对应商品空间的一个向量</p><p>相对应的，就是把上面的矩阵分解<br>$$<br>\hat{R}=P\cdot Q^T<br>$$<br>$R$是$M\times N$​的矩阵（有M个用户，N部电影），$P$是$M\times k$的矩阵，每个用户对应P矩阵里面的一行向量，也就是每个用户用一个k维的向量表示；$Q$是$N\times k$的矩阵，每部电影用一个k维的向量表示。</p><p>分解的过程一般使用<strong>梯度下降法</strong>或者<strong>最小二乘法</strong>去逼近原矩阵，把损失函数的值降为最小</p><p><img src="https://i.loli.net/2021/08/14/Lmb23SFkW6AzwCt.png"></p><p><strong>MF对线性回归和Poly2的改进</strong></p><p>从上面的图可以看出，因为隐向量的存在，即使我们没有第一个用户对memento的打分情况，我们仍然可以训练出来他对memento的打分的情况。对数据稀疏性的问题有了较好的解决。</p><p><strong>MF的缺点</strong></p><p>它只是把用户项目矩阵分解的一个简单想法，没有考虑电影类型，语言等辅助的判断信息</p><p>因此这时候引进FM方法</p><h2 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h2><p>FM全称Factorization Machines（FM）<br>$$<br>y=w_0+\sum\limits_{i=1}^nw_ix_i+\sum\limits_{i=1}^n\sum\limits_{j=i+1}^n&lt;v_i,v_j&gt;x_ix_j<br>$$<br>对每一个特征，都分配一个向量$v$来表示，所有的向量$v_i\quad i\in n$组成一个矩阵，该矩阵就是机器学习需要学习的参数之一。</p><p>若令该潜在的向量空间的维数为$k$维，也就是每个特征对应一个$k$维的向量，则学习相互作用项的复杂度为$O(nk)$。可以看出比Poly2的复杂度要低（一般令$k&lt;n$)</p><p>且FM解决了数据稀疏性的问题，因为当我们需要$(A_1,B_1)$对应的相互作用权重的时候，我们可以通过$(A_1,B_2)$训练$A_1$对应的隐向量的参数，同样$(A_2,B_1)$也可以训练$B_1$对应的隐向量的参数。即使我们在训练集中没有$(A_1,B_1)$这个组合，我们也可以训练到对应的权重，因为一个相互交互项也会和其它交互项会有关系，也就是说得到一个样本，训练该样本的交互项的同时会潜在地训练与该交互项相关的其他的交互项，这样就使得即使我们没有在数据集中得到$(A_1,B_1)$交互项的信息，但是也可以训练到该交互项的权重</p><p><img src="https://i.loli.net/2021/08/14/8GbAMTvFJNgIoj1.png"></p><p>如图，我们把用户-项目交互项用one-hot来编码，每行对应一个用户和一个项目，同时还加入了一些辅助的信息（以改进MF的过于简单的矩阵分解算法），辅助信息可以是比如该用户对其他电影的打分，该用户观看的最后一步电影等等。</p><p>FM的相互交互项还有降低计算复杂度的公式：</p><p><img src="https://i.loli.net/2021/08/14/Qagf1UDtu5VYHn4.png"></p><h2 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h2><p>FFM全称Field Aware Factorization Machines</p><p>FFM是FM模型的改进算法，Field – 域</p><p><img src="https://i.loli.net/2021/08/14/oV3qGn6yiWAm5Ll.png"></p><table><thead><tr><th>Gender: Male</th><th>Gender: Female</th><th>Genre: Action</th><th>Genre: Romance</th><th>Genre: Thriller</th><th>Genre: Sci-Fi</th><th>Rating</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>5</td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>3</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>4</td></tr><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>4</td></tr></tbody></table><p>FFM与FM最大的区别就是权重计算的不同</p><p>FM计算权重，以第一行为例，则为<br>$$<br>v_{male}\cdot v_{action}+v_{male}\cdot v_{northAmerica}+v_{action}\cdot v_{northAmerica}<br>$$<br>这样male和Genre域的特征隐向量相乘的隐向量和male和Region域的特征隐向量相乘的隐向量都是$v_{male}$</p><p>但是直观上来看，$&lt;v_{male},v_{action}&gt;$和$&lt;v_{male},v_{northAmerica}&gt;$是由区别的，也就是说male这个域和不同的域的相关性是不同的，我们应该更细致地划分male和不同域之间的权重</p><p>于是就有FFM的计算权重的方法：<br>$$<br>v_{male,genre}\cdot v_{action,gender}+v_{male,region}\cdot v_{NAmerica,gender}+v_{action,region}\cdot v_{NAmerica,genre}<br>$$<br>隐向量的个数变得更多</p><p>众所周知，越复杂的模型就越容易过拟合，当数据量足够大的时候，我们才应该考虑使用FFM，数据量小的时候用FM效果更好</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://i.loli.net/2021/08/14/PSr7JUjhoxX9Yb1.png"></p><p>英语参考原文，中间也加入了我自己的一些理解</p><p><a href="https://towardsdatascience.com/an-intuitive-explanation-of-field-aware-factorization-machines-a8fee92ce29f">https://towardsdatascience.com/an-intuitive-explanation-of-field-aware-factorization-machines-a8fee92ce29f</a></p>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>日语杂学（1）</title>
    <link href="/2021/08/14/%E6%97%A5%E8%AF%AD%E6%9D%82%E5%AD%A6%EF%BC%881%EF%BC%89/"/>
    <url>/2021/08/14/%E6%97%A5%E8%AF%AD%E6%9D%82%E5%AD%A6%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>​    <img src="https://i.loli.net/2021/08/14/9VdoGY3iy7eI4X1.jpg"></p><p>​    最近了解到的冷知识，第一次看到感觉有点震惊。平时总是说日语的汉字都是抄中国的，我在学习的时候也总是觉得这个词不就是中文的那个词吗，印象中总是日本把中国的东西搬了过去。</p><p>​    像是“社会”“主义”，如果有人告诉我，这两个词是日本造的汉字，应该不太难接受，因为知道日本在明治维新后改革了国家体制，走上资本主义道路，思想也基本“脱亚入欧”，吸取了很多西方的先进的思想，中国在甲午海战被打败后才意识到自己的不足，后面孙中山那些人也基本都去了日本学习西方的思想。社会主义又是这么先进的词汇，因此大概也可以理解为是20世纪初孙中山他们从日本传回中国的词汇。</p><p>​    但是像是“历史”“表现”“场合”“取消”“手续”，这些词，居然也是从日语反向传播到中文里的，我觉得有点不可思议。毕竟像是“历史”，感觉应该中国古代就会出现的词语。查了一下，确实是中国古代就出现过的词语，在东晋时代，古书里就出现过</p><blockquote><p><a href="https://baike.baidu.com/item/%E8%A3%B4%E6%9D%BE%E4%B9%8B">裴松之</a> 注引《<a href="https://baike.baidu.com/item/%E5%90%B4%E4%B9%A6/36618">吴书</a>》：“﹝ 吴王 ﹞志存经略，虽有馀闲，博览书传<strong>历史</strong>，藉采奇异，不效诸生寻章摘句而已。”</p></blockquote><p>但是，在当时似乎没有流传开来该词。在古代一般都以“史”字表示历史，大概是古代汉语讲究语言的效率所以没有流传开来吧，毕竟“你吃了饭没有”，在古代只需要用“饭否”两字就可以表示。”历史“一词是日本学者在翻译英语的history的时候把其翻译为“历史”，最后流传到了中国，广泛使用。</p><p>​    还有“手续”那一个词，我一开始以为它是音读的词语，大概是读成“しゅうぞく”这个感觉，结果第一次查读音，发现是训读的”てつづき”,当时的我还觉得这日本照搬的词语都不会汉字发音，要自己搞个这么怪的发音哈哈，现在才知道小丑竟是我自己。原来是中国抄的日本的词语。或许可以给点启发，日语词中如果是训读的汉字，那大概是日本自创的汉字词，而不是中国传过去的汉字。同样的还有「場合」，也是训读「ばあい」</p><p>​    这样看来，语言真的是不断随着时间变化，流传。我们早已习以为常的东西，或许只是最近的100年才出现的，如果穿越回到200年前，我们说的很多东西大概清朝人都不会听明白吧</p>]]></content>
    
    
    <categories>
      
      <category>Language</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Japanese</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Debugging Gradient Checking</title>
    <link href="/2021/08/13/Debugging-Gradient-Checking/"/>
    <url>/2021/08/13/Debugging-Gradient-Checking/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在很多机器学习的算法中，在训练模型的时候，我们都会经常使用梯度下降法来对模型中的需要学习的参数进行更新，而梯度下降法就必须要运用到导数的运算了，因此验证自己算出来的导数是否正确十分有必要，不然最后模型效果不理想却又不知道具体的原因就不好了</p><p><img src="https://i.loli.net/2021/08/13/l7Ttik9xOGbDRPH.png"></p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>回忆一下导数的数学推导, 假设$J:R\to R,\theta\in R$<br>$$<br>\frac{dJ{(\theta)}}{d\theta}=\lim\limits_{\epsilon\to0}\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}<br>$$<br>因此，给定一个点$\theta$，我们可以用上面的式子去接近导数的值</p><p>在实际应用中，通常可以取$\epsilon$为一个很小的值$10^{-4}$</p><p>然后还需要考虑的是$\theta$的取值。在实际应用中，一般$J:R^n\to R,\theta\in R^n$</p><p>通常可以对每一个维度都进行上面的操作<br>$$<br>g_i(\theta)=\frac{\partial J(\theta)}{\partial\theta_i}=\frac{J(\theta^{(i+)})-J(\theta^{(i-)})}{2\times EPSILON}<br>$$<br>$\theta^{(i+)}=\theta+ \epsilon e_i$</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradcheck_naive</span>(<span class="hljs-params">f, x, gradientText</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot; </span><br><span class="hljs-string">    Gradient check for a function f.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    f -- a function that takes a single argument and outputs the</span><br><span class="hljs-string">         loss and its gradients</span><br><span class="hljs-string">    x -- the point (numpy array) to check the gradient at</span><br><span class="hljs-string">    gradientText -- a string detailing some context about the gradient computation</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    rndstate = random.getstate()<br>    random.setstate(rndstate)<br>    fx, grad = f(x) <span class="hljs-comment"># Evaluate function value at original point</span><br>    h = <span class="hljs-number">1e-4</span>        <span class="hljs-comment"># Do not change this!</span><br><br>    <span class="hljs-comment"># Iterate over all indexes ix in x to check the gradient.</span><br>    it = np.nditer(x, flags=[<span class="hljs-string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>])<br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> it.finished:<br>        ix = it.multi_index<br><br>        x[ix] += h <span class="hljs-comment"># increment by h</span><br>        random.setstate(rndstate)<br>        fxh, _ = f(x) <span class="hljs-comment"># evalute f(x + h)</span><br>        x[ix] -= <span class="hljs-number">2</span> * h <span class="hljs-comment"># restore to previous value (very important!)</span><br>        random.setstate(rndstate)<br>        fxnh, _ = f(x)<br>        x[ix] += h<br>        numgrad = (fxh - fxnh) / <span class="hljs-number">2</span> / h<br><br>        <span class="hljs-comment"># Compare gradients</span><br>        reldiff = <span class="hljs-built_in">abs</span>(numgrad - grad[ix]) / <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">abs</span>(numgrad), <span class="hljs-built_in">abs</span>(grad[ix]))<br>        <span class="hljs-keyword">if</span> reldiff &gt; <span class="hljs-number">1e-5</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient check failed for %s.&quot;</span> % gradientText)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First gradient error found at index %s in the vector of gradients&quot;</span> % <span class="hljs-built_in">str</span>(ix))<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Your gradient: %f \t Numerical gradient: %f&quot;</span> % (<br>                grad[ix], numgrad))<br>            <span class="hljs-keyword">return</span><br><br>        it.iternext() <span class="hljs-comment"># Step to next dimension</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient check passed!.&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Technology</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
